\documentclass[letterpaper,final,12pt,reqno]{amsart}

\usepackage[total={6.3in,9.2in},top=1.1in,left=1.1in]{geometry}

\usepackage{times,bm,bbm,empheq,fancyvrb,graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{longtable}
\usepackage{booktabs}

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}

\usepackage[kw]{pseudo}
\pseudoset{left-margin=15mm,topsep=5mm,idfont=\texttt}

% hyperref should be the last package we load
\usepackage[pdftex,
colorlinks=true,
plainpages=false, % only if colorlinks=true
linkcolor=blue,   % ...
citecolor=Red,    % ...
urlcolor=black    % ...
]{hyperref}

\renewcommand{\baselinestretch}{1.05}

\newtheoremstyle{claim}% name
  {5pt}% space above
  {5pt}% space below
  {\itshape}% body font
  {}% indent amount
  {\itshape}% theorem head font
  {.}% punctuation after theorem head
  {.5em}% space after theorem head
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}% theorem head spec
\theoremstyle{claim}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\newcommand{\eps}{\epsilon}
\newcommand{\RR}{\mathbb{R}}

\newcommand{\grad}{\nabla}
\newcommand{\Div}{\nabla\cdot}
\newcommand{\trace}{\operatorname{tr}}

\newcommand{\hbn}{\hat{\mathbf{n}}}

\newcommand{\bb}{\mathbf{b}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bbf}{\mathbf{f}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bn}{\mathbf{n}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}

\newcommand{\bF}{\mathbf{F}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bX}{\mathbf{X}}

\newcommand{\bxi}{\bm{\xi}}

\newcommand{\bzero}{\bm{0}}

\newcommand{\rhoi}{\rho_{\text{i}}}

\newcommand{\ip}[2]{\left<#1,#2\right>}

\newcommand{\mR}{R^{\bm{\oplus}}}

% numbering
\setcounter{tocdepth}{3}
\makeatletter
\def\l@subsection{\@tocline{2}{0pt}{4pc}{5pc}{}}
\makeatother

\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}
\numberwithin{theorem}{section}


\begin{document}
\title[Geometric multigrid for glacier modeling]{Geometric multigrid for glacier modeling: \\ New techniques and concepts}

\author{Ed Bueler}

\begin{abstract} FIXME: two principles in introduction: mass conservation complementarity, solver optimality.  four examples in sections \ref{sec:subspace}--\ref{sec:stokes}: poisson equation from subspace decomp point of view, obstacle problem by subset decomposition, monotone multigrid for implicitly-evolving SIA geometry, Schur-complement and Vanka Newton-multigrid for fixed-geometry Glen-Stokes
\end{abstract}

\maketitle

\tableofcontents

\thispagestyle{empty}
\bigskip

\section{Introduction} \label{sec:intro}

The construction of effective numerical glacier and ice sheet models is challenging for two fundamental reasons.  First is the complexity of the equations and boundary conditions.  Indeed, the physics of glaciers is nonlinear, nontrivially-coupled, and subject to imperfectly-understood boundary processes, such as at contact with ocean water.  The coupling is critical in the sense that mass, momentum, and energy conservation interact in ways which are relevant to glaciological modeling goals, such as when basal sliding, and thus ice velocity, is only determined though a simultaneous momentum and energy solution.  Second, the geometry of glaciers and ice sheets is complex, and in particular the fastest-flowing parts of ice sheets are often located at the geometrically-nontrivial lateral boundary where fjord-like bed geometry is also common.  Numerical models therefore need to perform expensive fine-mesh calculations, so as to accomodate the complicated, changing boundary geometry, while solving relatively-complicated multiphysics equations.

On the other hand, since the 1980s researchers in numerial methods have developed multigrid methods to solve partial differential equations like those which describe the ice fluid in glaciers.   For simpler problems like scalar elliptic equations and the linear Stokes system, especially on domains which have a simpler geometry, these methods are now in routine use \cite{Briggsetal2000,Bueler2021,Trottenbergetal2001}.

FIXME Accessible introductions to FE methods are in \cite{Bueler2021,Elmanetal2014,Johnson2009}.  Closest to our view in section \ref{sec:subspace} is \cite[Chapter V]{Braess2007}

FIXME cite for multigrid on classical obstacle \cite{BrandtCryer1983,Bueler2021,GraeserKornhuber2009}


\section{From subspace decomposition to geometric multigrid} \label{sec:subspace}

\subsection*{Finite elements for a Poisson model problem}  In this section we will consider how to solve a simple differential equation, namely a linear Poisson-like problem
\begin{equation}
- \big(\alpha(x)\,u'(x)\big)' = f(x) \quad \text{on} \quad 0 \le x \le 1, \label{eq:poisson}
\end{equation}
with Dirichlet boundary conditions $u(0)=u(1)=0$, using a finite element discretization and a multigrid method.  The functions $\alpha(x)$ and $f(x)$ are the given data here, assumed sufficiently well-behaved for the computations which follow, and we assume $\alpha(x)$ is bounded and positive so the problem is elliptic \cite{Evans2010}: $0 < c_1 \le \alpha(x) \le c_2$.  Over the course of the next three sections, this simple problem will evolve into a realistic model for glacier geometry.

Our numerical approximation of \eqref{eq:poisson} uses a mesh of $m$ interior \emph{nodes} (points) $x_p$ on $(0,1)$.  The $m+1$ intervals between the nodes are the \emph{elements}.  The numerical solution $u^h(x)$ is then a linear combination of the piecewise-linear \emph{hat functions} $\psi_p(x)$, shown in Figure \ref{fig:finehats}, one for each interior node:
\begin{equation}
u^h(x) = \sum_{p=1}^m u_p \psi_p(x). \label{eq:trialsolution}
\end{equation}
Each hat function $\psi_p(x)$ is continuous on $[0,1]$, linear on each element, and satisfies $\psi_p(x_q) = \delta_{pq}$.  The set $\{\psi_p(x)\}_{p=1}^m$ is a \emph{nodal basis} of the space $\mathcal{V}^h$ of continuous, piecewise-linear functions; the coefficients equal the function values: $u_p=u^h(x_p)$.  Note that the derivative of $u^h(x)$ is well-defined on the elements, thus almost everywhere, but not at the nodes.  In a computer program the coefficients $u_p$ will be formed into a (column) vector $\bu=\{u_p\}$ in $\RR^m$.

\begin{figure}
\includegraphics[width=0.65\textwidth]{genfigs/finehats.pdf}
\caption{Hat functions $\psi_p(x)$ at interior points $x_p$ form a basis for a vector space $\mathcal{V}^h$ of piecewise-linear functions.}
\label{fig:finehats}
\end{figure}

As usual in numerical differential equations, we are interested in the fine-mesh limit where $m$ is large.  Previewing the glacier problems in upcoming sections, what is the need for a high-resolution mesh, i.e.~a large value of $m$ and small spacing between the nodes $x_p$?  For an ice sheet model, the need is obvious: a high-resolution mesh can capture realistically-bumpy bed topography and spatially-varying climatic mass-balance.  These fields roughly correspond in problem \eqref{eq:poisson} to variations in the coefficient function $\alpha(x)$ and the source function $f(x)$, respectively.  We will need the mesh to be \emph{fine}, i.e.~high-resolution, so as to capture the fine scales in these model input data.  In addition, as we start to address in section \ref{sec:obstacle}, a fine mesh will allow precise modeling of glacier extent and the location of glacier margins.

Our application of multigrid ideas to glacier problems will use finite element (FE) concepts, and therefore we must rephrase \eqref{eq:poisson} into \emph{weak form} using integrals.  (The original equation will be called the \emph{strong form}.)  To do this we suppose that the solution $u(x)$ comes from a vector space $\mathcal{H}$ of functions which have values of zero at $x=0$ and $x=1$, and are smooth enough to make sense of the form which follows.  While we will not over-use this mathematical language, in fact $\mathcal{H}=H_0^1[0,1]=W_0^{1,2}[0,1]$ is a Hilbert and Sobolev space \cite[for example]{Evans2010} consisting of functions with one square-integrable derivative.  Also, we assume $\alpha(x)$ is at least in $L^\infty[0,1]$ and $f(x)$ at least in $L^2[0,1]$.

The weak form arises by multiplying both sides of \eqref{eq:poisson} by a \emph{test function} from $\mathcal{H}$ and integrating by parts so that only first derivatives remain.  Denoting the test function as $v(x)$, integrating on $[0,1]$, and using $v(0)=v(1)=0$, we find
\begin{equation}
\int_0^1 \alpha(x) u'(x) v'(x)\,dx = \int_0^1 f(x) v(x)\, dx.  \label{eq:weakpoissonearly}
\end{equation}
We now write this equation more compactly as
\begin{equation}
  a(u,v) = \ip{f}{v}, \label{eq:weakpoisson}
\end{equation}
defining each side as in \eqref{eq:weakpoissonearly}; we seek $u$ in $\mathcal{H}$ so that \eqref{eq:weakpoisson} holds for all $v$ in $\mathcal{H}$.  The left side functional $a(u,v)$ is linear in each argument (\emph{bilinear}) while the right side defines a \emph{linear functional} $\ell[v] = \ip{f}{v}$ acting on $v$ in $\mathcal{H}$.  (The convenience of such abstract notation will become clear as we describe multilevel algorithms.)

Our FE method seeks a solution $u^h$ in $\mathcal{V}^h$ of the same, but now finite-dimensional, weak-form,
\begin{equation}
  a(u^h,v) = \ip{f}{v},  \label{eq:feweakpoisson}
\end{equation}
for all test functions $v$ in $\mathcal{V}^h$.  However, $\mathcal{V}^h$ is a subset of $\mathcal{H}$ so we can compare $u^h$ and $u$.  In fact, by subtracting \eqref{eq:feweakpoisson} from \eqref{eq:weakpoisson} we see that the \emph{numerical error} $u-u^h$ satisfies the simple equation $a(u-u^h,v)=0$ for all $v$ in $\mathcal{V}^h$.  That is, the numerical error is orthogonal, in the sense determined by $a(\cdot,\cdot)$, to the finite-dimensional subspace $\mathcal{V}^h$.  Standard FE arguments then show that the error goes to zero as $h$ goes to zero \cite{Braess2007,Elmanetal2014}.

One may substitute formula \eqref{eq:trialsolution} for $u^h$ into \eqref{eq:feweakpoisson} to derive a linear system
\begin{equation}
A \bu = \bbf, \label{eq:linearsystem}
\end{equation}
where $A$ is an $m\times m$ matrix and $\bbf$ is in $\RR^m$.  Each equation (row) in system \eqref{eq:linearsystem} is constructed by using a hat function as a test function; substitution of $v=\psi_p$ gives the $p$th equation.  The matrix $A$ has entries $a_{pq} = a(\psi_p,\psi_q)$, is symmetric, and is positive definite.  For the right side one defines entries $f_p = \ip{f}{\psi_p}$ to form the vector $\bbf = \{f_p\}$.  For the 1D problem here, only three values $a_{p,p-1}, a_{p,p}, a_{p,p+1}$ are nonzero in row $p$, so $A$ is tridiagonal.  (Writing these entries in detail, a worthy exercise, may remind the reader of finite difference approximations for the Laplacian.)

Depending on the form of $\alpha(x)$, the matrix entries
\begin{equation}
  a_{pq} = \int_0^1 \alpha(x) \psi_p'(x) \psi_q'(x)\,dx \label{eq:poissonentries}
\end{equation}
may be computed either exactly, or approximately by quadrature, and similar considerations apply to the values $f_p = \int_0^1 f(x) \psi_p(x)\,dx$.  The errors arising from doing these integrals by quadrature will have only a modest effect on the accuracy of the whole FE method \cite{Braess2007}.

The most straightforward way to solve assembled linear system \eqref{eq:linearsystem} would be via a ``direct'' method like Gaussian elimination.  In 2D and 3D problems, where the number of unknowns $m$ is large, such direct methods need much more that $O(m)$ operations to solve the system, while, as noted in the introduction, our applications demand optimal $O(m)$ solution methods, or nearly so.  Furthermore, excluding the current section, all of our problems will be nonlinear, thus no finite-time direct method will be available anyway.  Thus we will generally not assemble matrices, but instead construct rapidly-convergent iterations using easy-to-implement pointwise iterations and multiple mesh levels.

Our solution methods will be phrased in terms of ``residuals''.  Suppose $w$ in $\mathcal{V}^h$ is any estimate of the solution $u^h$ of \eqref{eq:feweakpoisson}.  We define the \emph{residual} of $w$ as a linear functional acting on $v$ in $\mathcal{V}^h$:
\begin{equation}
  F(w)[v] = a(w,v) - \ip{f}{v}.  \label{eq:residual}
\end{equation}
Finding $u^h$ is equivalent to making all components of its residual, i.e.~all values $F(w)[v]$ for $v$ in $\mathcal{V}^h$, zero:
\begin{equation}
  F(u^h)[v]=0 \qquad \text{ if and only if } \qquad a(u^h,v)=\ip{f}{v}. \label{eq:residualweakequivalence}
\end{equation}

Given $w$ which does not already solve \eqref{eq:feweakpoisson}, we will iteratively update it using simple operations which make residual components $F(w)[v]$ smaller.  Similarly to \eqref{eq:trialsolution}, we may represent $w$ using the basis of hat functions, i.e.~$w = \sum w_q \psi_q$ with $w_q = w(x_q)$, and then compute a component in this basis using the linearity of $a(\cdot,\cdot)$:
\begin{equation}
  F(w)[\psi_p] = \sum_{q=1}^{m} a(\psi_q,\psi_p) \,w(x_q) - \ip{f}{\psi_p}.  \label{eq:residualpoisson}
\end{equation}

The number of nonzero terms in sum \eqref{eq:residualpoisson} is equal to the number of hat functions $\psi_q$ whose support overlaps the support of $\psi_p$.  This number is at most three in the current 1D case, and for general 2D and 3D meshes it remains small if the elements do not have small angles \cite{Braess2007}.  Thus the computation of a component $F(w)[\psi_p]$ requires $O(1)$ work, so computing all components of a residual linear functional $F(w)$ requires $O(m)$ work.  This observation about the cost of evaluating a residual is foundational in all of our methods.

\subsection*{Coarse mesh levels and subspace decompositions}  From the above simple FE scheme we now take the first steps to build a \emph{multilevel} scheme.  Consider an enlarged set of hat functions:
    $$\underbrace{\psi_1(x),\dots,\psi_m(x)}_{\text{existing fine level}},\underbrace{\psi_{m+1}(x),\dots,\psi_M(x)}_{\text{\small coarser levels}}$$
For example, two coarser levels, derived from the fine level in Figure \ref{fig:finehats} by coarsening, are shown in Figure \ref{fig:coarsehats}.  The first coarsening (top) by-passes every other node on the fine mesh, and the next coarsening (bottom) does this again.

\begin{figure}
\includegraphics[width=0.55\textwidth]{genfigs/coarsehats.pdf}
\smallskip

\includegraphics[width=0.55\textwidth]{genfigs/coarsesthats.pdf}
\caption{Coarser levels are additional sets of hat functions which spread over a greater distance.}
\label{fig:coarsehats}
\end{figure}

On 2D and 3D meshes this manner of constructing coarse-mesh hats is less straightforward, and it is more common to start from a coarse mesh and refine level-by-level up to the finest.  In any case, our methods are not restricted to equally-spaced meshes, and models with a high-resolution structured mesh \cite[for example]{Bueler2016,Winkelmannetal2011} are among our target applications. We will return to these mesh-generation issues in section \ref{sec:sia}.

We need notation for the levels.  Suppose that the coarsest level is indexed as $j=0$ and the finest as $j=J$, and that the hat functions $\psi_p^j(x)$, for $p=1,\dots,m_j$, form the $j$th level.  The interior nodes $x_p^j$ use the same multilevel indexing scheme.  For example, Figures \ref{fig:finehats} and \ref{fig:coarsehats} show a three-level scheme ($J=2$) with $m_2=7$ fine-level, $m_1=3$ middle-level, and $m_0=1$ coarsest-level hat functions.  The original mesh has $m_2+1=8$ elements, divisible by four, so this three-level coarsening was allowed.  Note that the original fine level has now gained a superscript $J$; the original hats are $\psi_p^J(x)$ and the original nodes are $x_p^J$.

On each level the hat functions are linearly-independent and form a basis for a vector space:
\begin{equation}
  \mathcal{V}^j = \operatorname{span}\{\psi_1^j(x),\dots,\psi_{m_j}^j(x)\} \subset \mathcal{H}.  \label{eq:definevk}
\end{equation}
These spaces are nested, i.e.~$\mathcal{V}^{j-1} \subset \mathcal{V}^j$, because each hat function can be written as a linear combination of finer-level hats,
\begin{equation}
   \psi_p^{j-1}(x) = \sum_{q=1}^{m_j} c_{pq} \psi_q^j(x). \label{eq:hatcombination}
\end{equation}
In fact, the coefficients are
\begin{equation}
  c_{pq} = \psi_p^{j-1}(x_q^j) \label{eq:nodalcoefficients}
\end{equation}
because $\{\psi_q^j\}$ form a nodal basis.  Nonzero coefficients $c_{pq}$ therefore occur only when a fine-level node $x_q^j$ is in the non-zero set (the open \emph{support}) of a coarser hat $\psi_p^{j-1}(x)$.  Statements \eqref{eq:hatcombination} and \eqref{eq:nodalcoefficients}, while simple, will be used throughout this paper when we need to pass between levels.

A \emph{multilevel subspace decomposition} is now described by the vector-space sum:
\begin{equation}
  \mathcal{V}^h = \mathcal{V}^0 + \mathcal{V}^1 + \dots + \mathcal{V}^J. \label{eq:subspacedecomposition}
\end{equation}
This decomposition will be useful despite the facts that the subspaces are nested and the final term $\mathcal{V}^J$ is actually equal to the whole FE space $\mathcal{V}^h$.  Equation \eqref{eq:subspacedecomposition} asserts that a piecewise-linear function in $\mathcal{V}^h$ \emph{can} be written as a linear combination of hat functions from all the levels, but there is no unique representation.  A multilevel method will use this decomposition to find the components of the solution in $\mathcal{V}^h$ via fast computations on all levels $\mathcal{V}^j$.

Using our hat function bases, a subspace decomposition provides an approximate \emph{scale of frequencies}.  Informally, the value of the inner product of a function $g(x)$ with a fine-level hat, $\ip{g}{\psi_p^J}$, is, relative to the norm $\|g\| = \ip{g}{g}^{1/2}$, mostly a measure of its high-frequency content at $x_p^J$.  The inner product with a coarser-mesh hat $\psi_p^j$, by contrast, measures lower frequency content because it averages over many fine-mesh nodes.  If decomposition \eqref{eq:subspacedecomposition} were instead a Fourier decomposition, with each $\mathcal{V}^j$ spanned by waves of disjoint frequency ranges, then the sum would be orthogonal and the ``scale of frequencies'' meaning would be exact.  Our decompositions will always involve overlapping ranges of frequencies, but our multilevel methods will reduce the energy of the error on each level $\mathcal{V}^j$, and on coarser levels this will rapidly reduce the low frequencies present in the error.

\subsection*{Gauss-Seidel, Jacobi, and time-stepping as smoothers}  Our first solution method for FE problem \eqref{eq:feweakpoisson}, called \emph{Gauss-Seidel} (GS) iteration, is sequential and point-wise \emph{relaxation} of the residual \eqref{eq:residual}.  Though matrices are often used to present such classical iteration algorithms \cite[for example]{Bueler2021,Greenbaum1997}, we present them using only residuals and hat functions, a beneficial approach when we consider nonlinear glacier models.

The $j$th-level GS algorithm sweeps through the mesh nodes $x_p^j$, modifying the iterate $w(x)$ by a multiple of $\psi_p^j$ to make that residual value zero.  That is, we compute $c$ so that
\begin{equation}
  F(w+c\,\psi_p^j)[\psi_p^j] = 0.  \label{eq:gaussseidelpoint}
\end{equation}
Because $a(\cdot,\cdot)$ is bilinear, $F(w+c\,\psi_p^j)[\psi_p^j] = F(w)[\psi_p^j] + c\, a(\psi_p^j,\psi_p^j)$, and thus we have the following algorithm which takes the residual function as an argument and modifies $w$ in-place:
\begin{pseudo*} \label{ps:gs-sweep}
\pr{gs-sweep}(j,w,F,\id{omega}=1)\text{:} \\+
    for $p=1,\dots,m_j$ \\+
        $\displaystyle c = - F(w)[\psi_p^j]\, \big/ \,a(\psi_p^j,\psi_p^j)$  \\
        $w \gets w + \id{omega} \,c \,\psi_p^j$
\end{pseudo*}

Assuming the values $a(\psi_p^j,\psi_q^j)$ and $\ip{f}{\psi_p^j}$ are available, computing $c$ involves $O(1)$ work, and thus one application of \pr{gs-sweep} requires $O(m_j)$ work.  Though \pr{gs-sweep} only works as stated for a linear equation in the form of \eqref{eq:feweakpoisson}, it can be modified to solve nonlinear systems by applying a few steps of Newton's iteration to each scalar equation $f(c) = F(w+c\,\psi_p^j)[\psi_p^j] = 0$.  This \emph{nonlinear Gauss-Seidel} algorithm is used in section \ref{sec:sia}.

What does a sweep of GS do to an iterate $w$?  By sequentially making the residual zero on each hat function we intend that the residual becomes smaller.  However, modifying $w$ to make the residual zero at one place generally means that previously-zeroed locations are no longer zero because the equations are non-trivially coupled.  One can prove for the Poisson problem that this iteration, applied on the fine level, converges to the solution $u^h$ of \eqref{eq:feweakpoisson} \cite[for example]{Greenbaum1997}, but that is not our focus.  Instead, a key observation is that such a sweep is a fast \emph{smoother} of the (algebraic) error $e=w-u^h$, even when it is slow to make this error small.

Informally, the formula for $c$ in \textsc{gs-sweep} combines neighboring values of $w$ so as to flatten a peak or trough in the error.  An example is shown in Figure \ref{fig:residualpoints}, where we start with a non-smooth initial iterate $w$ on an $m_J=6$ mesh; its residual $F(w)$ and error $e$ are at the top.  The Figure shows the residual and error after each step in the \textbf{for} loop in \pr{gs-sweep}, indicating the location which is zeroed.  (We plot the linear functionals $F(w)[\cdot]$ as piecewise-constant functions with values $F(w)[\psi_p^J]$.)  While both the residual and error become smaller in norm, the strongest effect is the damping of high frequencies.

\begin{figure}[t]
\includegraphics[width=0.8\textwidth]{genfigs/residualpoints.pdf}
\caption{One Gauss-Seidel (GS) sweep on the fine level adjusts the iterate $w$ so that the residual $F(w)[\psi_p^J]$ at each successive node $x_p^J$ is zero (left).  The corresponding errors $e=w-u^h$ get significantly smoother (right).}
\label{fig:residualpoints}
\end{figure}

One may describe the smoothing property of GS quantitatively by considering the frequencies which are supported on a mesh having spacing $h$.  The highest-frequency faithfully-represented mode is the sawtooth with (spatial) frequency $\sigma=(2h)^{-1}$.  For a 1D Poisson equation, with constant coefficient $\alpha$, one GS sweep multiplies all modes with frequencies higher than $\frac{1}{2} \sigma$ by factors of at most $1/\sqrt{5}\approx 0.45$ (for sufficiently-fine meshes \cite[Chapter 4]{Briggsetal2000}).  That is, a GS sweep \emph{strongly damps the highest half} of the frequencies in the error, though the particular damping factor depends on the dimension and the differential operator.

One may also adjust the \emph{relaxation factor} \id{omega} in \pr{gs-sweep} away from its default value \id{omega} $=1$, which yields \emph{successive over-relaxation} for \id{omega} $>1$.  This iteration has superior properties as a stand-alone solver \cite{Greenbaum1997}, but it is no better than GS as a smoother.

Algorithm \pr{gs-sweep} has no dependence on the dimension of the problem.  In particular, for any linear elliptic PDE which can be written in weak form \eqref{eq:weakpoisson}, \pr{gs-sweep} applies as stated with $m_j$ denoting the number of $j$th-level mesh nodes and $\psi_p^j$ the corresponding hat functions.  However, the rate of convergence and the smoothing efficiency of GS iterations depend on the details of the PDE, including its dimension.

Note that GS involves repeatedly re-evaluating the pointwise residual, i.e.~computing $F(w)[\psi_p^j]$ after each update of $w$.  Where each evaluation of a point residual is expensive, for instance when a residual evaluation is non-local (see section \ref{sec:stokes}), an alternative is to evaluate the residual vector (i.e.~at all points) once at the start of the iteration.  The resulting \emph{(weighted) Jacobi} method is parallel in the sense that iterate updates can then be done in any order.

\begin{pseudo*} \label{ps:jacobi-sweep}
\pr{jacobi-sweep}(j,w,F,\id{omega}=0.7)\text{:} \\+
    $r_p = F(w)[\psi_p^j]$ \qquad\qquad\qquad\qquad \ct{for all $p$} \\
    for $p=1,\dots,m_j$ \\+
        $\displaystyle c = - r_p \, \big/ \, a(\psi_p^j,\psi_p^j)$  \\
        $w \gets w + \id{omega} \,c \,\psi_p^j$
\end{pseudo*}

The Jacobi iteration with relaxation factor \id{omega} $=1$ is neither a rapid solution method nor an effective smoother---no damping is applied to the highest-frequency sawtooth---but underrelaxation can be a good smoother.  For a 1D Poisson equation and optimal factor \id{omega} $=\frac{2}{3}$, the method multiplies all frequencies higher than $\frac{1}{2} \sigma$ by at most $\frac{1}{3}$ \cite[Chapter 4]{Briggsetal2000}.  Other optimal factors are known for 2D and 3D Poisson equation, and to varying degrees this good smoother performance extends to other PDEs.  One disadvantage of Jacobi is the need to store the starting residual $\br = \{r_p\}$, which doubles the required storage versus \pr{gs-sweep}.

These smoother considerations seem to be separate from how evolving-geometry glacier models have been designed.  Most ice sheet models proceed by explicit time-stepping of their complicated, coupled equations \cite[for example]{Winkelmannetal2011}.  However, it turns out that such time-stepping is closely related to Jacobi smoothing.  Consider the time-dependent problem corresponding to the $\alpha=1$ case of \eqref{eq:poisson}, namely the \emph{heat equation}
\begin{equation}
u_t = u_{xx} + f, \qquad u(t,0)=u(t,1)=0, \label{eq:heat}
\end{equation}
for $u(t,x)$ and $t>0$, and subject to some initial condition $u(0,x)=g(x)$.  The corresponding FEM weak form follows (as usual) by multiplying by $v$ from $\mathcal{V}^j$ \cite[Chapter 8]{Johnson2009}:
\begin{equation}
\frac{d}{dt}\ip{u^h}{v} = -a(u^h,v) + \ip{f}{v} = - F(u^h)[v]. \label{eq:feheat}
\end{equation}
Expanding the FEM solution $u^h(t,x)$ in hat functions, we have
\begin{equation}
u^h(t,x) = \sum_{q=1}^{m_j} u^h(t,x_p^j) \psi_p^j(x). \label{eq:trialheat}
\end{equation}
Equation \eqref{eq:feheat} with $v=\psi_p^j$ generates an ODE system for the coefficients:
\begin{equation}
\sum_{q=1}^{m_j} \ip{\psi_p^j}{\psi_q^j} u^h(t,x_p^j) = - F(u^h)[\psi_p^j] \label{eq:odeheatearly}
\end{equation}
Identifying the \emph{mass matrix} $B$ with entries $b_{pq} = \ip{\psi_p^j}{\psi_q^j}$, which is an easily inverted matrix with $O(1)$ condition number \cite{Elmanetal2014}, and we can write the ODE system as
\begin{equation}
u^h(t,x_p^j)' = - (B^{-1} F(u^h)[\cdot])_p. \label{eq:odeheat}
\end{equation}

Suppose we discretize time with spacing $\Delta t$ and approximate consecutive states $u^h(t,x)$, $u^h(t+\Delta t,x)$ by $w(x)$, $\tilde w(x)$, respectively.  Using the forward Euler method, \eqref{eq:odeheat} becomes
\begin{equation}
\frac{\tilde w_p - w_p}{\Delta t} = - (B^{-1} F(w)[\cdot])_p. \label{eq:forwardeulerheat}
\end{equation}
Let us write this time-step as a pseudocode which takes the current values $w$ and computes the new values $\tilde w$.  Up to the inversion of the mass matrix $B$, and choosing to compute the updated iterate in-place, we have a method similar to Jacobi:
\begin{pseudo*} \label{ps:euler-timestep}
\pr{euler-timestep}(j,w,F,\id{deltat})\text{:} \\+
    $r_p = (B^{-1} F(w)[\cdot])_p$ \qquad\qquad\qquad\qquad \ct{for all $p$} \\
    for $p=1,\dots,m_j$ \\+
        $\displaystyle c = - r_p$  \\
        $w \gets w + \id{deltat}\, c\, \psi_p^j$ \\-
\end{pseudo*}
In particular, the time step \id{deltat} apparently plays the role of \id{omega} in \pr{jacobi-sweep}.  However, the change $c$ to the value $w_p$ is not divided by the diagonal entries $a(\psi_p^j,\psi_p^j)$, as in the Jacobi iteration, so for \emph{conditional stability} \cite{Bueler2021} one must compensate by choosing \id{deltat} $=O(h^2)$.

In summary, applied to the Poisson problem \eqref{eq:poisson}, the GS and Jacobi iterations act as smoothers.  Subject to stability restrictions on the time step, forward Euler time-stepping for the corresponging heat equation \eqref{eq:heat} is also a smoother.  Applied on a single mesh level, all of these methods are slow to converge.  Optimal performance of the multilevel methods of this paper depends on applying such smoothers on different mesh levels.  We will see in sections \ref{sec:sia} and \ref{sec:stokes} that the particular smoothers here are not sufficient, and indeed smoother choice is nontrivial for glacier problems, but fortunately the space of possible smoothers is sufficiently large.

\subsection*{Multilevel subspace corrections}  When a smoother is applied on a given level $j$, whether once or a few times, the error and residual of the iterate will no longer contain much energy in the $j$th-level modes.  Thus it makes sense to remove the energy in the next-coarser modes by applying the smoother on the $j-1$ level.  Starting on the finest level $J$, we have the following \emph{multilevel subspace decomposition} algorithm which corrects the residual on each level.
\begin{pseudo*} \label{ps:msd-slash}
\pr{msd-slash}(w,F)\text{:} \\+
    for $j=J$ downto $0$ \\+
        \pr{smoother}(j,w,F)
\end{pseudo*}
Here \pr{smoother} stands for \pr{gs-sweep} or \pr{jacobi-sweep} in particular.  The reason this downward-only scheme is called ``slash'' is illustrated in Figure \ref{fig:msdcycles}.  Other correction sequences are possible, for example one may return to the fine level in a ``V'' cycle, as follows.
\begin{pseudo*} \label{ps:msd-vcycle}
\pr{msd-vcycle}(w,F)\text{:} \\+
    for $j=J$ downto $0$ \\+
        \pr{smoother}(j,w,F) \\-
    for $j=1$ to $J$ \\+
        \pr{smoother}(j,w,F)
\end{pseudo*}
Furthermore one may repeatedly apply the smoother on each level, or go up-and-down the mesh hierarchy in a more complicated manner (``W-cycles'', for example), or even use coarser subspace corrections before finer; there are many possibilities \cite{Briggsetal2000,Trottenbergetal2001}.

\begin{figure}
\input{tikz/msdcycles.tex}
\caption{Subspace corrections may be applied downward-only as in \pr{msd-slash} (left) or in a V-cycle as in \pr{msd-vcycle} (right).  Each dot in this three-level hierarchy ($J=2$) is a smoother application on that level.}
\label{fig:msdcycles}
\end{figure}

These \pr{msd} ``cycle'' algorithms improve a fine-level iterate $w$ but they do not (generally) exactly solve the problem \eqref{eq:feweakpoisson}.  Instead one applies them via iteration, testing for convergence using some tolerance on the norm of the (computable) fine-level residual.  (An alternative view of MSD algorithms would treat them as preconditioners \cite[for example]{Bueler2021}, but we will not need to exploit this idea.)  For example, the following algorithm requires the residual to decrease from its initial value by a certain factor.
\begin{pseudo*} \label{ps:msd-solver}
\pr{msd-solver}(F,\id{rtol}=10^{-4})\text{:} \\+
    $w=0$ \qquad\qquad\qquad\qquad\qquad \ct{or other fine-level initial iterate} \\
    $r_0 = \|F(w)\|$ \\
    repeat \\+
        \pr{msd-slash}(w,F) \qquad\qquad \ct{or \pr{msd-vcycle}} \\-
    until $\|F(w)\| \le r_0\, \id{rtol}$ \\
    return $w$
\end{pseudo*}

Such simply-stated MSD schemes appeared relatively-late in the history of multigrid \cite{Xu1992}, but they already contain the core multigrid ideas and possess excellent convergence properties.  For example, the following theorem applies to linear elliptic PDEs in any dimension.  The constant $\rho$ depends only on the element aspect ratios (\emph{shape regularity} \cite{Elmanetal2014}) in the mesh---this is not relevant in 1D---and on the coefficient-minimum (\emph{ellipticity bound}) $\alpha_0=\inf_x \alpha(x)$.  Recall that $u^h$ is the exact solution of FE weak form \eqref{eq:feweakpoisson}.

\begin{theorem} \cite[Thm.~3.10]{GraeserKornhuber2009}\,  \label{thm:msdconvergence}  Suppose $w^{(s)}$  results from $s$ applications of \pr{msd-slash} or \pr{msd-vcycle} on the fine ($J$th) level, with spacing $h$, starting with any $w^{(0)}$.  There is $\rho<1$ independent of $J$ and $h$ so that
\begin{equation}
  \|w^{(s+1)} - u^h\|_{\mathcal{H}} \le \rho \|w^{(s)} - u^h\|_{\mathcal{H}}.  \label{eq:msdconvergence}
\end{equation}
\end{theorem}

This result, that each cycle reduces the error by a mesh-independent factor $\rho<1$, is called \emph{multigrid convergence}, and $\rho$ the \emph{multigrid convergence rate} \cite{Braess2007}.  It follows from \eqref{eq:msdconvergence} that if we want the error norm $e^{(s)} = \|w^{(s)}-u^h\|_{\mathcal{H}}$ to be reduced to below some level $1>\eps>0$ then we should do $s>(\log\eps - \log e^{(0)})/\log \rho$ iterations, where $e^{(0)}$ is the initial error norm.  That is, we need $O(|\log\eps|)$ iterations.

Regarding this theorem, note that error norms are not generally computable, but they are proportional to computable residual norms within a factor of the condition number of the system matrix \cite[Chapter 2]{Bueler2016}.  Also, smaller $\rho$ implies faster convergence, and it is common for a V-cycle to have a better convergence rate than a slash cycle in this sense, but the latter scheme does less work per cycle.  In section \ref{sec:obstacle} we will measure performance by the amount of computational work for a given residual norm reduction.

However, \pr{msd-solver} is not yet a complete solver.  Its performance depends on how iterates $w$ and residuals $F(w)[\cdot]$ are \emph{represented on each mesh level}.  For example, if we represent $w$ and $F(w)$ using the fine-level basis $\{\psi_p^J\}$ then one application of \pr{gsweep}$(J,w,F)$ is indeed fast, i.e.~$O(m_J)$ operations, with a small constant, because evaluating each $F(w)[\psi_p^J]$ requires only a few operations.  However, evaluating $F(w)[\psi_p^j]$ on coarse levels ($j<J$) means computing integrals over the wide support of $\psi_p^j$, which is nonzero at many fine-mesh nodes $x_p^J$.  For example, \pr{gs-sweep}$(j,w,F)$ for $j<J$ is not an efficient operation if $w$ and $F(w)$ are represented on the fine level; it is not an $O(m_j)$ operation with a coefficient which is independent of $j$.  This fundamental concern is addressed via hierarchical representations which are compatible with multilevel corrections.

\subsection*{Linear geometric multigrid}  Once the smoother has been applied on a finer level, so that the error and the residual no longer contain much energy in the high-frequency modes, the smoother should then be applied on the next-coarser level.  However, appropriate coarse-level data structures must be used to represent the problem on the new level.  This idea is then applied down the mesh-level hierarchy.  By using level-specific representations we go beyond multilevel subpace corrections and create a true multigrid algorithm.

Efficiency on coarser levels requires addressing two concerns:
\renewcommand{\labelenumi}{\emph{\roman{enumi})}}
\begin{enumerate}
\item How do we represent an iterate $w$ and a residual $F(w)[\cdot]$ on the $j$th level?
\item How does a $j$th-level problem descend to the representation on the $j-1$ level?
\end{enumerate}

Our answer to \emph{i)} is straightforward.  A function $w(x) = \sum_p w_p \psi_p^j(x)$ in $\mathcal{V}^j$ is represented by its nodal values $w_p=w(x_p^j)$, thus $\bw = \{w_p\}$ is a vector in $\RR^{m_j}$.  Representing a residual $F(w)$, a linear functional in $(\mathcal{V}^j)'$, is just as simple because the values $F_p = F(w)[\psi_p^j]$ form a vector $\bF=\{F_p\}$, also in $\RR^{m_j}$.  It is common to think of $\bw$ as a column vector and $\bF$ as a row vector, but this is not essential.

For \emph{ii)} we must derive a new equation for the coarser level, one which loses only minimal information if the key quantities are smooth.\footnote{The equation is new in the sense that we have not yet given it, but it is standard in multgrid literature.  The ``key quantities'' are the (algebraic) error of an iterate and its residual.}  Note that for an iterate $w$ in the original FE space $\mathcal{V}^h$, the residual definition \eqref{eq:residual} can be rewritten as
\begin{equation}
  a(w,v) = \ip{f}{v} + F(w)[v].  \label{eq:residualrewrite}
\end{equation}
Subtracting the weak form \eqref{eq:feweakpoisson} from \eqref{eq:residualrewrite} cancels the source term:
\begin{equation}
  a(w,v) - a(u^h,v) = F(w)[v].  \label{eq:errorequationearly}
\end{equation}
Because of the linearity of $a(\cdot,\cdot)$ in the first position, we have the (weak-form) \emph{error equation},
\begin{equation}
  a(e,v) = F(w)[v],  \label{eq:errorequation}
\end{equation}
for all $v$ in $\mathcal{V}^h$, where $e=w-u^h$ is the algebraic error.

Regarded as applying in $\mathcal{V}^h$, error equation \eqref{eq:errorequation} is equivalent to the original FE weak form \eqref{eq:feweakpoisson}.  (Just reverse the above derivation.)  However, if the smoother has already been applied to an iterate $w$ then both $e$ and the residual $F(w)$ are smoothed quantities.  That is, both sides of \eqref{eq:errorequation} should have accurate representations in terms of the coarser hats $\{\psi_p^{J-1}\}$, without using the fine-level hats $\{\psi_p^{J}\}$.  Multigrid therefore approximates \eqref{eq:errorequation} with a new equation in which all quantities are represented using the coarser-level basis.

To state this \emph{coarse-level equation} we generalize to an arbitrary level $j>0$ and suppose $w^j$ is any iterate in $\mathcal{V}^j$.  Then \eqref{eq:errorequation} is approximated on the $j-1$ level as follows:
\begin{equation}
  a(e^{j-1},v) = (R\ell^j)[v] \qquad \text{where} \qquad \ell^j = F^j(w^j),  \label{eq:coarsecorrection}
\end{equation}
for all $v$ in $\mathcal{V}^{j-1}$.  Note $F^j(w^j)$ is the residual from the $j$th-level equation, and on the finest level $F^J(w^J)[v] = F(w^J)[v] = a(w^J,v) - \ip{f}{v}$.  We will represent $e^{j-1}$ in the basis $\{\psi_p^{j-1}\}$, and $R\ell^j$  in $(\mathcal{V}^{j-1})'$ by its values $(R\ell^j)[\psi_p^{j-1}]$.

However, in \eqref{eq:coarsecorrection} we have used the \emph{canonical restriction operator} $R$ to put the $j$th-level residual onto the $j-1$ level.  By definition, $R$ maps a linear functional $\ell$ in $(\mathcal{V}^j)'$ to a linear functional in $(\mathcal{V}^{j-1})'$:
\begin{equation}
  (R \ell)[v] = \ell[v], \label{eq:canonicalrestriction}
\end{equation}
for all $v$ in $\mathcal{V}^{j-1}$.  That is, $R \ell$ acts on $\mathcal{V}^{j-1}$ in the same manner as $\ell$, but we may (and will) \emph{represent} $R\ell$ by its $j-1$ level values.  From \eqref{eq:hatcombination} and \eqref{eq:nodalcoefficients},
\begin{equation}
  (R \ell)[\psi_p^{j-1}] = \sum_{q=1}^{m_j} c_{pq}\, \ell[\psi_q^j], \label{eq:canonicalrestrictionaction}
\end{equation}
with $c_{pq}=\psi_p^{j-1}(x_q^j)$.  Note that using this \emph{full-weighting} formula \cite{Briggsetal2000} requires a bit of computational work.

Given the solution $e^{j-1}$ of \eqref{eq:coarsecorrection}, the update
\begin{equation}
  w^j \gets w^j + P e^{j-1}  \label{eq:update}
\end{equation}
then ``corrects'' the fine-level iterate.  This requires yet another map, the \emph{canonical prolongation} $P$, which acts on (a function) $y$ in $\mathcal{V}^{j-1}$ to give $Py$ in $\mathcal{V}^j$:
\begin{equation}
  (P y)(x) = y(x). \label{eq:canonicalprolongation}
\end{equation}
Again \eqref{eq:hatcombination} and \eqref{eq:nodalcoefficients} can be used to find the components of $Py$ in the fine-level basis.  Specifically, if $y=\sum_p y_p \psi_p^{j-1}$ then $Py = \sum_q (Py)_q \psi_q^j$ where
\begin{equation}
  (Py)_q = \sum_{p=1}^{m_{j-1}} c_{pq}\, y_p, \label{eq:canonicalprolongationaction}
\end{equation}
and $c_{pq} = \psi_p^{j-1}(x_q^j)$ as before.  The result $P y$ is the same piecewise-linear function as the input $y$, but its representation has changed.  No information is lost because $\mathcal{V}^{j-1} \subset \mathcal{V}^j$.

Combining the above ideas with the earlier MSD cycles gives a \emph{geometric multigrid} (GMG) method, and we show a V-cycle for an example.  In addition to sweeps of the GS smoother, one solves the coarse-level correction equation \eqref{eq:coarsecorrection}, applies update \eqref{eq:update}, and follows by more smoother sweeps.  Our pseudocode modifies the fine-level iterate $w$ in-place and can be called repeatedly so as to improve the iterate.
\begin{pseudo*} \label{ps:gmg-vcycle}
\pr{gmg-vcycle}(w,F,\id{down}=1,\id{up}=1)\text{:} \\+
    $w^J, F^J = w, F$ \\
    for $j=J$ downto $1$ \\+
        $\text{\pr{smoother}}^{\text{\id{down}}}(j,w^j,F^j)$ \\
        $\ell^{j-1} := R(F^j(w^j))$ \\
        $F^{j-1}(y)[\cdot] := a(y,\cdot) - \ell^{j-1}[\cdot]$ \\
        $w^{j-1} = 0$ \qquad\qquad\qquad\qquad\qquad \ct{note $w^{j-1}=e^{j-1}$} \\-
    \pr{gmg-coarsesolve}(w^0,F^0) \\
    for $j=1$ to $J$ \\+
        $w^j \gets w^j + P w^{j-1}$ \\
        $\text{\pr{smoother}}^{\text{\id{up}}}(j,w^j,F^j)$ \\-
    $w = w^J$
\end{pseudo*}

Figure \ref{fig:vcycle} illustrates such a V-cycle.  After \texttt{down} smoother sweeps ($\text{\textsc{smoother}}^{\text{\texttt{down}}}$), the correction equation \eqref{eq:coarsecorrection} is applied on the next-coarser level with a ``new'' source (linear functional) $\ell^{j-1}$, and a zero initial iterate.  Observe that $\ell^{j-1}$ is the amount by which the finer-level iterate $w^j$ did not already solve the problem.  (For instance, $\ell^{J-1}=0$ if $w^J=u^h$.)  After the correction an additional \texttt{up} sweeps are done to remove high-frequency components from the updated iterate.  Observe that memory is needed to hold the states $w^0,\dots,w^J$ until after the coarse-level correction, and it is straightforward to implement \pr{gmg-vcycle} recursively.

\begin{figure}
\input{tikz/vcycle.tex}
\caption{A four-level V-cycle with distinguished methods: down-smoother (solid dots), up-smoother (circles), and coarse-level solver (square).}
\label{fig:vcycle}
\end{figure}

The algorithm calls a solver subroutine on the coarsest $j=0$ level.  For a linear problem like the one here, one may apply a direct solver, but they are not available for nonlinear (e.g.~glacier) problems.  For simplicity and generalizability we suppose \pr{gmg-coarsesolve} is also implemented as a fixed number of in-place smoother sweeps:
\begin{pseudo*} \label{ps:gmg-coarsesolve}
\pr{gmg-coarsesolve}(w,F,\id{coarse}=1)\text{:} \\+
    $\text{\pr{smoother}}^{\text{\id{coarse}}}(0,w,F)$ \\
\end{pseudo*}
If the coarsest mesh has a single node ($m_0=1$) then a single sweep gives an exact solution, and in general an accurate solution should be fast if $m_0$ is small.  Nonetheless the choice of the coarse level and its solver is nontrivial for realistic problems; see sections \ref{sec:sia} and \ref{sec:stokes}.

Before proceeding, the reader might ask what makes the above V-cycles ``geometric''?  The label is partly historical, but defining the restriction and prolongation operators require the FE mesh, i.e.~coefficients $c_{pq} = \psi_p^{j-1}(x_q^j)$ from the mesh hat functions.  Also note that we \emph{rediscretize} on coarser levels.  An alternative \emph{Galerkin} approach is to form mesh-level matrices recursively by $A^{j-1} = R A^j P$; see \cite{Bueler2021} and \cite[Chapter V]{Braess2007}.  For comparison to our geometric method, in \emph{algebraic multigrid} (AMG) \cite{Trottenbergetal2001} the prolongation $P$ is constructed using operations on the entries of the linear system matrix $A$.  (Then such algorithms invariably define $R=P^\top$.)  For linear elliptic problems the geometric and algebraic approaches are closely-related, in part because AMG has been ``tuned'' to generate prolongations $P$ with similar coefficients.  For our upcoming nonlinear and inequality-constrained problems, however, the geometric approach generalizes directly, while AMG can only solve the linear steps in a separately-constructed iteration, e.g.~an outer Newton iteration.  GMG and AMG approaches are natural competitors for the highest-performance solutions, but we persevere with the geometric approach.

\subsection*{On transfer operators}  The operators $R$ and $P$ make no choices, which is the meaning of ``canonical''.  However, their application requires computation because of how we represent functions and functionals on each level.  In fact, full-weighting formulae \eqref{eq:canonicalrestrictionaction} and \eqref{eq:canonicalprolongationaction}, which generalize verbatim to unstructured 2D and 3D meshes when the indices $p,q$ denote the nodes of the mesh \cite[Chapter V]{Braess2007}, arise from the simple fact that each coarse-level hat function is a linear combination of fine-level hats.  It turns out that three such mesh-level \emph{transfer} operators are used in this paper (Table \ref{tab:restrictionsprolongations}).  The third, a critical component allowing multilevel methods for problems involving inequality constraints, appears in the next section.

\newcommand{\iP}{P^{\hookrightarrow}}
\begin{table}
\begin{tabular}{l|ccccc}
\emph{operator}              & \emph{equation}  & \emph{domain}          & \emph{range}
                  & \emph{one-to-one} & \emph{linear} \\ \hline
canonical restriction $R$    & \eqref{eq:canonicalrestrictionaction} & $(\mathcal{V}^j)'$     & $(\mathcal{V}^{j-1})'$
                  &            & \checkmark \\
monotone restriction $\mR$   & \eqref{eq:monotonerestriction} & $\mathcal{V}^j$        & $\mathcal{V}^{j-1}$
                  &            &            \\
canonical prolongation $P$   & \eqref{eq:canonicalprolongationaction} & $\mathcal{V}^{j-1}$    & $\mathcal{V}^j$
                  & \checkmark & \checkmark \\
\end{tabular}

\medskip
\caption{We use three transfer operators in this paper.  The nonlinear monotone restriction operator $\mR$ is defined in section \ref{sec:obstacle}.}
\label{tab:restrictionsprolongations}
\end{table}

As linear operators, $R$ and $P$ may be represented as sparse non-square matrices, and then they are transposes ($P=R^\top$), but using memory to store them as matrices is unnecessary.  They are applied in $O(m_j)$ operations via \eqref{eq:canonicalrestrictionaction} and \eqref{eq:canonicalprolongationaction}.  However, the two operators differ in their invertibility.  A function $y$ in $\mathcal{V}^{j-1}$ can be exactly recovered from $Py$ because $y(x)$ and $(Py)(x)$ are the same piecewise-linear function, and in fact we only explicitly write $P$ when indicating a change in vector representation.  (Said differently, as a map $P$ is one-to-one, and thus it has a left inverse.)  By contrast, $\ell$ in $(\mathcal{V}^j)'$ is not recoverable from $R\ell$; the values $\ell[\psi_q^j]$, the action of $\ell$ on the finer level hats, are averaged onto the coarse level and thus lost.

We have now presented a basic geometric multigrid algorithm for linear PDEs via a particular FE viewpoint, namely the subspace decomposition approach pioneered by Xu \cite{Xu1992} and others, which is foundational for multilevel, domain-decomposition, and other advanced algorithms \cite[for example]{Farrelletal2019}.  Our next step might be to show computational results demonstrating the efficiency of a GMG algorithm, giving evidence of optimal $O(m_J)$ time to solve the problem.  (Such results appear in all multigrid references \cite{Braess2007,Briggsetal2000,Bueler2021,Elmanetal2014,Trottenbergetal2001}.)  However, we first introduce a less-standard ``obstacle'' problem, with the essential free-boundary character of the glacier geometry problem.  Computational results appear in each of the next three sections.


\section{Constraint decomposition for the obstacle problem} \label{sec:obstacle}

\subsection*{An ice-like model problem}  We now have a clear view of the geometric multigrid method, for a linear equation and from the multilevel subspace decomposition point of view.  However, as addressed in section \ref{sec:intro}, the main problem of how glacier geometry and ice velocity co-evolve in response to climatic inputs requires an inequality constraint for well-posedness, and the fact that the ice surface elevation is above the bed generates the land-terminating mass-conservation boundary condition.  This constraint, by itself, makes the problem nonlinear and reduces solution regularity, which is challenging to numerical methods.

To address constraints, before actually modeling glaciers in section \ref{sec:sia}, we introduce the \emph{classical obstacle problem} which adds an inequality constraint to linear Poisson equation \eqref{eq:poisson}.  After stating the weak form, which is now solved over a convex \emph{subset} of the function space, we will modify the multilevel subspace decomposition approach to be the \emph{multilevel constraint decomposition} method.  Each mesh level will host an inequality-constrained problem, and together all the levels will capture the original constraint set.  The nonlinear SIA model in section \ref{sec:sia} will use the nonlinear extension of this method.

Consider the same 1D domain and solution space $\mathcal{H}=H_0^1[0,1]$ as before.  Let $\varphi(x)$ be a fixed function, the obstacle, from $\mathcal{H}$.  The strong form of the classical obstacle problem is the following \emph{complementarity problem} (CP) \cite{Bueler2021,KinderlehrerStampacchia1980} which says that the solution $u(x)$ is above $\varphi(x)$ \emph{and} that a differential equation applies whereever it is strictly above:
\begin{align}
  u - \varphi &\ge 0 \label{eq:obstaclecp} \\
  -(\alpha u')'-f &\ge 0 \notag \\
  (u-\varphi)(-(\alpha u')'-f) &= 0 \notag
\end{align}
The third condition, \emph{complementarity} itself, implies that for each $x$ in $[0,1]$ the solution either coincides with the obstacle ($u(x)=\varphi(x)$) or \eqref{eq:poisson} holds.  (Both facts can hold at $x$, but in the generic \emph{nondegenerate} \cite{KinderlehrerStampacchia1980} case the obstacle does not itself solve the PDE.)  Also, by \eqref{eq:obstaclecp}, where the solution coincides with the obstacle the source term is bounded above: $u=\varphi \implies f \le -(\alpha\varphi')'$; in this sense the source term and the obstacle are related.  In the $u=\varphi$ region the constraint is said to be \emph{active}, thus \eqref{eq:poisson} holds in the \emph{inactive} portion where $u>\varphi$.  One refers to PDE \eqref{eq:poisson} as the \emph{interior condition} of the CP \cite{KinderlehrerStampacchia1980}.

Simply by choosing a source term $f(x)$ which is positive in the middle of the domain and negative near the boundaries, we get an ``ice-like'' solution to \eqref{eq:obstaclecp}.  For example, Figure \ref{fig:icelike} (top) shows the exact, piecewise-quadratic solution $u(x)$ for the following data:
\begin{equation}
\varphi(x) = x(1-x), \quad \alpha(x)=1, \quad f(x) = \begin{cases} 8, & 0.2 < x < 0.8, \\
                                                                 -16, & x<0.2 \text{ or } x>0.8. \end{cases}  \label{eq:icelikedetails}
\end{equation}
(Note that $f$ is in $L^2[0,1]$ and is defined almost everywhere.)  Finding the exact formula for $u(x)$, which smoothly-connects the five quadratic pieces, is an exercise for the reader.\footnote{Solution in\, \href{https://github.com/bueler/mg-glaciers/blob/master/py/obstacle.py}{\texttt{github.com/bueler/mg-glaciers/blob/master/py/obstacle.py}}.}

% regenerate:
%   $ cd py/1D/
%   $ ./obstacle.py -plain -jfine 7 -o icelike.pdf
%   $ ./obstacle.py -plain -jfine 7 -o parabola.pdf -problem parabola
%   $ pdfcrop icelike.pdf icelike.pdf
%   $ pdfcrop parabola.pdf parabola.pdf
\begin{figure}
\,\,\includegraphics[width=0.8\textwidth]{fixfigs/icelike.pdf}

\bigskip\medskip
\includegraphics[width=0.82\textwidth]{fixfigs/parabola.pdf}

\medskip
\caption{\emph{Top:} An ice-like configuration of the classical obstacle problem with $f$ of both signs. \emph{Bottom:} A traditional configuration with a parabolic obstacle and $f=0$.}
\label{fig:icelike}
\end{figure}

Unlike problem \eqref{eq:poisson}, the solution to \eqref{eq:obstaclecp} does \emph{not} depend linearly on the source function $f$.  For example, if $f \le 0$ and the obstacle is concave-down ($\varphi'' \le 0$) then the solution is $u=\varphi$; there are no inactive points by the maximum principle \cite{Evans2010}.  In such a case, if $\tilde u$ solves the problem for $\tilde f= -1$ then $2\tilde u$ \emph{does not} solve the problem for source term $2\tilde f = -2$.  In this sense the classical obstacle problem is nonlinear even though its interior condition is linear.

The glacier problem also has a CP formulation (section \ref{sec:sia}; see also \cite{Calvoetal2002}) in which the obstacle is the bed elevation, the solution is the glacier surface elevation, and the source term is the surface mass balance.  In the inactive region, i.e.~on the glacier, the mass conservation equation applies, but the surface mass balance must be negative in the active region off the glacier.

\subsection*{Weak formulation with constraints}  Now define a closed subset of the function space which incorporates the constraint:
\begin{equation}
\mathcal{K} = \left\{v \ge \varphi\right\} \subseteq \mathcal{H}.  \label{eq:Kdefine}
\end{equation}
If $v$ is in $\mathcal{K}$ then we say $v$ is \emph{admissible}.  Note $\mathcal{K}$ is not a vector space, but it is \emph{convex}.  That is, if $v,w$ are in $\mathcal{K}$ then any point on the line segment connecting them, namely $\theta v + (1-\theta) w$ for $0 \le \theta \le 1$, is also in $\mathcal{K}$.

As before, derivation of the weak form involves multiplying the strong form \eqref{eq:obstaclecp} by a test function and integrating by parts.  The inequalities enter into the derivation; see \cite[Chapter 12]{Bueler2021}, \cite{JouvetBueler2012}, or \cite{KinderlehrerStampacchia1980} for details.  The result is a single \emph{variational inequality} (VI):
\begin{equation}
  a(u,v-u) \ge \ip{f}{v-u} \quad \text{ for all } v \text{ in } \mathcal{K}. \label{eq:obstaclevi}
\end{equation}
(The bilinear form $a$ is defined in \eqref{eq:weakpoissonearly}.)  Recalling \eqref{eq:residual}, the VI can be restated using the residual functional:
\begin{equation}
  F(u)[v-u] \ge 0 \quad \text{ for all } v \text{ in } \mathcal{K}. \label{eq:obstacleviresidual}
\end{equation}

Formulations \eqref{eq:obstaclecp} and \eqref{eq:obstacleviresidual} are equivalent up to the same regularity concerns which relate the strong and weak forms of a PDE.  (See reference \cite{Evans2010} regarding the solution regularity of PDEs, and \cite{KinderlehrerStampacchia1980} for the corresponding VI theory.)  However, some intuition for VIs is needed for understanding the current paper, and so, in an attempt to help, we provide a second weak form.

Inequality \eqref{eq:obstacleviresidual} is equivalent to \emph{constrained minimization}:
\newcommand{\argmin}{\mathop{\mathrm{arg\text{-}min}}}
\begin{equation}
  u = \argmin_{w \text{ in } \mathcal{K}} I(w) \quad \text{where} \quad I(w) = \frac{1}{2} a(w,w) - \ip{f}{w}. \label{eq:obstaclemin}
\end{equation}
That is, $u$ is the minimizer, over the constraint set $\mathcal{K}$, of the scalar, \emph{coercive} quadratic \emph{objective} functional $I$.  (Coercive means that $I(w) \to +\infty$ as $\|w\|_{\mathcal{H}} \to \infty$ \cite{Evans2010}.  Note that the unconstrained minimum of $I(w)$ on $\mathcal{H}$ may lie outside $\mathcal{K}$.)  The (Gateaux) derivative of $I$ is a linear functional in $\mathcal{H}'$:
\begin{equation}
  \grad I(w)[v] = \lim_{\eps\to 0} \frac{I(w+\eps v) - I(w)}{\eps} = a(u,v) - \ip{f}{v}.  \label{eq:gradobjective}
\end{equation}
In fact $\nabla I = F$, the residual, so the VI \eqref{eq:obstacleviresidual} can re-stated yet again:
\begin{equation}
  \nabla I(u)[v-u] \ge 0 \quad \text{ for all } v \text{ in } \mathcal{K}. \label{eq:obstaclevigradient}
\end{equation}

Formulation \eqref{eq:obstaclevigradient} permits a clear, geometrical meaning.  The solution $u$ sits at a location in $\mathcal{K}$, often on the boundary, where any vector pointing into the admissible set, namely $v-u$ for $v$ in $\mathcal{K}$, points ``uphill'' on the graph of $I$.  In fact, identifying $\mathcal{H}$ and its dual space $\mathcal{H}'$, one may write \eqref{eq:obstaclevigradient} as ``$\ip{\nabla I(u)}{v-u} \ge 0$'': the angle between $\nabla I(u)$ and $v-u$ is at most 90 degrees.  See Figure \ref{fig:cartoonplane}.

The solutions shown in the earlier Figure \ref{fig:icelike} recall what points in $\mathcal{K}$ actually are, namely functions on $[0,1]$.  When a solution $u$ coincides with the obstacle $\varphi$ on a part of the domain then $u$ is on the boundary of $\mathcal{K}$.  That is, arbitrarily-small perturbations $\delta u$, from $\mathcal{H}$, will cause $u+\delta u$ to leave $\mathcal{K}$.  One should also observe that $\mathcal{K}$ is a \emph{cone} with vertex at $\varphi$.  That is, if $v$ is in $\mathcal{K}$ then $\lambda(v-\varphi) + \varphi$ is also in $\mathcal{K}$ for any $\lambda \ge 0$; the ray from $\varphi$ through $v$ is in $\mathcal{K}$.

\begin{figure}
\includegraphics[width=0.6\textwidth]{genfigs/cartoonplane.pdf}
\caption{Variational inequality \eqref{eq:obstaclevigradient} characterizes the minimum $u$ of $I(w)$ over the convex cone $\mathcal{K}=\{v\ge \varphi\} \subset \mathcal{H}$.  Any vector $v-u$, for $v$ in $\mathcal{K}$, is within $90^\circ$ of $\grad I(u)$.  (This is slightly informal; see text.)}
\label{fig:cartoonplane}
\end{figure}

The glacier problem in section \ref{sec:sia} will be formulated both as a CP like \eqref{eq:obstaclecp} and a VI like \eqref{eq:obstacleviresidual}.  However, for general bed elevation functions (obstacles) the glacier problem has no constrained minimization formulation like \eqref{eq:obstaclemin}, and thus the VI has no strict interpretation like \eqref{eq:obstaclevigradient}.  (The essential reason is the lack of a symmetry of the weak form \cite{JouvetBueler2012}.)  For the glacier problem we possess a map $F$, from $\mathcal{K}$ to $\mathcal{H}'$, and if $u$ solves the VI then $F(u)$ points perpendicularly into $\mathcal{K}$.  We may visualize the general VI form \eqref{eq:obstacleviresidual} essentially as in Figure \ref{fig:cartoonplane}, but without the contours of $I$ or the identification of $F$ as a gradient.  We will return to this point in section \ref{sec:sia}.

Regardless of how the classical obstacle problem is formulated, a \emph{free boundary} generally arises in the interior of the domain.  For example, as we move away from the midpoint in Figure \ref{fig:icelike} there are locations where the solution first becomes fully in contact with the obstacle.  At these locations both $u=\varphi$ and $u'=\varphi'$ hold, that is, the solution is tangent to the obstacle at the free boundary.  (The analogous conditions in the glacier problem are that both the ice thickness and the horizontal ice flux go to zero at a grounded glacier margin.)  These simultaneous Dirichlet and Neumann conditions occur at a location which must be found as part of the solution; they characterize the free boundary \cite[Chapter V]{KinderlehrerStampacchia1980}.

The solution of the classical obstacle problem will generally be non-smooth at a free boundary even when the data are arbitrarily smooth.  For example, smoothing the source term $f$ in \eqref{eq:icelikedetails} by ``mollification'' \cite{Evans2010}, so that the transition between positive and negative values would be $C^\infty$, would give a solution nearly the same as shown in Figure \ref{fig:icelike} (top).  However, the free boundary would remain, and at that free boundary the second derivative $u''$ would jump discontinuously from value $+16=-f$ to value $-2=\varphi''$.  For smooth $C^\infty$ data ($f,\varphi$) the obstacle problem solution $u$ is in the Sobolev space $W^{2,\infty}$, but not $C^2$ in general \cite[section IV.6]{KinderlehrerStampacchia1980}.

\subsection*{Projected Gauss-Seidel and Jacobi iterations}  Now consider the FE method for VI like \eqref{eq:obstacleviresidual}.  On each mesh level we will define an obstacle $\phi^j$ in $\mathcal{V}^j$, an admissible set $\mathcal{K}^j = \{v \ge \phi^j\} \subset \mathcal{V}^j$, and a residual function $F^j$.  (Recall $\mathcal{V}^j$ is the vector space spanned by the $j$th-level hats $\psi_p^j$; see section \ref{sec:subspace}.)  The FE method seeks $y^j$ in $\mathcal{K}^j$ so that a finite-dimensional VI holds:
\begin{equation}
  F^j(y^j)[v-y^j] \ge 0 \quad \text{ for all } v \text{ in } \mathcal{K}^j. \label{eq:feobstacleviresidual}
\end{equation}

If we were to solve on the fine-level only, i.e.~in a single-level method, then we would choose the obstacle $\phi^J$ to be the piecewise-linear interpolant of the continuum obstacle $\varphi$, i.e.~$\phi^J=\varphi^J$, and $F^J$ as the original residual $F^J(w)[\cdot] = a(w,\cdot) - \ip{f}{\cdot}$.  However, in a multilevel method the constructions of $\phi^j$ and $F^j$ are nontrivial, including on the finest level.  We will have much more to say regarding the obstacles $\phi^j$, but at least for the classical obstacle problem we will always be able to write the residual in the form $F^j(w)[\cdot] = a(w,\cdot) - \ell^j[\cdot]$ for some $\ell^j$ in $(\mathcal{V}^j)'$.

We propose an iterative method for solving \eqref{eq:feobstacleviresidual}, namely \emph{projected Gauss-Seidel} (PGS).  Supposing $w$ in $\mathcal{K}^j$ is any iterate, we modify $w$ at the $p$th node so that \eqref{eq:feobstacleviresidual} holds ``at'' that node.  Note that if $w$ is admissible then $w+c\psi_p^j$ is admissible if and only if $w_p + c \ge \phi_p$ where $\phi_p = \phi^j(x_p^j)$.  (\emph{Warning.}  This equivalence holds for piecewise-linear elements, but not for higher-order polynomial elements.)

For clarity we first describe PGS using the constrained minimization form \eqref{eq:obstaclemin}, using a convex scalar function based on the objective functional,
\begin{equation}
i(b) = I^j(w+b\psi_p^j),
\end{equation}
where $I^j(w) = \frac{1}{2} a(w,w) - \ell^j[w]$ and $b$ is a real number.  At each point $p$ we seek the minimizer over admissible perturbations,
\begin{equation}
  c = \argmin_{b \ge \phi_p - w_p} \, i(b).  \label{eq:pgsminimization}
\end{equation}
The minimum of $i(b)$ on the interval $\phi_p - w_p \le b < \infty$ occurs at the critical point $i'(b)=0$, if it is in the interval, or at the left end of the interval.  Since $i'(b) = F^j(w)[\psi_p^j] + b a(\psi_p^j,\psi_p^j)$,
\begin{equation}
  c = \max\left\{-\frac{F^j(w)[\psi_p^j]}{a(\psi_p^j,\psi_p^j)}, \phi_p - w_p\right\}  \label{eq:pgsformula}
\end{equation}
is the solution of \eqref{eq:pgsminimization}.  In other words, we compute the critical point and project it into the admissible interval.  Then we update $w \gets w + c\psi_p^j$ and proceed to the next point.

The minimization view is not essential, however, and we can derive \eqref{eq:pgsformula} using only the VI form.  Given an iterate $w$ in $\mathcal{K}^j$ we seek $c$ such that $w+c\psi_p^j$ is admissible and so that \eqref{eq:feobstacleviresidual} holds for all admissible $v=w+\tilde c\psi_p^j$.  That is, we find $c\ge \phi_p-w_p$ so that
\begin{equation}
  F^j(w+c\psi_p^j)[(w+\tilde c\psi_p^j) - (w+c\psi_p^j)] = (\tilde c - c) F^j(w+c\psi_p^j)[\psi_p^j] \ge 0,  \label{eq:pgspointwisevi}
\end{equation}
for all $\tilde c\ge \phi_p-w_p$.  Inequality \eqref{eq:pgspointwisevi} is a one-dimensional VI of form $(\tilde c - c)g(c) \ge 0$, on an interval, thus $g(c)=0$ or $c$ is at the end of the interval, thus \eqref{eq:pgsformula} when $F$ is linear.

The following pseudocode, a small modification of \pr{gs-sweep} on page \pageref{ps:gs-sweep}, implements PGS including a relaxation parameter \id{omega}.
\begin{pseudo*} \label{ps:pgs-sweep}
\pr{pgs-sweep}(j,w,F,\phi,\id{omega}=1)\text{:} \\+
    \ct{check admissibility: $w\ge \phi$} \\
    for $p=1,\dots,m_j$ \\+
        $c = -F(w)[\psi_p^j] \,\big/\, a(\psi_p^j,\psi_p^j)$ \\
        $w_p \gets \max\{w_p + \id{omega}\,c, \phi_p\}$
\end{pseudo*}

As noted in section \ref{sec:subspace}, GS-type (i.e.~multiplicative) smoothers require re-evaluation of the residual after each point update.  This is an acceptable $O(1)$ cost when the residual of an iterate at a point $x_p^j$ is computable from a few neighboring values (on the $j$th level mesh), as applies for the classical obstacle problem and the SIA-based glacier equation in section \ref{sec:sia}.  However, when the residual is nonlocal, and especially when it is both nonlocal and expensive to evaluate, as for the Stokes-based glacier model considered in section \ref{sec:stokes}, a GS-type smoother becomes less practical.  The alternative is a Jacobi-type (additive) smoother in which the vector residual is evaluated once at the beginning.  The following modifies \pr{jacobi-sweep} on page \pageref{ps:jacobi-sweep}.
\begin{pseudo*} \label{ps:pjacobi-sweep}
\pr{pjacobi-sweep}(j,w,F,\phi,\id{omega}=0.7)\text{:} \\+
    \ct{check admissibility: $w\ge \phi$} \\
    $r = F(w)[\cdot]$ \\
    for $p=1,\dots,m_j$ \\+
        $c = -r_p \,\big/\, a(\psi_p^j,\psi_p^j)$ \\
        $w_p \gets \max\{w_p + \id{omega}\,c, \phi_p\}$
\end{pseudo*}

When the evaluation of each pointwise residual $F(w)[\psi_p^j]$ is an $O(1)$ operation in the $j$th-level representation, \pr{pgs-sweep} and \pr{pjacobi-sweep} are $O(m_j)$ operations.  The latter remains $O(m_j)$ as long as the vector residual (and the diagonal entries) can be computed in $O(m_j)$ operations given $w$, even if individual pointwise residuals are not $O(1)$.  For appropriate ranges of \id{omega} both algorithms are known to converge to the solution $y^j$ of VI problem \eqref{eq:feobstacleviresidual} \cite[Proposition 4.5]{GraeserKornhuber2009}, but, of course, slowly on fine meshes.  Regarding the relaxation parameter \id{omega}, finding the most-effective smoother values is a topic for numerical experimentation.

However, the residual norm $\|F(w)\|_2$ does not go to zero (in general) at convergence of iterations for VI problems.  Recalling the CP formulation of the problem \eqref{eq:obstaclecp}, at convergence we have $F(w)[\psi_p^j] = 0$ where $w(x_p^j) > \phi(x_p^j)$ (inactive points), but if $w(x_p^j) = \phi(x_p^j)$ (active points) then $F(w)[\psi_p^j]$ is only required to be nonnegative.  Thus the convergence criterion for iterated PGS sweeps is that the norm of the \emph{CP residual}, the vector
\begin{equation}
  (\hat \bF(w))_p = \begin{cases} F(w)[\psi_p^j], & w_p > \phi_p, \\
                                  \min\{F(w)[\psi_p^j],0\}, & \text{otherwise}, \end{cases} \label{eq:cpresidual}
\end{equation}
in $\RR^{m_j}$, should be small.  Our relative-norm convergence criterion is thus
\begin{equation}
\|\hat\bF(w)\|_2 < \text{\texttt{rtol}}\,\|\hat\bF(w^0)\|_2. \label{eq:cpconvergencecriterion}
\end{equation}
For an example, see the \pr{mcdl-solver} pseudocode below.

The above projected iterations will serve as our smoothers and coarse-level solvers, applied one or two times on each level.  However, we are not yet prepared to build a multilevel method for problem \eqref{eq:feobstacleviresidual} because the nontrivial construction of the obstacles $\phi^j$ and residuals $F^j$ remains.  We need to decompose the continuum constraint $\varphi$ across the mesh-level hierarchy.

\subsection*{Multilevel constraint decomposition (MCD)}  Consider the coarse-level correction $e^{j-1}$ in a linear V-cycle (section \ref{sec:subspace}, equation \eqref{eq:coarsecorrection}), which is prolonged onto the $j$th level.  In the obstacle problem the resulting iterate ($w+Pe^{j-1}$) would need to be admissible, that is, above $\phi^j$.  However, as illustrated in Figure \ref{fig:prolongobstacle}, if the obstacles are interpolants of a (common) continuum obstacle $\varphi$ then an iterate which is admissible on the $j-1$ level is \emph{not} necessarily admissible on the $j$ level.  (In the glacier context, an admissible coarse-mesh ice surface may be breached by the fine-mesh bed topography.)  To resolve this issue we might try an ad hoc fix, namely overwriting $\tilde w = w+Pe^{j-1}$ with $\max\{\tilde w,\phi^j\}$, that is, enforce admissibility by \emph{truncation}.  However, this reintroduces high frequencies, requiring additional smoother effort to remove.  So, how do we maintain admissibility throughout the V-cycle while not introducing high-frequencies?  One way forward is the \emph{multilevel constraint decomposition} (MCD) method of Tai \cite{Tai2003}, which can be formulated as a slash-cycle \cite[Algorithm 4.7]{GraeserKornhuber2009}.

\begin{figure}
\qquad \includegraphics[width=0.75\textwidth]{genfigs/prolongobstacle.pdf}
\caption{An admissible iterate on the coarse level, when prolonged, may not be admissible on the fine level if the obstacles interpolate a common continuum obstacle.}
\label{fig:prolongobstacle}
\end{figure}

Starting with $w^J$ in $\mathcal{V}^J$ on the finest level, which is admissible in the original sense that
\begin{equation}
  w^J \ge \varphi^J, \label{eq:fineadmissibleiterate}
\end{equation}
where $\varphi^J$ is the interpolant of the continuum obstacle ($\varphi^J(x_p^J)=\varphi(x_p^J)$), we define the (nonpositive) \emph{defect constraint} \cite{GraeserKornhuber2009} of $w^J$ to be
\begin{equation}
  \chi^J = \varphi^J - w^J.  \label{eq:defectconstraint}
\end{equation}
The idea behind this definition is that if we modify $w^J$ by adding $y$ then the result is admissible if and only if $y$ is above $\chi^J$:
\begin{equation}
  w^J + y \ge \varphi^J  \qquad \iff \qquad y \ge \chi^J.  \label{eq:defectmeaning}
\end{equation}

Now our intention is to \emph{put as much as possible of $\chi^J$ into the coarsest levels.}  Corrections on the coarse levels are inexpensive, so we seek the largest corrections there.  To decompose $\chi^J$ we use a \emph{monotone restriction} operator $\mR$ from $\mathcal{V}^j$ to $\mathcal{V}^{j-1}$ \cite[equation (4.22)]{GraeserKornhuber2009}, a nonlinear operator defined as follows.  For $z$ in $\mathcal{V}^j$, $\mR z$ is computed by maximizing nodal values of $z$ over the interior of the support of coarser-level hat functions.  That is, if $z = \sum_q z_q \psi_q^j$ then
\begin{equation}
  \mR z = \sum_{p=1}^{m_{j-1}} \zeta_p \psi_p^{j-1} \qquad \text{where} \qquad \zeta_p = \max \{z_q \,:\, \psi_p^{j-1}(x_q^j) > 0\}.  \label{eq:monotonerestriction}
\end{equation}
Observe that $\mR z \ge z$ and that $\mR$ acts on functions $\mathcal{V}^j$ while canonical restriction $R$ acts on linear functionals $(\mathcal{V}^j)'$; see Table \ref{tab:restrictionsprolongations}.

We then define the \emph{$j$th-level defect constraint} inductively by
\begin{equation}
  \chi^{j-1} = \mR \chi^j  \label{eq:chik}
\end{equation}
for $j=J$ down to $j=1$.  These defect constraints have two key properties: $\chi^j$ is in $\mathcal{V}^j$ and $\chi^j \le \chi^{j-1}$.  (Formally one might write $\chi^j \le P \chi^{j-1}$, using the canonical prolongation operator, so as to compare functions in the same space.)  Now the \emph{$j$th-level obstacle} is the difference:
\begin{equation}
  \phi^j = \chi^j - \chi^{j-1} \quad \text{ for } j=0,1,\dots,J,  \label{eq:levelobstacle}
\end{equation}
where we also define $\chi^{-1}=0$ so that $\phi^0 = \chi^0$.  Note $\phi^j$ is in $\mathcal{V}^j$ and $\phi^j\le 0$.

%REGENERATE Figures \ref{fig:gooddecomposition} and \ref{fig:icelikedecomposition}:
%$ ./obstacle.py -jfine 5 -jcoarse 1 -irtol 1.0e-7 -random -randommodes 8 -diagnostics -o defect.pdf
%fine level 5 (m=63) using 52 V(1,0) cycles (102.375 WU)
%...
%saving hierarchical decomposition to decomp_defect.pdf ...
%saving "ice-like" decomposition to icedec_defect.pdf ...
\begin{figure}
\includegraphics[width=0.75\textwidth]{fixfigs/decomp_defect.pdf}
\caption{The MCD method writes a fine-level defect constraint $\chi^J = \varphi^J - w^J$, with $J=4$ shown, as a sum of obstacles $\phi^j = \chi^j - \chi^{j-1}$ on each level.}
\label{fig:gooddecomposition}
\end{figure}

We have now decomposed the defect constraint $\chi^J$ via a ``telescoping'' sum:
\begin{equation}
  \sum_{j=0}^J \phi^j = \chi^0 + (\chi^1 - \chi^0) + (\chi^2 - \chi^1) + \dots + (\chi^J - \chi^{J-1}) = \chi^J.  \label{eq:telescopingdecomposition}
\end{equation}
An example is shown in Figure \ref{fig:gooddecomposition}; compare Figure 1 in \cite{GraeserKornhuber2009}.  The obstacles $\phi^j$ are the gaps between the plotted defect constraints $\chi^j$.

From the obstacles $\phi^j$ we define closed, convex \emph{constraint sets}
\begin{equation}
\mathcal{K}^j = \left\{v \ge \phi^j\right\} \subseteq \mathcal{V}^j \label{eq:defineKj}
\end{equation}
for $j=0,1,\dots,J$.  Because $\phi^j \le 0$, the zero function is admissible on every level.  On the finest level, note that $\phi^J$ is not the same as $\varphi^J$, and so $\mathcal{K}^J$ does not approximate $\mathcal{K}$.  Instead, as suggested by Figure \ref{fig:gooddecomposition}, if the $J$th level is a high-resolution mesh and the defect constraint $\chi^J$ is smooth then $\phi^J\approx 0$, because $\chi^J$ is well-approximated on the $J-1$ level.

We also define the fine-level \emph{defect constraint set}
\begin{equation}
  \mathcal{D}^J = \left\{v \ge \chi^J\right\} \subset \mathcal{V}^J.
\end{equation}
The sets $\mathcal{K}^j$ decompose $\mathcal{D}^J$:
\begin{equation}
  \mathcal{D}^J = \mathcal{K}^0 + \mathcal{K}^1 + \dots + \mathcal{K}^J. \label{eq:constraintdecomposition}
\end{equation}
(The proof uses the telescoping sum \eqref{eq:telescopingdecomposition}.)  That is, every admissible modification of the current fine-mesh iterate, i.e.~every $y$ from $\mathcal{V}^J$ for which $y\ge \chi^J$, can be built by choosing a function from each of the mesh-level constraint sets $\mathcal{K}^j$.  As with \eqref{eq:subspacedecomposition}, where the subspaces $\mathcal{V}^j$ decompose the FE space $\mathcal{V}^h$, this representation is usually not unique.  While our specific choices follow \cite{GraeserKornhuber2009} in determining $\mathcal{K}^j$, a subset decomposition like \eqref{eq:constraintdecomposition} is fundamental to any application of the MCD method in \cite{Tai2003}.

\begin{figure}
\includegraphics[width=0.65\textwidth]{genfigs/innerconeapprox.pdf}

\caption{The MCD method approximates the fine-level cone $\mathcal{D}^J = \{v\ge \chi^J\}$ from inside, using coarse-level constraint cones $\mathcal{K}^0+\dots+\mathcal{K}^j$.}
\label{fig:innerconeapprox}
\end{figure}

As shown in Figure \ref{fig:innerconeapprox}, decomposition \eqref{eq:constraintdecomposition} can be regarded as nested cones:
\begin{equation}
  \mathcal{K}^0 \subset \mathcal{K}^0 + \mathcal{K}^1 \subset \dots \subset \mathcal{K}^0 + \mathcal{K}^1 + \dots + \mathcal{K}^j \subset \dots \subset \mathcal{D}^J.  \label{eq:nestedcones}
\end{equation}
The defect constraint set $\mathcal{D}^J$ is approximated ``from within'' by the coarse level cones $\sum_{k=0}^j \mathcal{K}^k = \{v \ge \chi^j\} \subset \mathcal{V}^j$.

Looking forward to the glacier problem in section \ref{sec:sia}, the fine-mesh obstacle $\varphi^J$ will be the bed elevation, $w^J$ will be a candidate ice surface elevation, and $-\chi^J$ will be the corresponding ice thickness.  With this in mind, Figure \ref{fig:icelikedecomposition} illustrates the same constraint decomposition as in Figure \ref{fig:gooddecomposition}, but pictured as a decomposition of the ice into layers.  (Compared to Figure \ref{fig:icelike} (top), the bed topography $\varphi^J$ here is bumpy.)  However, Figure \ref{fig:gooddecomposition} plots the decomposition in the \emph{correct} sense, so that defect constraints $\chi^j$ and obstacles $\phi^j$ are piecewise-linear.

\begin{figure}
\includegraphics[width=0.7\textwidth]{fixfigs/icedec_defect.pdf}
\caption{A heuristic view: MCD decomposes the ``ice'' between a fine-mesh iterate $w^J$ and the fine-mesh obstacle $\varphi^J$.}
\label{fig:icelikedecomposition}
\end{figure}

\subsection*{MCD coarse-level corrections}  On each coarse level there is an obstacle problem, and we write this problem in three forms.  For the classical obstacle problem the forms are equivalent, but only one will generalize to glacier-geometry problems.  Recall that $F(w)[v] = a(w,v) - \ip{f}{v}$ denotes the continuum residual \eqref{eq:residual}, and $I(v) = \frac{1}{2} a(v,v) - \ip{f}{v}$ is the corresponding objective function.

Suppose $w^J$ is an admissible iterate on the fine level: $w^J\ge \varphi^J$.  Let us assume that we have already descended from the fine level $J$ to some level $j$, computing admissible corrections $y^k$ from each constraint set $\mathcal{K}^k=\{v\ge \phi^k\}$, for $k\ge j$, as we go.  Because of \eqref{eq:defectmeaning} and decomposition \eqref{eq:constraintdecomposition}, $z = w^J+y^J+\dots+y^j$ is admissible in the original sense, i.e.~$z\ge \varphi^J$.

In an MCD method the coarse-level correction is a function $y$ in $\mathcal{K}^{j-1}$, i.e.~$y \ge \phi^{j-1}$, such that one of the following holds:
\begin{itemize}
\item variational inequality:
\begin{equation}
  F(w^J+y^J+\dots+y^j+y)[v - y] \ge 0 \qquad \text{for all } v \text{ in } \mathcal{K}^{j-1}.  \label{eq:mcdvi}
\end{equation}
\item constrained minimization:
\begin{equation}
  y = \argmin_{v \text{ in } \mathcal{K}^{j-1}} I(w^J+y^J+\dots+y^j+v).  \label{eq:mcdminimization}
\end{equation}
\item linear variational inequality:
\begin{equation}
  F^{j-1}(y)[v - y] \ge 0 \qquad \text{for all } v \text{ in } \mathcal{K}^{j-1}.   \label{eq:mcdvilinear}
\end{equation}
\end{itemize}
Whichever form is used, the correction problem is defined for $j-1=0,\dots,J$, that is, we compute a correction $y$ in $\mathcal{K}^{j-1}$ on every level in the hierarchy including the finest ($j-1=J$ so $j=J+1$ in the above forms) and the coarsest ($j-1=0$).

The third form \eqref{eq:mcdvilinear} exploits linearity to inductively define a coarse-level residual functional which does not refer to the fine-level admissible state $w^J+y^J+\dots+y^j+y$:
\begin{align}
  F^{j-1}(y)[\cdot] &= F(w^J+y^J+\dots+y^j+y)[\cdot] \label{eq:residuallinearlevel} \\
                    &= a(w^J+y^J+\dots+y^j+y,\cdot) - \ip{f}{\cdot} \notag \\
                    &= a(y,\cdot) + F^j(y^j)[\cdot]. \notag
\end{align}
Equivalently, we construct a coarse-level source term,
\begin{equation}
  \ell^{j-1}[\cdot] = \begin{cases} - F^j(y^j)[\cdot], & j < J, \\
                                    - F(w^J)[\cdot],   & j = J, \end{cases} \label{eq:rhslinearlevel}
\end{equation}
and then define $F^{j-1}(y)[\cdot] = a(y,\cdot) - \ell^{j-1}[\cdot]$.  While $F^{j-1}$ is in fact defined on $\mathcal{V}^h$, that is, on the fine-mesh FE space, and $\ell^{j-1}$ is in $(\mathcal{V}^h)'$, in our implemented multigrid cycle (below) smoothing occurs before each coarse correction.  Thus, using canonical restriction we will approximate $\ell^{j-1}$ by coarse-level linear functional in $(\mathcal{V}^{j-1})'$.

For the classical obstacle problem, wherein the (continuum) residual $F(w)[v]$ is built from a symmetric bilinear form $a(w,v)$, the three problems \eqref{eq:mcdvi}--\eqref{eq:mcdvilinear} are equivalent:
   $$\eqref{eq:mcdvilinear} \quad \stackrel{a \text{ bilinear}}{\iff\strut} \quad \eqref{eq:mcdvi} \quad \stackrel{F=\grad I}{\iff\strut} \quad \eqref{eq:mcdminimization}.$$
The left equivalence depends on linearity and the right on symmetry.  The nonlinear VI form \eqref{eq:mcdvi} is thus the most general.  On the other hand, in the general situation where $F$ is not the gradient of a coercive objective function, as in the SIA and Stokes glacier problems, additional structural properties of $F$ are required for well-posedness (e.g.~monotonicity \cite{Bueler2020,JouvetBueler2012,KinderlehrerStampacchia1980}).  In summary, we will solve the general VI form \eqref{eq:mcdvi} for general-bed glacier problems.

\subsection*{MCD cycles and convergence results}  For now we are may implement a multilevel solver for the classical obstacle problem.  Because the problem is linear, we solve  \eqref{eq:mcdvilinear}, the form special to linear problems, and call the method \pr{MCDL}, with L for ``linear''.

Our slash cycle, also known as a V(1,0) cycle \cite{GraeserKornhuber2009}, is based on formulae \eqref{eq:chik}, \eqref{eq:levelobstacle}, \eqref{eq:residuallinearlevel}, and \eqref{eq:rhslinearlevel}.  (A slash cycle is shown in Figure \ref{fig:fcycle} below.)  As already described, before computing the coarse-level correction, after smoothing we approximate the residual by applying canonical restriction, thus $\ell^{j-1}[\cdot] = - R(F^j(y^j))[\cdot]$, which loses any remaining residual information for high-frequency modes on the $j$th level.

The following pseudocode uses \pr{pgs-sweep} or \pr{pjacobi-sweep} as a smoother and coarse-level solver.  It takes $F^J$ as an argument, and not the original $F$; it only knows about the fine-level iterate $w^J$ except through $F^J$ and $\chi^J$.
\begin{pseudo*} \label{ps:mcdl-slash}
\pr{mcdl-slash}(F^J,\chi^J,\id{down}=1,\id{coarse}=1)\text{:} \\+
    for $j=J$ downto $j=1$ \\+
      $\chi^{j-1} = \mR \chi^j$ \\
      $\phi^j = \chi^j - P\chi^{j-1}$ \qquad\qquad\qquad\quad \ct{define obstacle} \\
      $y^j = 0$ \\
      $\text{\pr{smoother}}^{\text{\id{down}}}(j,y^j,F^j,\phi^j)$ \qquad\quad \ct{down smoother} \\
      $\ell^{j-1}[\cdot] := - R (F^j(y^j))[\cdot]$ \qquad\qquad \ct{update and restrict residual} \\
      $F^{j-1}(y)[\cdot] := a(y,\cdot) - \ell^{j-1}[\cdot]$ \\-
    $y^0 = 0$ \\
    $\text{\pr{smoother}}^{\text{\id{coarse}}}(0,y^0,F^0,\chi^0)$ \qquad\quad \ct{coarsest-level correction} \\
    for $j=1$ to $j=J$ \\+
      $y^j \gets y^{j} + P y^{j-1}$ \qquad\qquad\qquad \ct{prolong and accumulate corrections} \\-
    return $y^J$
\end{pseudo*}

In the implementation, the functions $F^j(y)[\cdot]$ are not passed as functions in the pro\-gramming-language sense.  Rather, assuming a subroutine which evaluates $a(w,v)$ as needed on each level, the linear functional $\ell^{j-1}$ is passed, from which $F^{j-1}$ is constructed.  The actual implementation signatures are \pr{smoother}$(j,y^j,\ell^j,\phi^j)$ and \pr{mcdl-slash}$(J,\ell^J,\chi^J)$.

The following solver adds an outer loop which sets-up the fine-level defect constraint $\chi^J$, sets-up $F^J$, calls the cycle, and checks for convergence.  Compare \pr{msd-solver} in section \ref{sec:subspace}.
\begin{pseudo*} \label{ps:mcdl-solver}
\pr{mcdl-solver}(F,\varphi,\id{rtol}=10^{-4})\text{:} \\+
    $w=\varphi$ \qquad\qquad\qquad\qquad\qquad\quad \ct{or other admissible fine-level initial iterate} \\
    $r_0=\|\hat\bF(w)\|$ \qquad\qquad\qquad\qquad\qquad \ct{initial CP residual norm; see \eqref{eq:cpresidual}} \\
    repeat \\+
        $\chi^J = \varphi - w$ \qquad\qquad\qquad\qquad\quad \ct{fine-level defect constraint} \\
        $\ell^J[\cdot] := - F(w)[\cdot]$ \qquad\qquad\qquad\quad \ct{see \eqref{eq:rhslinearlevel}} \\
        $F^J(y)[\cdot] := a(y,\cdot) - \ell^J[\cdot]$ \\
        $w\gets w+\pr{mcdl-slash}(F^J,\chi^J)$ \\-
    until $\|\hat\bF(w)\| \le \id{rtol} \, r_0$  \qquad\qquad\qquad \ct{see \eqref{eq:cpresidual}} \\
    return $w$
\end{pseudo*}

These pseudocodes have been implemented in Python.\footnote{At\, \href{https://github.com/bueler/mg-glaciers/}{\texttt{github.com/bueler/mg-glaciers/}} see the \texttt{py/1D/} directory and its \texttt{README.md}.}  Recall the two exact solutions $u$ shown in Figure \ref{fig:icelike}.  (The ``ice-like'' case has a discontinuous source $f$, while ``traditional'' has $f=0$, but both have smooth, parabolic obstacles.)  Figure \ref{fig:convergence} demonstrates convergence of \pr{mcdl-solver}, with the numerical error norm $\|u^h-u\|_2$ as a function of mesh spacing $h$, for runs with \id{rtol} $=10^{-7}$.  The ``traditional'' convergence rate is the best possible; $O(h^2)$ error is expected for the unconstrained problem \cite{Elmanetal2014}.  The ``ice-like'' rate is reduced because of the lower regularity of $f$.

\begin{figure}
\includegraphics[width=0.6\textwidth]{genfigs/convergence.pdf}
\caption{Convergence of \pr{mcdl-solver} for the two cases shown in Figure \ref{fig:icelike}.}
\label{fig:convergence}
\end{figure}

\subsection*{Nested iteration and performance results}  Performance can be improved by exploiting additional multilevel techniques.  One of these is \emph{nested iteration}, where the initial iterate on a given mesh comes from prolonging (interpolating) a solution iterate from a coarser mesh.  Figure \ref{fig:fcycle} shows the resulting \emph{F-cycle}, constructed by concatenating slash cycles with an additional prolongation action.

\begin{figure}
\input{tikz/fcycle.tex}
\caption{\emph{Left:} A four-level slash cycle with down-smoother (dots), coarse solver (square), and accumulation of corrections back to the finest level (gray line).  \emph{Right:} A nested iteration F-cycle prepends shorter slash cycles, with solution prolongation $\hat P$ (double lines) for the initial iterate in the next cycle.}
\label{fig:fcycle}
\end{figure}

Only the update $y^{j-1}$ is prolonged in the \pr{mcdl-slash} cycle, now we need to prolong the full solution $w^{j-1}$ from $\mathcal{V}^{j-1}$, admissible in the sense that $w^{j-1} \ge \varphi^{j-1}$, where $\varphi^{j-1}$ is the interpolant of the continuum obstacle $\varphi$.  However, the new iterate also needs to be admissible.  Any prolongation onto the $j$th level will have to contend with not-yet-seen obstacle data in $\varphi^j$, and thus truncation is obligatory.  Thus we define \emph{solution prolongation} using canonical prolongation \eqref{eq:canonicalprolongation} and truncation:
\begin{equation}
\hat P w^{j-1} = \max\{P w^{j-1}, \varphi^{j}\}  \label{eq:solutionprolongation}
\end{equation}

In our F-cycle we do one or more slash cycles before moving up to the next-finer level.  The resulting algorithm can be used as is, or to generate the initial iterate for \pr{mcdl-solver}, that is, one may do additional slash cycles on the output of this pseudocode.
\begin{pseudo*} \label{ps:mcdl-fcycle}
\pr{mcdl-fcycle}(J,F,\varphi,\id{coarse}=1,\id{nicycles}=1)\text{:} \\+
    $w^0=\varphi^0$ \qquad\qquad\qquad\qquad\qquad\quad \ct{admissible coarsest-level initial iterate} \\
    $\ell^0[\cdot] := \ip{f}{\cdot}$ \\
    $F^0(y)[\cdot] := a(y,\cdot) - \ell^0[\cdot]$ \\
    $\pr{smoother}^{\id{coarse}}(0,w^0,F^0,\varphi^0)$ \\
    for $j=1,\dots,J$ \\+
        $w^j = \hat P w^{j-1}$ \qquad\qquad\qquad\qquad \ct{initial iterate from solution prolongation} \\
        for $s=1,\dots,\id{nicycles}$ \qquad\qquad\qquad \ct{one or more slash cycles} \\+
            $\chi^j = \varphi^j - w^j$ \\
            $\ell^j[\cdot] := \ip{f}{\cdot}$ \\
            $F^j(y)[\cdot] := a(y,\cdot) - \ell^j[\cdot]$ \\
            $w^j \gets w^j+\pr{mcdl-slash}(F^j,\chi^j)$ \\--
    return $w^J$
\end{pseudo*}


FIXME for performance test we consider icelike-with-random-bed problem; we apply \pr{mcdl-solver}, and do slash cycles until convergence, i.e.~\eqref{eq:cpconvergencecriterion}, with \id{rtol} $=10^{-4}$

FIXME 4 solvers: slash cycle V(1,0), V(2,0), prepend F-cycle, and prepend 2-iterations-on-way-up-F-cycle

FIXME Figure showing number of cycles; note irregularity, which relates to position of free boundary unavoidably ``jumping'' as we move levels; F-cycles are less powerful than in PDE cases because multilevel cycles tasked with both ``find the free boundary'' and ``smooth low frequencies'' jobs

FIXME explain WU; for ideal PDE case like Poisson, WU are $O(1)$ (though actually slightly increasing); Figure \ref{fig:mcdl-wu} shows WU for \pr{mcdl-solver}; result here: WU may be increasing a bit more

\begin{figure}
\includegraphics[width=0.6\textwidth]{genfigs/mcdl-wu.pdf}
\caption{Work units (see text) as a function of degrees of freedom $m$ for runs of \pr{mcdl-solver} using four MCD algorithms (see text).}
\label{fig:mcdl-wu}
\end{figure}

FIXME compare $J=6,7,8,9$ results with PGS only

FIXME Figure \ref{fig:mcdl-timeper} shows time per $m$; for same 4 solvers and problem

\begin{figure}
\includegraphics[width=0.6\textwidth]{genfigs/mcdl-timeper.pdf}
\caption{Run time per degrees of freedom $m$ as a function of $m$ for the same four MCD algorithms.}
\label{fig:mcdl-timeper}
\end{figure}

FIXME mention V(1,1) in Appendix B, \cite{Blumetal2004} on cascadic and CG variants

\subsection*{Convergence theory and performance models}  Mathematical theory allows us to compare the convergence rates of our MCD solver versus the PGS method.  Let $u^h$, in $\mathcal{V}^h = \mathcal{V}^J$ and satisfying $u^h \ge \varphi^J$, be the exact solution of the finite-dimensional VI
\begin{equation}
  F(u^h)[v-u^h] \ge 0 \quad \text{ for all } v \text{ in $\mathcal{V}^h$ such that } v \ge \varphi^J. \label{eq:feobstaclevioriginal}
\end{equation}
This is the direct FE approximation of \eqref{eq:obstacleviresidual}; compare \eqref{eq:feobstacleviresidual} which uses the decomposition of the defect constraint.  Recall that $F=\grad I$ for an objective function $I(w)$ (see \eqref{eq:obstaclemin}).  Denote the spacing of the fine ($J$th) mesh by $h$.

First consider the PGS method.  Note that the following theorem describes the norm of the algebraic error of an iterate, i.e.~$\|w-u^h\|_{\mathcal{H}}$, relative to the objective value on the initial iterate.

\begin{theorem} \cite[Prop.~4.5]{GraeserKornhuber2009}\,  \label{thm:pgsconvergence}  Suppose $w^{(s)}$  results from $s$ applications of \pr{pgs-sweep} on the fine level, starting with any $w^{(0)}$.  There is $C>0$ independent of $h$ so that
\begin{equation}
  \|w^{(s)} - u^h\|_{\mathcal{H}}^2 \le 2 (1-C h^2)^s\,\left(I(w^{(0)}) - I(u^h)\right).  \label{eq:pgsconvergence}
\end{equation}
\end{theorem}

Thus the asymptotic convergence rate at which $\|w^{(s)} - u^h\|_{\mathcal{H}} \to 0$ is $O(\rho_{\text{PGS}}^s)$ where
    $$\rho_{\text{PGS}} = \sqrt{1-Ch^2} = 1 - O(h^2).$$
Thus as we refine and $h\to 0$ the value of $\rho_{\text{PGS}}$ goes to one.  In fact if we seek to reduce the error norm below $\eps$ times its original value then we expect to need $O(|\log\eps|/h^2)$ iterations.  The number of iterations increases rapidly as $h\to 0$.

\begin{theorem} \cite[Thm.~4.6]{GraeserKornhuber2009}\,  \label{thm:mcdlconvergence}  FIXME Suppose $w^{(s)}$  results from $s$ applications of \pr{mcdl-slash} on the fine level, starting with any $w^{(0)}$.  There is $\rho<1$ so that
\begin{equation}
  \|w^{(s+1)} - u^h\|_{\mathcal{H}} \le \rho \|w^{(s)} - u^h\|_{\mathcal{H}}.  \label{eq:mcdlconvergence}
\end{equation}
\end{theorem}

FIXME Compare Theorem \ref{thm:msdconvergence}; model performance when $h=O(2^{-J})$ and $N=2^{J+1}-1=O(2^J)$ so $J = O(\log N)$; Table \ref{tab:performancemodels}; all constants proportional to $|\log\eps|$

\begin{table}
\begin{tabular}{l|l|l}
\emph{method} & \emph{iterations} & \emph{work} \\ \hline
\pr{pgs-sweep}s & $O(N^2)$ & $O(N^3)$ \\
\pr{mcdl-solver} & $O(\log N)$ & $O(N \log N)$ \\ \hline
\pr{gmg-vcycle} & $O(1)$ & $O(N)$
\end{tabular}

\medskip
\caption{Number of iterations to reduce the error norm $\|w-u^h\|_{\mathcal{H}}$ by a factor of $\eps$, and amount of work (flops) as degrees of freedom $N\to\infty$.}
\label{tab:performancemodels}  % \label must be last for correct reference
\end{table}

FIXME in Table \ref{tab:performancemodels}, the first two solvers are for the classical obstacle problem, but \pr{gmg-vcycle} solves the linear Poisson equation


\section{Multigrid for the shallow-ice mass conservation problem} \label{sec:sia}

FIXME model problem in previous section has the wrong ``physics'' but correctly addresses the free boundary and obstacle nature of the glacier problem; glaciers as obstacle problems introduced by \cite{Calvoetal2002}; extended by \cite{Bueler2016,Bueler2020,JouvetBueler2012}, but these do not use multigrid; multigrid is applied to SIA in \cite{Jouvetetal2013,JouvetGraeser2013} with similar algorithm; we apply MCD because of its geometric clarity; there are other fast solution methods, for which theory is incomplete also, e.g.~PFAS in \cite{BrandtCryer1983} and inactive-set Newton-Multigrid \cite[Chapter 12]{Bueler2021}, and monotone and truncated multigrid \cite{GraeserKornhuber2009}

FIXME SIA model equations \cite{Bueler2016} with constants from \cite{Huybrechtsetal1996}; in this section we consider the steady-state problem; an 1D exact solution is Bueler profile from \cite{vanderVeen2013} section 5.3, Figure \ref{fig:siadatafigure}

FIXME SIA modifications to smoother in 1D case

FIXME 1D results

\begin{figure}
\includegraphics[width=0.7\textwidth]{fixfigs/siadatafigure.pdf}
\caption{\emph{Top:} Exact geometry solution for a flat-bed ice sheet.  \emph{Bottom:} Surface mass balance rate, the source term.}
\label{fig:siadatafigure}
\end{figure}

FIXME for 2D domains use Firedrake


\section{Multigrid for Glen-Stokes glacier flow} \label{sec:stokes}

FIXME multigrid already used for Blatter-Pattyn model \cite{BrownSmithAhmadia2013}; for hybrid \cite{Jouvetetal2013,JouvetGraeser2013}; for Stokes \cite{IsaacStadlerGhattas2015} and \cite{Tuminaroetal2016} using AMG; one goal of this section is to make these approaches more understandable

FIXME obstacle problem view extended to Stokes by \cite{WirbelJarosch2020}

FIXME we use Schur complement \cite{Bueler2021,Elmanetal2014} and compare it to Vanka monolithic smoother \cite{Farrelletal2019}

FIXME time-dependent runs


\small

\bigskip
\bibliography{review}
\bibliographystyle{siam}

\normalsize

%\clearpage
\appendix

\section{Tables to assist the reader}

This review has been written attempting to use the simplest effective notation, but the amount of notation is still substantial.  Furthermore a number of new algorithms are discussed.  Tables \ref{tab:notation} and \ref{tab:pseudocodes}, of notation and pseudocodes respectively, both of which are arranged alphabetically where possible, should help in managing the terminology.

\renewcommand{\arraystretch}{1.2}
\begin{longtable}{l|l}
\toprule
\textbf{Symbol} {\Large$\strut$} & \textbf{Meaning} \\ \hline
$a(\cdot,\cdot)$ & bilinear form associated to the Poisson problem; left side of equation \eqref{eq:weakpoissonearly} \\
$\mathcal{D}^J$ & defect constraint set; $\mathcal{D}^J = \{v \ge \chi^J\}$ \\
$F(w)[\cdot]$ & residual of iterate $w$, e.g.~$F(w)[v] = a(w,v) - \ip{f}{v}$ for the Poisson equation \\
$\mathcal{H}$ & Hilbert space for the continuum problem; e.g.~$\mathcal{H}=H_0^1[0,1]$ \\
$I(w)$ & scalar-valued objective function for the Poisson problem; $I(w) = \frac{1}{2} a(v,v) - \ip{f}{v}$ \\
$J$ & level index of finest mesh \\
$j$ & index of mesh level; $j=0,1,\dots,J$ from coarse to fine \\
$\mathcal{K}$ & continuum constraint set; $\mathcal{K} = \{v \ge \varphi\} \subset \mathcal{H}$ \\
$\mathcal{K}^j$ & $j$th-level admissible functions; $\mathcal{K}^j = \{v \ge \phi^j\} \subset \mathcal{V}^j$ \\
$\ell[\cdot]$ & linear functional, e.g.~$\ell[v] = \ip{f}{v}$ \\
$m_j$ & number of nodes in $j$th-level; $\dim \mathcal{V}^j=m_j$ \\
$N$ & number of degrees of freedom in discretized problem \\
$P$ & canonical prolongation of functions, $\mathcal{V}^{j-1} \to \mathcal{V}^j$; see \eqref{eq:canonicalprolongation} and Table \ref{tab:restrictionsprolongations} \\
$R$ & canonical restriction of linear functionals, $(\mathcal{V}^j)' \to (\mathcal{V}^{j-1})'$; see \eqref{eq:canonicalrestriction} and Table \ref{tab:restrictionsprolongations} \\
$\mR$ & monotone restriction of functions, $\mathcal{V}^j \to \mathcal{V}^{j-1}$; see \eqref{eq:monotonerestriction} and Table \ref{tab:restrictionsprolongations} \\
$\mathcal{V}^h$ & finite element function space; $= \mathcal{V}^J$ \\
$\mathcal{V}^j$ & $j$th-level vector space \\
$(\mathcal{V}^j)'$ & dual space (linear functionals) of $\mathcal{V}^j$  \\
$x_p^j$ & $p$th node on $j$th-level mesh \\
$\varphi(x)$ & obstacle in continuum problem \\
$\varphi^J(x)$ & fine-level interpolant of continuum obstacle \\
$\phi^j(x)$ & $j$th-level obstacle in MCD method; $\phi^j=\chi^j - P\chi^{j-1}$ \\
$\chi^J(x)$ & fine-level defect obstacle; $\chi^J = \varphi^J - w^J$ for iterate $w^J$ \\
$\chi^j(x)$ & $j$th-level (monotone) restriction of defect obstacle; $\chi^{j-1} = \mR \chi^j$ \\
$\psi_p^j(x)$ & $j$th-level hat function at $x_p$ \\
$\ip{\cdot}{\cdot}$ & $L^2$ inner product \\
$\|\cdot\|$ & $L^2$ norm; $\|f\|=\ip{f}{f}^{1/2}$ \\
$\|\cdot\|_{\mathcal{H}}$ & norm on solution space; $\|f\|_{\mathcal{H}}^2 =\ip{f}{f} + \ip{f'}{f'}$ for $\mathcal{H}=H_0^1[0,1]$ \\  % final \\ required
\bottomrule
\caption{Notation.}
\label{tab:notation}
\end{longtable}

\renewcommand{\arraystretch}{1.1}
\begin{longtable}{l|l|l}
\toprule
\textbf{Name} {\Large$\strut$} & \textbf{Page} & \textbf{Description} \\ \hline
\pr{euler-timestep} & \pageref{ps:euler-timestep} & forward Euler as a smoother iteration \\
\pr{gmg-coarsesolve} & \pageref{ps:gmg-coarsesolve} & geometric multigrid (GMG) coarse-level solver \\
\pr{gmg-vcycle} & \pageref{ps:gmg-vcycle} & GMG V-cycle \\
\pr{gs-sweep} & \pageref{ps:gs-sweep} & Gauss-Seidel (GS) iteration \\
\pr{jacobi-sweep} & \pageref{ps:jacobi-sweep} & Jacobi iteration \\
\pr{msd-slash} & \pageref{ps:msd-slash} & multilevel subspace decomposition (MSD) slash-cycle; \\
  &  & \qquad calls \pr{[gs$|$jacobi]-sweep} as smoother \\
\pr{msd-solver} & \pageref{ps:msd-solver} & MSD solver; calls \pr{msd-[slash$|$vcycle]} \\
\pr{msd-vcycle} & \pageref{ps:msd-vcycle} & MSD V-cycle \\ \hline
\pr{mcdl-slash} & \pageref{ps:mcdl-slash} & multilevel constraint decomposition (MCD) slash-cycle \\
  &  & \qquad in linear case; calls \pr{p[gs$|$jacobi]-sweep} as smoother \\
\pr{mcdl-solver} & \pageref{ps:mcdl-solver} & MCD solver; calls \pr{mcdl-slash} \\
\pr{pgs-sweep} & \pageref{ps:pgs-sweep} & projected GS iteration \\
\pr{pjacobi-sweep} & \pageref{ps:pjacobi-sweep} & projected Jacobi iteration \\ % final \\ required
\bottomrule
\caption{Alphabetical lists of pseudocodes.  Above the line are PDE methods, and below are obstacle-problem methods.}
\label{tab:pseudocodes}
\end{longtable}


\section{Mysterious V(1,1) cycles}  In section \ref{sec:obstacle} we implemented a ``slash cycle'', also known as a V(1,0) cycle because we only do smoothing downward in the mesh hierarchy.  The implementation of a true V-cycle for an obstacle problem, a so-called V(1,1)-cycle \cite{GraeserKornhuber2009} with smoothing after the coarse-level correction, requires reconsideration of the constraint decomposition and an additional prolongation operation.  As always, each correction from a different level needs to yield an admissible iterate, and in a V-cycle this applies on the way up, too.

Gr\"aser and Kornhuber \cite{GraeserKornhuber2009} implement the constraint decomposition in a particular manner which we have implemented, but only unsuccessfully regarding convergence.  They keep the $j$th-level defect constraints $\chi^j$ \eqref{eq:chik} the same, but split the $j$th-level obstacles $\phi^j$ in half and then apply them twice, yielding the following formulae for $j=1,\dots,J$:
\begin{equation}
\phi_V^j = \frac{1}{2}(\chi^j-\chi^{j-1}), \qquad \mathcal{K}_V^j = \{v \ge \phi_V^j\} \subset \mathcal{V}^j.
\end{equation}
The coarsest-level obstacle remains a special case, $\phi_V^0 = \chi^0,\mathcal{K}_V^0 = \{v \ge \chi^0\}$.  A telescoping sum like \eqref{eq:telescopingdecomposition}, but now with negative indices for going down and positive for going up, retains a consistent decomposition \cite{GraeserKornhuber2009,Tai2003}:
\begin{equation}
\chi^J = \sum_{j=-J}^J \phi_V^{|j|}, \qquad \mathcal{D}^J = \sum_{j=-J}^J \mathcal{K}_V^{|j|}.
\end{equation}
On the way up we denote the corrections by $\eta^j$.  Computation \eqref{eq:residuallinearlevel} is thus extended with additional terms:
\begin{equation}
  \tilde F^j(z)[\cdot] = F(w^J + y^J + \dots + y^0 + \eta^1 + \dots + \eta^{j-1} + z)[\cdot] = a(z,\cdot) - \tilde \ell^j[\cdot] \label{eq:residuallinearlevelv}
\end{equation}
where $\tilde\ell^j[\cdot] = - \tilde F^{j-1}(\eta^{j-1})[\cdot]$ for $j>1$ and $\tilde\ell^1[\cdot] = - F^0(y^0)[\cdot]$.  However, to use $\tilde\ell^j$ on the $j$th level, in a representation $\tilde{\bm{\ell}} = \{\tilde\ell^j[\psi_q^j]\}$ as a vector in $\RR^{m_j}$, it must be prolonged.  (This is the first time we have prolonged a linear functional; see Table \ref{tab:restrictionsprolongations}.)  Suppose $\ell$ is in $(\mathcal{V}^{j-1})'$ and that we want to compute its prolongation $\lambda$, a linear functional in $(\mathcal{V}^j)'$.  Recalling \eqref{eq:hatcombination} once again, we need to determine the values $\lambda[\psi_q^j]$ so that
\begin{equation}
  \sum_{q=1}^{m_j} c_{pq} \lambda[\psi_q^j] = \ell[\psi_p^{j-1}] \label{eq:dualprolongderive}
\end{equation}
for $p=1,\dots,m_{j-1}$, where $c_{pq} = \psi_p^{j-1}(x_q^j)$.  Equation \eqref{eq:dualprolongderive} is an underdetermined system of $m_{j-1}$ equations in $m_j$ unknowns, and it is exactly-solvable in many different ways.  However, noting that $c_{pq}=1$ if $x_q^j=x_p^{j-1}$, we may choose $\lambda[\psi_q^j] = \ell[\psi_p^{j-1}]$ when the nodes coincide, and choose the other values to be zero, and this yields \emph{injection prolongation},
\begin{equation}
  (\iP\ell)[\psi_q^j] = \begin{cases} \ell[\psi_p^{j-1}], & x_q^j=x_p^{j-1}, \\
                                      0, & \text{otherwise}. \end{cases}  \label{eq:injectprolong}
\end{equation}
In other words, we keep the values of $\ell$ on the coarser-level nodes and just fill-in with zero.  Using this prolongation gives a $j$th-level residual that is not smooth, but the solution of the equation on the $j$th level (i.e.~$\tilde F^j(y^j)[v]=0$ for all $v$ in $\mathcal{K}^j$) will be relatively smooth by ellipticity.  These ideas come together in the following pseudocode.
\begin{pseudo*} \label{ps:mcdl-vcycle}
\pr{mcdl-vcycle}(J,\ell^J,\chi^J,\id{down}=1,\id{up}=1,\id{coarse}=1)\text{:} \\+
    for $j=J$ downto $j=1$ \\+
      $\chi^{j-1} = \mR \chi^j$; \quad $\phi_V^j = \frac{1}{2}(\chi^j - P\chi^{j-1})$ \qquad  \ct{define obstacle} \\
      $y^j = 0$; \quad $\text{\pr{pgs-sweep}}^{\text{\id{down}}}(j,y^j,\ell^j,\phi_V^j)$ \\
      $\ell^{j-1}[\cdot] := - R (F^j(y^j))[\cdot]$ \qquad\qquad\qquad\quad \ct{update and restrict residual} \\-
    $y^0 = 0$; \quad $\text{\pr{pgs-sweep}}^{\text{\id{coarse}}}(0,y^0,\ell^0,\chi^0)$ \\
    $\omega^0 = y^0$ \\
    for $j=1$ to $j=J$ \\+
      $\ell^{j}[\cdot] := - \iP(\tilde F^{j-1}(\omega^{j-1}))[\cdot]$ \qquad\qquad\quad \ct{update and prolong residual} \\
      $\eta^j = 0$; \quad $\text{\pr{pgs-sweep}}^{\text{\id{up}}}(j,\eta^j,\ell^j,\phi_V^j)$ \\
      $\omega^j = y^j + \eta^j + P \omega^{j-1}$ \qquad\qquad\qquad\qquad \ct{accumulate corrections} \\-
    return $w^J$
\end{pseudo*}
Note that the vectors $\omega^j$ in the second \kw{for} loop are used to add-up (accumulate) the corrections in \eqref{eq:residuallinearlevelv}, $y^j$ from the way down and $\eta^j$ from the way up.  The procedure returns an iterate in $\mathcal{V}^J$,
    $$\omega^J = y^J + \dots + y^1 + y^0 + \eta^1 + \dots + \eta^J,$$
and then the calling solver (\pr{mcdl-solver}) does the update $w^J \gets w^J + \omega^J$.

FIXME unable to get the fine-level residuals to become arbitrarily small, though initially they decrease a little

\end{document}
