\documentclass[letterpaper,final,12pt,reqno]{amsart}

\usepackage[total={6.3in,9.2in},top=1.1in,left=1.1in]{geometry}

\usepackage{times,bm,bbm,empheq,fancyvrb,graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{longtable}

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}

\usepackage[kw]{pseudo}
\pseudoset{left-margin=15mm,topsep=5mm,idfont=\texttt}

% hyperref should be the last package we load
\usepackage[pdftex,
colorlinks=true,
plainpages=false, % only if colorlinks=true
linkcolor=blue,   % ...
citecolor=Red,    % ...
urlcolor=black    % ...
]{hyperref}

\renewcommand{\baselinestretch}{1.05}

\newtheoremstyle{claim}% name
  {5pt}% space above
  {5pt}% space below
  {\itshape}% body font
  {}% indent amount
  {\itshape}% theorem head font
  {.}% punctuation after theorem head
  {.5em}% space after theorem head
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}% theorem head spec
\theoremstyle{claim}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\newcommand{\eps}{\epsilon}
\newcommand{\RR}{\mathbb{R}}

\newcommand{\grad}{\nabla}
\newcommand{\Div}{\nabla\cdot}
\newcommand{\trace}{\operatorname{tr}}

\newcommand{\hbn}{\hat{\mathbf{n}}}

\newcommand{\bb}{\mathbf{b}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bbf}{\mathbf{f}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bn}{\mathbf{n}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}

\newcommand{\bF}{\mathbf{F}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bX}{\mathbf{X}}

\newcommand{\bxi}{\bm{\xi}}

\newcommand{\bzero}{\bm{0}}

\newcommand{\rhoi}{\rho_{\text{i}}}

\newcommand{\ip}[2]{\left<#1,#2\right>}

\newcommand{\mR}{R^{\bm{\oplus}}}

% numbering
\setcounter{tocdepth}{3}
\makeatletter
\def\l@subsection{\@tocline{2}{0pt}{4pc}{5pc}{}}
\makeatother

\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}
\numberwithin{theorem}{section}


\begin{document}
\title[Geometric multigrid for glacier modeling]{Geometric multigrid for glacier modeling: \\ New techniques and concepts}

\author{Ed Bueler}

\begin{abstract} FIXME: two principles in introduction: mass conservation complementarity, solver optimality.  four examples in sections \ref{sec:subspace}--\ref{sec:stokes}: poisson equation from subspace decomp point of view, obstacle problem by subset decomposition, monotone multigrid for implicitly-evolving SIA geometry, Schur-complement and Vanka Newton-multigrid for fixed-geometry Glen-Stokes
\end{abstract}

\maketitle

\tableofcontents

\thispagestyle{empty}
\bigskip

\section{Introduction} \label{sec:intro}

The construction of effective numerical glacier and ice sheet models is challenging for two fundamental reasons.  First is the complexity of the equations and boundary conditions.  Indeed, the physics of glaciers is nonlinear, nontrivially-coupled, and subject to imperfectly-understood boundary processes, such as at contact with ocean water.  The coupling is critical in the sense that mass, momentum, and energy conservation interact in ways which are relevant to glaciological modeling goals, such as when basal sliding, and thus ice velocity, is only determined though a simultaneous momentum and energy solution.  Second, the geometry of glaciers and ice sheets is complex, and in particular the fastest-flowing parts of ice sheets are often located at the geometrically-nontrivial lateral boundary where fjord-like bed geometry is also common.  Numerical models therefore need to perform expensive fine-mesh calculations, so as to accomodate the complicated, changing boundary geometry, while solving relatively-complicated multiphysics equations.

On the other hand, since the 1980s researchers in numerial methods have developed multigrid methods to solve partial differential equations like those which describe the ice fluid in glaciers.   For simpler problems like scalar elliptic equations and the linear Stokes system, especially on domains which have a simpler geometry, these methods are now in routine use \cite{Briggsetal2000,Bueler2021,Trottenbergetal2001}.

FIXME Accessible introductions to FE methods are in \cite{Bueler2021,Elmanetal2014,Johnson2009}.

FIXME cite for multigrid on classical obstacle \cite{BrandtCryer1983,Bueler2021,GraeserKornhuber2009}


\section{From subspace decomposition to multigrid} \label{sec:subspace}

\subsection*{Finite elements for a Poisson model problem}  In this section we will demonstrate how to solve a simple differential equation, namely a linear Poisson-like problem
\begin{equation}
- \big(\alpha(x)\,u'(x)\big)' = f(x) \quad \text{on} \quad 0 \le x \le 1, \label{eq:poisson}
\end{equation}
with Dirichlet boundary conditions $u(0)=u(1)=0$, using a finite element (FE) discretization and a multigrid method.  The functions $\alpha(x)$ and $f(x)$ are given data here, assumed sufficiently well-behaved for the computations which follow, and we assume $\alpha(x)$ is bounded and positive: $0 < c_1 \le \alpha(x) \le c_2$.  Over the course of the next three sections, this simple equation will evolve into a realistic model for glacier geometry.

Our numerical approximation of \eqref{eq:poisson} uses a mesh of $m$ interior \emph{nodes} (points) $x_p$ on $(0,1)$.  The $m+1$ open intervals between the nodes are the \emph{elements}.  The numerical solution $u^h(x)$ is then a linear combination of the piecewise-linear \emph{hat functions} $\psi_p(x)$, shown in Figure \ref{fig:finehats}, one for each interior node:
\begin{equation}
u^h(x) = \sum_{p=1}^m u_p \psi_p(x). \label{eq:trialsolution}
\end{equation}
Each hat function $\psi_p(x)$ is continuous on $[0,1]$, linear on each element, and satisfies $\psi_p(x_q) = \delta_{pq}$.  The set $\{\psi_p(x)\}_{p=1}^m$ is a \emph{nodal basis} of the space $\mathcal{V}^h$ of continuous, piecewise-linear functions, thus the coefficients equal the function values: $u_p=u^h(x_p)$.  Note that the derivative of $u^h(x)$ is well-defined on the elements, thus almost everywhere, but not at the nodes.  In a computer program the coefficients $u_p$ will be formed into a (column) vector $\bu=\{u_p\}$ in $\RR^m$.

\begin{figure}
\includegraphics[width=0.65\textwidth]{genfigs/finehats.pdf}
\caption{Hat functions $\psi_p(x)$ at interior points $x_p$ form a basis for a vector space $\mathcal{V}^h$ of piecewise-linear functions.}
\label{fig:finehats}
\end{figure}

Previewing our analogy to glacier problems, the reader might ask what is the need for a high-resolution mesh, i.e.~a large value of $m$ and small spacing between nodes $x_p$, in such a simple problem?  For a glacier model, the need is more obvious; a high-resolution mesh can capture the realistic and bumpy bed topography, and the local variations in the climatic mass-balance.  These correspond in problem \eqref{eq:poisson} to rapid variations in the coefficient function $\alpha(x)$ and the source function $f(x)$, respectively.  From now on the reader should remember that we will need the mesh to be \emph{fine}, i.e.~high-resolution, so as to capture the fine scales in model input data.

FE methods, and our application of multigrid ideas to glacier problems, require that we rephrase problems like \eqref{eq:poisson} into \emph{weak form} using integrals.  (The original equation is called the \emph{strong form}.)  To do this we suppose that the exact solution $u(x)$ comes from a vector space $\mathcal{H}$ of functions which are smooth enough to allow the computations which follow and which have value zero at $x=0$ and $x=1$.  While we will not over-use the language of Sobolev spaces \cite[for example]{Evans2010}, in fact $\mathcal{H}=H_0^1[0,1]=W_0^{1,2}[0,1]$ is a Hilbert space, i.e.~functions with one square-integrable derivative, and also we are assuming $f(x)$ is in $L^2[0,1]$ and $\alpha(x)$ is in $L^\infty[0,1]$.

The weak form arises by multiplying both sides of \eqref{eq:poisson} by a \emph{test function} from $\mathcal{H}$ and integrating by parts so that only first derivatives remain.  Choosing a test function $v(x)$, integrating on $[0,1]$, and using $v(0)=v(1)=0$, we find
\begin{equation}
\int_0^1 \alpha(x) u'(x) v'(x)\,dx = \int_0^1 f(x) v(x)\, dx.  \label{eq:weakpoissonearly}
\end{equation}
We now write this equation more compactly as
\begin{equation}
  a(u,v) = \ip{f}{v}, \label{eq:weakpoisson}
\end{equation}
defining each side as in \eqref{eq:weakpoissonearly}.  The left side $a(u,v)$ is linear in each argument (\emph{bilinear}) while the right side defines a \emph{linear functional} $\ell[v] = \ip{f}{v}$ acting on $v$ in $\mathcal{H}$.  (The convenience of this abstract notation will become clear as we describe the multigrid algorithms.)

Our FE method seeks a solution $u^h$ in $\mathcal{V}^h$ of the corresponding finite-dimensional weak-form,
\begin{equation}
  a(u^h,v) = \ip{f}{v},  \label{eq:feweakpoisson}
\end{equation}
for all test functions $v$ in $\mathcal{V}^h$.  (Equation \eqref{eq:weakpoisson} holds for all $v$ in the larger space $\mathcal{H}$.)  Note that the \emph{numerical error} $u-u^h$ satisfies the simple equation $a(u-u^h,v)=0$ for all $v$ in $\mathcal{V}^h$, and standard arguments show that this error will converge to zero as the maximum mesh spacing decreases to zero \cite{Elmanetal2014}.

One may substitute formula \eqref{eq:trialsolution} for $u^h$ into \eqref{eq:feweakpoisson} to derive a linear system
\begin{equation}
A \bu = \bbf, \label{eq:linearsystem}
\end{equation}
where $A$ is an $m\times m$ matrix and $\bbf$ is in $\RR^m$.  Each equation (row) in system \eqref{eq:linearsystem} is constructed by using a hat function as a test function; substitution of $v=\psi_p$ gives the $p$th equation.  The matrix $A$ has entries $a_{pq} = a(\psi_p,\psi_q)$, is symmetric, and is positive definite \cite{Elmanetal2014}.  For the right side one defines entries $f_p = \ip{f}{\psi_p}$ to form the vector $\bbf = \{f_p\}$.  For our problem, only three values $a_{p,p-1}, a_{p,p}, a_{p,p+1}$ are nonzero in row $p$, so $A$ is tridiagonal.  (Writing these entries in detail may remind the reader of finite difference approximations for the Laplacian.)  Depending on the form of $\alpha(x)$, the matrix entries
\begin{equation}
  a_{pq} = \int_0^1 \alpha(x) \psi_p'(x) \psi_q'(x)\,dx \label{eq:poissonentries}
\end{equation}
may be computed either exactly, or approximately by quadrature, and similarly for the values $f_p = \int_0^1 f(x) \psi_p(x)\,dx$.

The most straightforward way to solve assembled linear system \eqref{eq:linearsystem} would be via a direct method such as Gaussian elimination.  However, in higher-dimensional PDE problems such methods need much more that $O(m)$ operations to solve the system (where $m$ is the number of unknowns), while, as noted in the introduction, large-scale applications demand optimal $O(m)$ solution methods, or nearly so.  Furthermore, excluding the current section, all of our problems will be nonlinear, thus no finite-time direct method will be available anyway.  We will construct rapidly-convergent iterations for our FE methods, based on slowly-converging, but easy to implement, pointwise iterations, but we will generally not assemble matrices.

\subsection*{Coarse mesh levels}  From the above simple FE scheme we take the first steps to build a \emph{multilevel} scheme.  Consider an enlarged set of hat functions:
    $$\underbrace{\psi_1(x),\dots,\psi_m(x)}_{\text{existing fine level}},\underbrace{\psi_{m+1}(x),\dots,\psi_M(x)}_{\text{\small coarser levels}}$$
For example, two coarser levels, derived from the fine level in Figure \ref{fig:finehats} by coarsening, are shown in Figure \ref{fig:coarsehats}.  The first coarsening (top) by-passes every other node on the fine mesh, and the next coarsening (bottom) does this again.

\begin{figure}
\includegraphics[width=0.55\textwidth]{genfigs/coarsehats.pdf}
\smallskip

\includegraphics[width=0.55\textwidth]{genfigs/coarsesthats.pdf}
\caption{Coarser levels are additional sets of hat functions which spread over a greater distance.}
\label{fig:coarsehats}
\end{figure}

On 2D and 3D meshes this manner of constructing coarse-mesh hats is much less straightforward, and it is more common to start from a coarse mesh and refine level-by-level up to the fine level, but the meaning of fine and coarse levels is the same.  Our methods are not restricted to equally-spaced meshes, but models with a high-resolution structured mesh \cite{Bueler2016,Winkelmannetal2011} are among our target applications. We will return to these mesh issues in section \ref{sec:sia}.

We need notation for the levels.  Suppose that the coarsest level is indexed as $j=0$ and the finest as $j=J$, and that the functions $\psi_p^j(x)$, for $p=1,\dots,m_j$, form the $j$th level.  The interior nodes $x_p^j$ use the same multilevel indexing scheme.  The original fine level has now gained a superscript $J$; the original hats are $\psi_p^J(x)$ and the original nodes are $x_p^J$.  Figures \ref{fig:finehats} and \ref{fig:coarsehats} show a three-level scheme ($J=2$) with a total of $m_2=7$ hats (and interior nodes).  There are $m_1=3$ middle-level hats and $m_0=1$ coarsest-level hat.  The original mesh has $m_2+1=8$ elements, divisible by four, so the above three-level coarsening scheme was allowed.

The $j$th-level hat functions are linearly-independent and form a basis for the $j$th-level vector space:
\begin{equation}
  \mathcal{V}^j = \operatorname{span}\{\psi_1^j(x),\dots,\psi_{m_j}^j(x)\} \subset \mathcal{H}.  \label{eq:definevk}
\end{equation}
These spaces are nested, i.e.~$\mathcal{V}^{j-1} \subset \mathcal{V}^j$, because each hat function can be written as a linear combination of finer-level hats,
\begin{equation}
   \psi_p^{j-1}(x) = \sum_{q=1}^{m_j} c_{pq} \psi_q^j(x). \label{eq:hatcombination}
\end{equation}
In fact the coefficients are
\begin{equation}
  c_{pq} = \psi_p^{j-1}(x_q^j) \label{eq:nodalcoefficients}
\end{equation}
because $\{\psi_q^j\}$ form a nodal basis.  Nonzero coefficients $c_{pq}$ occur only when a fine-level node $x_q^j$ is in the non-zero set (\emph{support}) of a coarser hat $\psi_p^{j-1}(x)$.

A \emph{multilevel subspace decomposition} is now described by the vector-space sum:
\begin{equation}
  \mathcal{V}^h = \mathcal{V}^0 + \mathcal{V}^1 + \dots + \mathcal{V}^J. \label{eq:subspacedecomposition}
\end{equation}
This decomposition will be useful even though the final term $\mathcal{V}^J$ is actually equal to the original FE space $\mathcal{V}^h$.  Equation \eqref{eq:subspacedecomposition} asserts that a piecewise-linear function in $\mathcal{V}^h$ \emph{can} be written as a linear combination of hat functions from all the levels, but there is no unique representation.  A multilevel method will use this hierarchy to find the components of the solution in $\mathcal{V}^h$ via fast computations on all levels $\mathcal{V}^j$.

The levels provide an approximate \emph{scale of frequencies}.  Informally, the value of the inner product of a function $g(x)$ with a fine-level hat, i.e.~$\ip{g}{\psi_p^J}$, is, relative to its norm $\|g\| = \ip{g}{g}^{1/2}$, mostly a measure of its high-frequency content at $x_p^J$.  The inner product with a coarser-mesh hat $\psi_p^j$, by contrast, measures lower frequency content because it averages over many fine-mesh nodes.  If decomposition \eqref{eq:subspacedecomposition} were instead a Fourier decomposition, with each $\mathcal{V}^j$ spanned by waves of disjoint frequency ranges, then the sum would be orthogonal and the ``scale of frequencies'' meaning would be exact.  In any case, a multilevel method will reduce the energy of the error in a solution estimate on each level $\mathcal{V}^j$, and doing so on a coarse level rapidly reduces the low frequencies present in the error.

\subsection*{The residual and Gauss-Seidel}  Suppose $w$ in $\mathcal{V}^h$.  We define the \emph{residual} of $w$ as a linear functional acting on $v$ in $\mathcal{V}^h$:
\begin{equation}
  F(w)[v] = a(w,v) - \ip{f}{v}.  \label{eq:residual}
\end{equation}
Finding $u^h$ which solves \eqref{eq:feweakpoisson} is equivalent to making all of the components of its residual zero:
\begin{equation}
  F(u^h)[v]=0 \qquad \text{ if and only if } \qquad a(u^h,v)=\ip{f}{v}. \label{eq:residualweakequivalence}
\end{equation}

Given $w$ which does \emph{not} solve \eqref{eq:feweakpoisson}, we will iteratively update it using simple operations so that the components $F(w)[v]$ of its residual become smaller.  We may represent $w$ using the basis of hat functions on any mesh level, i.e.~$w = \sum b_q \psi_q^j$, and thus we may compute a $j$th-level residual component using the linearity of $a(\cdot,\cdot)$:
\begin{equation}
  F(w)[\psi_p^j] = \sum_{q=1}^{m_j} a(\psi_q^j,\psi_p^j) \,b_q - \ip{f}{\psi_p^j}.  \label{eq:residualpoisson}
\end{equation}
The number of nonzero terms in this sum is equal to the number of hat functions $\psi_q^j$ whose support overlaps the support of the test function $\psi_p^j$; this number is at most three in the 1D case.  Assuming the values $a(\psi_p^j,\psi_q^j)$ and $\ip{f}{\psi_p^j}$ are already computed, it follows that the computation of a residual component $F(w)[\psi_p^j]$ requires $O(1)$ work.

Our basic solution method for \eqref{eq:feweakpoisson}, called \emph{Gauss-Seidel} (GS) iteration, is sequential and point-wise \emph{relaxation} of the residual.  (Though matrices are often used to present the GS algorithm \cite[for example]{Bueler2021,Greenbaum1997}, there is no need; we present it using residuals and hat functions.)  The $j$th-level GS algorithm sweeps through the mesh nodes $x_p^j$, modifying the iterate $w(x)$ by a multiple of $\psi_p^j$ so as to make the residual at $x_p^j$ equal to zero.  That is, for each $p=1,\dots,m_j$ it finds a real number $c$ so that
\begin{equation}
  F(w+c\,\psi_p^j)[\psi_p^j] = 0.  \label{eq:gaussseidelpoint}
\end{equation}
Noting that $F(w+c\,\psi_p^j)[\psi_p^j] = F(w)[\psi_p^j] + c\, a(\psi_p^j,\psi_p^j)$, we have the following algorithm:
\begin{pseudo*}
\pr{gssweep}(j,w,F)\text{:} \\+
    for $p=1,\dots,m_j$ \\+
        $\displaystyle c = - F(w)[\psi_p^j]\, \big/ \,a(\psi_p^j,\psi_p^j)$  \\
        $w \gets w + c \psi_p^j$
\end{pseudo*}

This procedure takes the residual function as an argument and it modifies $w$ in-place.  Assuming the values $a(\psi_p^j,\psi_q^j)$ and $\ip{f}{\psi_p^j}$ are available, the comments after formula \eqref{eq:residualpoisson} imply that computing $c$ involves $O(1)$ work, and thus one application of \textsc{gssweep} requires $O(m_j)$ work.  Though \textsc{gssweep} only works as stated for a linear equation in the form of \eqref{eq:feweakpoisson}, it can be modified to solve nonlinear systems, so-called \emph{nonlinear Gauss-Seidel}.  For example, one could apply a few steps of Newton's iteration to each scalar equation $f(c) = F(w+c\,\psi_p^j)[\psi_p^j] = 0$.  We will adopt this approach in section \ref{sec:sia}.

What does a sweep of GS do to an iterate $w$?  By sequentially making the residual zero on each hat function we can hope that the residual becomes smaller.  However, modifying $w$ to make the residual zero on one hat generally means the previously-zeroed locations are no longer zero.  (That is, the equations are non-trivially coupled!)  Considering only fine-level GS sweeps for a moment, one can prove for the Poisson problem that this iteration does eventually converge to the solution $u^h$ of \eqref{eq:feweakpoisson} \cite[for example]{Greenbaum1997}, but that is not our focus.  Instead, a key observation is that such a sweep is a fast \emph{smoother} of the (algebraic) error $e=w-u^h$, a function in $\mathcal{V}^h$, even when it is slow to make this error small.  Informally, the formula for $c$ in \textsc{gssweep} combines neighboring values of $w$ so as to flatten a peak or trough in the error.

The observation that GS is a good smoother can be made quantitative by considering the frequencies supported on a mesh with spacing $h$.  The highest-frequency faithfully-represented mode on such a mesh is the sawtooth mode with (spatial) frequency $\omega=(2h)^{-1}$.  One GS sweep multiplies all modes with frequencies higher than $\frac{1}{2} \omega$ by factors at most $1/\sqrt{5}\approx 0.45$ \cite[Chapter 4]{Briggsetal2000}.  (This damping factor depends on the dimension and the differential operator.)  That is, a GS sweep \emph{strongly damps the highest half} of the frequencies in the error.

\begin{figure}[t]
\includegraphics[width=0.8\textwidth]{genfigs/residualpoints.pdf}
\caption{One Gauss-Seidel (GS) sweep on the fine level adjusts the iterate $w$ so that the residual $F(w)[\psi_p^J]$ at each successive node $x_p^J$ is zero (left).  The corresponding errors $e=w-u^h$ get significantly smoother (right).}
\label{fig:residualpoints}
\end{figure}

An example is shown in Figure \ref{fig:residualpoints}, where we start with a non-smooth initial iterate $w$ on an $m_J=6$ mesh; its residual $F(w)$ is at top-left.  The Figure shows the residual (left sequence) and the error (right) after each step in the \textbf{for} loop in \textsc{gssweep}, indicating the location which is zeroed.  (We plot the linear functionals $F(w)[\cdot]$ as piecewise-constant functions with values $F(w)[\psi_p^J]$.)  While both the residual and error become smaller in norm, the strongest effect is the damping of high frequencies.

\subsection*{Multilevel subspace corrections}  Once the GS smoother is applied on a given level $j$, whether once or a few times, the error and the residual no longer contain much energy in the $j$th-level modes.  Thus it makes sense to remove the energy in the next-coarser modes by applying GS on the $j-1$ level.  Starting on the finest level $J$, we have the following \emph{multilevel subspace decomposition} algorithm which corrects the residual on each level.
\begin{pseudo*}
\pr{msd-slash}(w,F)\text{:} \\+
    for $j=J$ downto $0$ \\+
        \pr{gssweep}(j,w,F)
\end{pseudo*}

The reason this downward-only scheme is called ``slash'' is illustrated in Figure \ref{fig:msdcycles}.  Other correction sequences are possible, for example one may return to the fine level in a ``V'' cycle, as follows.
\begin{pseudo*}
\pr{msd-vcycle}(w,F)\text{:} \\+
    for $j=J$ downto $0$ \\+
        \pr{gssweep}(j,w,F) \\-
    for $j=1$ to $J$ \\+
        \pr{gssweep}(j,w,F)
\end{pseudo*}
Furthermore one may repeatedly apply the smoother on each level, or go up-and-down the mesh hierarchy in a more complicated manner (``W-cycles'', for example), or use coarser subspace corrections before finer; there are many easily-programmed possibilities \cite{Briggsetal2000,Trottenbergetal2001} once a V-cycle is correctly implemented.

These \pr{msd} algorithms improve a fine-level iterate $w$ but they do not (generally) exactly solve the problem \eqref{eq:feweakpoisson}.  Instead one applies them via iteration, testing for convergence using some tolerance on the norm of the (computable) fine-level residual.  For example, the following algorithm requires the residual to decrease from its initial value by a certain factor.
\begin{pseudo*}
\pr{msd-solver}(F,\id{rtol}=10^{-4})\text{:} \\+
    $w=0$ \qquad\qquad\qquad\qquad\qquad \ct{or other fine-level initial iterate} \\
    $r_0 = \|F(w)\|$ \\
    repeat \\+
        \pr{msd-slash}(w,F) \qquad\qquad \ct{or \pr{msd-vcycle}, etc.} \\-
    until $\|F(w)\| \le r_0\, \id{rtol}$ \\
    return $w$
\end{pseudo*}

\begin{figure}
\input{tikz/msdcycles.tex}
\caption{Subspace corrections can be downward-only as in \pr{msd-slash} (left) or a V-cycle as in \pr{msd-vcycle} (right).  Each dot in this three-level hierarchy ($J=2$) is an application of \pr{gssweep} on that level.}
\label{fig:msdcycles}
\end{figure}

These simply-stated schemes, which appeared relatively-late in the history of multigrid \cite{Xu1992}, contain the core ideas of all multigrid schemes, and they possess excellent convergence properties.  For example, the following theorem applies to linear elliptic PDEs in any dimension.  The constant $\rho$ depends only on the element aspect ratios (\emph{shape regularity} \cite{Elmanetal2014}) in the mesh---this is not relevant in 1D---and on the coefficient-minimum (\emph{ellipticity bound}) $\alpha_0=\inf_x \alpha(x)$.  Recall that $u^h$ is the exact solution of the FE weak form \eqref{eq:feweakpoisson}.

\begin{theorem} \cite[Thm.~3.10]{GraeserKornhuber2009}\,  \label{thm:msdconvergence}  Suppose $w^{(s)}$  results from $s$ applications of \pr{msd-slash} or \pr{msd-vcycle} on the fine level, starting with any $w^{(0)}$.  There is $\rho<1$ independent of $J$ and $h$ so that
\begin{equation}
  \|w^{(s+1)} - u^h\|_{\mathcal{H}} \le \rho \|w^{(s)} - u^h\|_{\mathcal{H}}.  \label{eq:msdconvergence}
\end{equation}
\end{theorem}

It follows that if we want the error norm $e^{(s)} = \|w^{(s)}-u^h\|_{\mathcal{H}}$ to be reduced to below some level $\eps>0$ then we should do $s>(\log\eps - \log e^{(0)})/\log \rho$ iterations, where $e^{(0)}$ is the initial error norm.  That is, we need $O(|\log\eps|)$ iterations.  (Note that a smaller $\rho$ implies a better constant.)  The error norms are not computable, but they are related to computable residual norms via the condition number of the system matrix $A$ \cite[Chapter 2]{Bueler2016}.

However, these \pr{msd} methods are not yet complete solvers.  In fact their performance entirely depends on how iterates $w$ and residuals $F(w)[\cdot]$ are represented on each mesh level.  For example, if we represent $w$ and $F(w)$ using the fine-level basis $\{\psi_p^J\}$ then one application of \pr{gsweep}$(J,w,F)$ is indeed fast, i.e.~$O(m_J)$ operations, with a small constant, because evaluating each $F(w)[\psi_p^J]$ requires only a few operations.  However, evaluating $F(w)[\psi_p^j]$ for $j<J$ means computing integrals over the wide support of $\psi_p^j$, which is nonzero at many fine-mesh nodes $x_p^J$.  That is, \pr{gssweep}$(j,w,F)$ for $j<J$ is not an efficient operation if $w$ and $F(w)$ are represented on the fine level.  Specifically, it is not an $O(m_j)$ operation with a coefficient which is independent of $j$.  Fortunately, these \pr{msd} methods can be improved, as solvers, via hierarchical representations which are compatible with multilevel corrections.

\subsection*{Linear geometric multigrid}  Once the smoother has been applied on the finest ($J$) level, so that the error and the residual no longer contain much energy in the high-frequency modes, the smoother should then be applied on the $J-1$ level, but efficiently using $J-1$ level data structures to represent the problem.  This idea is applied down the mesh-level hierarchy.  By identifying the coarse-level representations we go beyond multilevel subpace corrections and create a true multigrid algorithm.

Efficiency on coarser levels requires addressing two concerns:
\renewcommand{\labelenumi}{\emph{\roman{enumi})}}
\begin{enumerate}
\item How do we represent an iterate $w$ and a residual $F(w)[\cdot]$ on the $j$th level?
\item How does a $j$th-level problem descend to the representation on the $j-1$ level?
\end{enumerate}

Our answer to \emph{i)} is straightforward, and already addressed.  A function $w(x)$ in $\mathcal{V}^j$ is represented by its nodal values $w_p=w(x_p^j)$, thus $w = \sum_p w_p \psi_p^j$, and $\bw = \{w_p\}$ is a vector in $\RR^{m_j}$.  A residual $F(w)$, which is a linear functional in $(\mathcal{V}^j)'$, is just as simple because the values $F_p = F(w)[\psi_p^j]$ form a vector $\bF=\{F_p\}$, also in $\RR^{m_j}$.  It is common to think of $\bw$ as a column vector and $\bF$ as a row vector, but this is not essential.

For \emph{ii)} we will derive a \emph{new} equation for the coarser level, one which loses only minimal information if the key quantities are smooth.  To derive this equation we must consider the error for an iterate.  For an iterate $w$ in the original FE space $\mathcal{V}^h$, the residual definition \eqref{eq:residual} can be rewritten as the equation
\begin{equation}
  a(w,v) = \ip{f}{v} + F(w)[v].  \label{eq:residualrewrite}
\end{equation}
Subtracting the weak form \eqref{eq:feweakpoisson} from \eqref{eq:residualrewrite}, we may cancel the source term:
\begin{equation}
  a(w,v) - a(u^h,v) = F(w)[v].  \label{eq:errorequationearly}
\end{equation}
Because of the linearity of $a(\cdot,\cdot)$ in the first position, we have the (weak-form) \emph{error equation},
\begin{equation}
  a(e,v) = F(w)[v],  \label{eq:errorequation}
\end{equation}
for all $v$ in $\mathcal{V}^h$, where $e=w-u^h$.

Error equation \eqref{eq:errorequation} is actually equivalent to the original FE weak form \eqref{eq:feweakpoisson}.  However, if the smoother has already been applied to an iterate $w$, on the finest level, then both $e$ and the residual $F(w)$ are smoothed quantities.  That is, both sides of \eqref{eq:errorequation} should have accurate representations in terms of the coarser hats $\{\psi_p^{J-1}\}$, without using the fine-level hats.  Multigrid therefore proposes a new equation, an approximation of \eqref{eq:errorequation}, in which all quantities are represented using only the coarser-level basis.

Generalizing to an arbitrary level $j>0$, suppose $w^j$ is any iterate in $\mathcal{V}^j$.  We approximate \eqref{eq:errorequation} by the following \emph{coarse-level equation}:
\begin{equation}
  a(e^{j-1},v) = (R\ell^j)[v] \qquad \text{where} \qquad \ell^j = F(w^j),  \label{eq:coarsecorrection}
\end{equation}
for all $v$ in $\mathcal{V}^{j-1}$.  Note that we will represent $e^{j-1}$ in the basis $\{\psi_p^{j-1}\}$, and $R\ell^j$ by its values $(R\ell^j)[\psi_p^{j-1}]$.

We have used the \emph{canonical restriction operator} $R$ to put the $j$th-level residual $F(w^j)$ onto the $j-1$ level.  By definition, $R$ maps a linear functional $\ell$ on the $j$ level, i.e.~$\ell$ in $(\mathcal{V}^j)'$, to a linear functional in $(\mathcal{V}^{j-1})'$:
\begin{equation}
  (R \ell)[v] = \ell[v], \label{eq:canonicalrestriction}
\end{equation}
for all $v$ in $\mathcal{V}^{j-1}$.  That is, $R \ell$ acts on $\mathcal{V}^{j-1}$ in the same manner as $\ell$, but we \emph{represent} $R\ell$ by its $j-1$ level values.  From \eqref{eq:hatcombination} and \eqref{eq:nodalcoefficients}, the following formula applies with $c_{pq}=\psi_p^{j-1}(x_q^j)$:
\begin{equation}
  (R \ell)[\psi_p^{j-1}] = \sum_{q=1}^{m_j} c_{pq} \ell[\psi_q^j].  \label{eq:canonicalrestrictionaction}
\end{equation}
The right-hand side of \eqref{eq:coarsecorrection} requires a bit of computational work.  It also implies a loss of information because the fine-level values $\ell_q=\ell[\psi_q^j]$ are not recoverable from $R\ell$.

Given the solution $e^{j-1}$ of \eqref{eq:coarsecorrection}, the update
\begin{equation}
  w^j \gets w^j + P e^{j-1}  \label{eq:update}
\end{equation}
``corrects'' the fine-level iterate, but this introduces yet another map, the \emph{canonical prolongation} $P$.  It acts on a function $y$ in $\mathcal{V}^{j-1}$ to give $Py$ in $\mathcal{V}^j$, without loss of information because $\mathcal{V}^{j-1} \subset \mathcal{V}^j$:
\begin{equation}
  (P y)(x) = y(x). \label{eq:canonicalprolongation}
\end{equation}
The result $P y$ is the same piecewise-linear function as the input $y$, but again the representation changes, and again \eqref{eq:hatcombination} and \eqref{eq:nodalcoefficients} are used to find the components of $Py$ in the fine-level basis.  Specifically, if $y=\sum_p y_p \psi_p^{j-1}$ then $z=Py$ has the representation $z = \sum_q z_q \psi_q^j$ where
\begin{equation}
  z_q = \sum_{p=1}^{m_{j-1}} c_{pq} y_p. \label{eq:canonicalprolongationaction}
\end{equation}

Combining the above ideas gives a \emph{geometric multigrid} (GMG) \emph{V-cycle}.  In addition to sweeps of the GS smoother, our V-cycle algorithm solves the coarse-level correction equation \eqref{eq:coarsecorrection}, and it applies update \eqref{eq:update}, followed by more smoother sweeps.  The following pseudocode, which modifies the fine-level iterate $w^J$ in-place, can be called repeatedly so as to improve the iterate.
\begin{pseudo*}
\pr{gmg-vcycle}(J,w^J,F^J,\id{down}=1,\id{up}=1)\text{:} \\+
    for $j=J$ downto $1$ \\+
        $\text{\pr{gssweep}}^{\text{\id{down}}}(j,w^j,F^j)$ \\
        $\ell^{j-1} := R(F^j(w^j))$ \\
        $F^{j-1}(y)[\cdot] := a(y,\cdot) - \ell^{j-1}[\cdot]$ \\
        $w^{j-1} = 0$ \qquad\qquad\qquad\qquad\qquad \ct{note $w^{j-1}=e^{j-1}$} \\-
    \pr{coarsesolve}(w^0,F^0) \\
    for $j=1$ to $J$ \\+
        $w^j \gets w^j + P w^{j-1}$ \\
        $\text{\pr{gssweep}}^{\text{\id{up}}}(j,w^j,F^j)$ \\-
\end{pseudo*}

It is straightforward to implement \pr{gmg-vcycle} recursively, but the above form using loops will be easier to extend to inequality-constrained cases.  A non-recursive implementation also explicitly identifies the storage needed, namely memory to hold states $w^0,\dots,w^J$ and linear functionals $\ell^0,\dots,\ell^J$.

Observe that after \texttt{down} smoother sweeps, denoted here by $\text{\textsc{gssweep}}^{\text{\texttt{down}}}$, the coarse-level correction equation \eqref{eq:coarsecorrection} is applied on the next-coarser level with a ``new'' right-hand linear functional $\ell^{j-1}$ and a zero initial iterate.  The new source term $\ell^{j-1}$ is the amount by which the finer-level iterate $w^j$ did not already solve the problem.  (For instance, $\ell^{J-1}=0$ if $w^J=u^h$.)  After this correction an additional \texttt{up} sweeps are done to remove high-frequency components from the updated iterate.  Figure \ref{fig:vcycle} illustrates a three-level V-cycle.

\begin{figure}
\input{tikz/vcycle.tex}
\caption{A V-cycle with distinguished subroutines: down-smoother (solid dots), up-smoother (circles), and coarse-level solver (square).}
\label{fig:vcycle}
\end{figure}

The algorithm calls a coarse solver subroutine on the $j=0$ level.  For a linear problem like \eqref{eq:poisson} one may apply a direct solver to the linear system, but direct solvers are not available for nonlinear (e.g.~glacier) problems.  For simplicity and generalizability we suppose \textsc{coarsesolve} is implemented as a fixed number of in-place GS sweeps:
\begin{pseudo*}
\pr{coarsesolve}(w,F,\id{coarse}=1)\text{:} \\+
    $\text{\pr{gssweep}}^{\text{\id{coarse}}}(0,w,F)$ \\
\end{pseudo*}
If the coarsest mesh has a single node ($m_0=1$) then a single sweep gives an exact solution.  In general this solver should be accurate and fast if $m_0$ is small.  Nonetheless the choice of the coarse level and its solver can be nontrivial for realistic problems; see sections \ref{sec:sia} and \ref{sec:stokes}.

\subsection*{On restrictions and prolongations}  The operators $R$ and $P$ make no choices, which is the meaning of ``canonical'', but their application requires computational work because of how we represent functions and functionals on each level.  That work is implied by \eqref{eq:hatcombination} and \eqref{eq:nodalcoefficients}, which express each coarse-level hat function as a linear combination of fine-level hats.

As linear operators, $R$ and $P$ may be represented as sparse rectangular matrices, and $P=R^\top$ (transpose), but storing them as matrices in memory is not necessary.  Both operators are applied using $O(m_j)$ work via our straightforward local-averaging formulas, also called \emph{full weighting} \cite{Briggsetal2000}.  In fact our formulas \eqref{eq:hatcombination}, \eqref{eq:nodalcoefficients}, \eqref{eq:canonicalrestrictionaction}, and \eqref{eq:canonicalprolongationaction} generalize verbatim to unstructured 2D and 3D meshes, where indices $p$ and $q$ denote the nodes of the mesh.

However, the two operators differ in their invertibility.  If $y$ is in $\mathcal{V}^{j-1}$ then $y$ can be exactly recovered from $Py$.  Indeed, $y(x)$ and $(Py)(x)$ are the same piecewise-linear function, and we have made $P$ explicit in order to indicate a change in computer representation.  Saying the same thing a different way, as a linear operator $P$ is one-to-one, and thus it has a ``left'' inverse though it is not square.  By contrast, $\ell$ is not recoverable from $R\ell$; here $\ell$ is in $(\mathcal{V}^j)'$.  That is, the values of $\ell[\psi_p^j]$, the action of $\ell$ on the finer level hats, is averaged away.

FIXME discuss four ops in Table \ref{tab:restrictionsprolongations}; in notation table point back to this table

\newcommand{\iP}{P^{\hookrightarrow}}
\begin{table}
\begin{tabular}{l|cccc}
\emph{operator}              & \emph{domain}          & \emph{range}
                  & \emph{one-to-one} & \emph{linear} \\ \hline
canonical restriction $R$    & $(\mathcal{V}^j)'$     & $(\mathcal{V}^{j-1})'$
                  &            & \checkmark \\
monotone restriction $\mR$   & $\mathcal{V}^j$        & $\mathcal{V}^{j-1}$
                  &            &            \\
canonical prolongation $P$   & $\mathcal{V}^{j-1}$    & $\mathcal{V}^j$
                  & \checkmark & \checkmark \\
injection prolongation $\iP$ & $(\mathcal{V}^{j-1})'$ & $(\mathcal{V}^j)'$
                  & \checkmark & \checkmark
\end{tabular}

\medskip
\label{tab:restrictionsprolongations}
\caption{We use various restriction and prolongation operators, which must be carefully distinguished!}
\end{table}

We have now presented a basic geometric multigrid algorithm via a particular FE viewpoint, namely the subspace decomposition approach pioneered by Xu \cite{Xu1992} and others.  (Subspace decomposition is a foundation for multilevel, domain-decomposition, and a variety of other advanced algorithms \cite{Farrelletal2019}.)  Our natural next step might be to show computational results featuring the efficiency of the V-cycle algorithm, giving evidence of optimal $O(m_J)$ time to solve the problem, and indeed such results appear in all multigrid references \cite{Briggsetal2000,Bueler2021,Elmanetal2014,Trottenbergetal2001}.  However, we instead choose to introduce a less-standard ``obstacle'' problem in the next section, one which has the essential free-boundary character of the glacier geometry problem.  We provide computational results in each of the next three sections.

Before proceeding, the reader might ask what makes our approach ``geometric'' multigrid?  The answer is that the restriction and prolongation formulas directly use the original FE mesh.  In particular, the formula for $P$ has coefficients $c_{pq} = \psi_p^{j-1}(x_q^j)$, the values of hat functions at the mesh nodes.  For comparison, in an \emph{algebraic multigrid} \cite{Trottenbergetal2001} approach the prolongation $P$ would be constructed using operations on the entries of the linear system matrix $A$, and $R=P^\top$ by definition.  For linear elliptic problems the geometric and algebraic approaches are closely-related because the latter have been ``tuned'' to generate prolongations $P$ with similar coefficients.  For our upcoming nonlinear and inequality-constrained problems, the geometric approach generalizes directly.  By contrast, an algebraic multigrid approach could only be used within the linear steps in a separately-constructed iteration, e.g.~based on an outer Newton iteration.  Both approaches are potentially optimal, and are the natural competitors for the highest-performance solutions.


\section{Constraint decomposition for the obstacle problem} \label{sec:obstacle}

\subsection*{An ice-like model problem}  We now have a basic view of the geometric multigrid method from the multilevel subspace decomposition point of view.  However, as addressed in section \ref{sec:intro}, the main problem in glacier modeling, of how the ice geometry and velocity co-evolve in response to climatic inputs, needs an additional inequality constraint for well-posedness.  The fact that the ice surface elevation is above the bed generates the land-terminating boundary condition for the mass conservation problem, and this constraint, by itself, makes the problem nonlinear and reduces solution regularity, which is challenging to numerical methods.

To address such inequality constraints, and before actually modeling glaciers in section \ref{sec:sia}, we introduce a specifically-relevant model problem, namely the \emph{classical obstacle problem} which adds an inequality constraint to the same linear Poisson equation in section \ref{sec:subspace}.  After stating the weak form, which is now solved over a convex \emph{subset} of the function space, we will modify the multilevel subspace decomposition approach to be a \emph{multilevel constraint decomposition} method.  Each mesh level will host an inequality-constrained problem, and together all the levels will capture the original constraint set.  Switching the differential equation to the nonlinear SIA model in section \ref{sec:sia} will then be a comparatively easy change.

We use the same 1D domain and solution space $\mathcal{H}=H_0^1[0,1]$ as for the Poisson problem \eqref{eq:poisson}.  Let $\varphi(x)$ be a fixed function, also in $\mathcal{H}$, which is the obstacle.  The strong form of the obstacle problem is the following \emph{complementarity problem} (CP) \cite{Bueler2021,KinderlehrerStampacchia1980} which says that the differential equation applies whereever the solution $u(x)$ is strictly above the obstacle:
\begin{align}
  u - \varphi &\ge 0 \label{eq:obstaclecp} \\
  -(\alpha u')'-f &\ge 0 \notag \\
  (u-\varphi)(-(\alpha u')'-f) &= 0 \notag
\end{align}
The last condition, complementarity, implies that for each $x$ in $[0,1]$ the solution either coincides with the obstacle ($u(x)=\phi(x)$) or the Poisson equation holds.  (Or both, but in the generic \emph{nondegenerate} \cite{KinderlehrerStampacchia1980} case the obstacle does not itself solve the Poisson equation.)  Furthermore, where the solution coincides with the obstacle the source term is bounded above: $u=\varphi \implies f \le -(\alpha\varphi')'$.  In this $u=\varphi$ region the constraint is said to be \emph{active}, while the differential equation holds in the \emph{inactive} portion where $u>\varphi$.

The glacier problem will also have such a CP formulation (section \ref{sec:sia}; see also \cite{Calvoetal2002}) in which the obstacle is the bed elevation, the solution the glacier surface elevation, and the source term is the surface mass balance.  In the inactive region, i.e.~on the glacier, the mass conservation equation will apply, but off the glacier, in the active-constraint region, the surface mass balance must be negative.

Simply by choosing a source term $f(x)$ which is positive in the middle of the domain and negative near the boundaries, we get an ``ice-like'' solution to \eqref{eq:obstaclecp} as shown in Figure \ref{fig:icelike}.  In detail, the Figure shows the exact, piecewise-quadratic solution $u(x)$ for the following data:
\begin{equation}
\varphi(x) = x(1-x), \quad \alpha(x)=1, \quad f(x) = \begin{cases} 8, & 0.2 < x < 0.8, \\
                                                                 -16, & x<0.2 \text{ or } x>0.8. \end{cases}  \label{eq:icelikedetails}
\end{equation}
(Note that $f$ is in $L^2[0,1]$, and is defined almost everywhere.)  Finding the exact formula for $u(x)$, which smoothly-connects five quadratic pieces, is an exercise for the reader.\footnote{Find the solution in: \, \href{https://github.com/bueler/mg-glaciers/blob/master/py/obstacle.py}{\texttt{github.com/bueler/mg-glaciers/blob/master/py/obstacle.py}}.}

% regenerate:
%   $ cd py/1D/
%   $ ./obstacle.py -plain -jfine 5 -o icelike.pdf
%   $ pdfcrop icelike.pdf icelike.pdf
\begin{figure}
\includegraphics[width=0.7\textwidth]{fixfigs/icelike.pdf}
\caption{An ice-like configuration of the classical obstacle problem.}
\label{fig:icelike}
\end{figure}

Unlike problem \eqref{eq:poisson}, the solution to \eqref{eq:obstaclecp} does \emph{not} depend linearly on the source function $f$.  For example, if $f(x)\le 0$ for all $x$ then the solution for the same obstacle $\varphi(x)$ as in \eqref{eq:icelikedetails} is $u=\varphi$.  Observe that if $\tilde u$ solves the problem for $\tilde f= -1$ then $2\tilde u$ \emph{does not} solve the problem for source term $2\tilde f = -2$.  In this sense the classical obstacle problem is nonlinear even though the corresponding differential equation is linear.  (The differential equation is the so-called \emph{interior condition} of the CP \cite{KinderlehrerStampacchia1980}.)

\subsection*{Weak formulation with constraints}  In considering the weak form of \eqref{eq:obstaclecp} we observe that the solution and test functions must be in a closed subset which incorporates the constraint:
\begin{equation}
\mathcal{K} = \left\{v \ge \varphi\right\} \subseteq \mathcal{H}.  \label{eq:Kdefine}
\end{equation}
This subset is not a vector space, but it is \emph{convex}.  That is, if $v,w$ are in $\mathcal{K}$ then any point on the line segment connecting them, namely $\theta v + (1-\theta) w$ for $0 \le \theta \le 1$, is also in $\mathcal{K}$.  If $v$ is in $\mathcal{K}$ then we say $v$ is \emph{admissible}.

As before, derivation of the weak form of \eqref{eq:obstaclecp} involves multiplying by a test function and integrating by parts.  However, the inequalities enter nontrivially into the derivation; see \cite[Chapter 12]{Bueler2021}, \cite{JouvetBueler2012}, or \cite{KinderlehrerStampacchia1980}.  The result is a single \emph{variational inequality} (VI):
\begin{equation}
  a(u,v-u) \ge \ip{f}{v-u} \quad \text{ for all } v \text{ in } \mathcal{K}. \label{eq:obstaclevi}
\end{equation}
The bilinear form $a$ and function $f$ are the same as in the linear weak form \eqref{eq:weakpoisson}, and thus the VI can be restated using the residual \eqref{eq:residual}:
\begin{equation}
  F(u)[v-u] \ge 0 \quad \text{ for all } v \text{ in } \mathcal{K}. \label{eq:obstacleviresidual}
\end{equation}

Formulations \eqref{eq:obstaclecp} and \eqref{eq:obstaclevi} are equivalent up to the same regularity concerns which relate the strong and weak forms of a PDE.  (See reference \cite{Evans2010} regarding the solution regularity of PDEs, and \cite{KinderlehrerStampacchia1980} for the corresponding VI concepts.)  However, some of the intuition behind a VI is needed for understanding, so, in an attempt to help, we provide another weak form.  Inequality \eqref{eq:obstaclevi} is precisely equivalent to \emph{constrained minimization}:
\newcommand{\argmin}{\mathop{\mathrm{arg\text{-}min}}}
\begin{equation}
  u = \argmin_{w \text{ in } \mathcal{K}} I(w) \quad \text{where} \quad I(w) = \frac{1}{2} a(w,w) - \ip{f}{w}. \label{eq:obstaclemin}
\end{equation}
That is, $u$ is the minimizer, over the constraint set $\mathcal{K}$, of the scalar, quadratic \emph{objective} functional $I$.  (The proof of equivalence is standard in continuous optimization.)  The (Gateaux) derivative of $I$ is a linear functional in $\mathcal{H}'$:
\begin{equation}
  \grad I(w)[v] = \lim_{\eps\to 0} \frac{I(w+\eps v) - I(w)}{\eps} = a(u,v) - \ip{f}{v}.  \label{eq:gradobjective}
\end{equation}
Noting $\nabla I = F$, the VI \eqref{eq:obstacleviresidual} can re-stated yet again:
\begin{equation}
  \nabla I(u)[v-u] \ge 0 \quad \text{ for all } v \text{ in } \mathcal{K}. \label{eq:obstaclevigradient}
\end{equation}

Now we can propose a geometrical meaning for the VI formulation \eqref{eq:obstaclevigradient}.  The solution $u$ sits at a location in $\mathcal{K}$, possibly on the boundary of $\mathcal{K}$, where a vector pointing further into the constraint set, namely $v-u$ for $v$ in $\mathcal{K}$, points ``uphill'' on the graph of $I$.  If we think of $\nabla I(u)$ as a vector, instead of a linear functional, we could write \eqref{eq:obstaclevigradient} as $\ip{\nabla I(u)}{v-u} \ge 0$, which says that the angle between $\nabla I(u)$ and $v-u$ is at most 90 degrees.

On the other hand, $\mathcal{K} \subset \mathcal{H}$ is an infinite-dimensional set, so visualizing geometry is problematic.  To further aid the intuition we propose the following low-dimensional, but global, mental image.  Imagine $\mathcal{K}$ as analogous to the closed first quadrant $\mathcal{Q}$ in the two-dimensional plane, and $I$ as analogous to a smooth, concave-up (i.e.~\emph{coercive} \cite{Evans2010}), objective function $\gamma(x)$ defined on the plane.  The solution of the corresponding VI, say $\hat x$ in $\mathcal{Q}$, may not be a location where $\nabla \gamma$ is zero.  However, $\gamma$ will have as low a value as possible at $\hat x$, even if it is on the boundary of $\mathcal{Q}$.  At $\hat x$ all directions \emph{into} $\mathcal{Q}$ will increase (not decrease) the value of $\gamma$, as sketched by contours in Figure \ref{fig:cartoonplane}.  Note that the solution $u$ shown in Figure \ref{fig:icelike} is an infinite-dimensional version of the point $\hat x$ shown in the Figure \ref{fig:cartoonplane} cartoon view.  The fact that $u$ in Figure \ref{fig:icelike} coincides with the obstacle $\varphi$ on a part of the domain implies that $u$ is on the boundary of $\mathcal{K}$.  (That is, arbitrarily-small perturbations $\delta u$ from $\mathcal{H}$ exist which cause $u+\delta u$ to leave $\mathcal{K}$.)

\begin{figure}
\includegraphics[width=0.35\textwidth]{genfigs/cartoonplane.pdf}
\caption{For analogy only:  A 2-dimensional constrained-minimization or VI solution, in a planar quadrant $\mathcal{Q}$, with contours of a scalar function $\gamma(x)$.}
\label{fig:cartoonplane}
\end{figure}

The glacier problem in section \ref{sec:sia} will be formulated both as a CP like \eqref{eq:obstaclecp} and a VI like \eqref{eq:obstaclevi} or \eqref{eq:obstacleviresidual}.  However, for general bed elevation functions (obstacles) the glacier problem has no constrained minimization formulation like \eqref{eq:obstaclemin}, and thus no interpretation like \eqref{eq:obstaclevigradient}.  (The essential reason is the lack of a symmetry of the weak form \cite{JouvetBueler2012}.)  For the glacier problem we possess a map like $F$, from $\mathcal{K}$ to $\mathcal{H}'$, but not a scalar objective like $I$ itself; the glacier $F$ is \emph{not} a gradient.  We will return to this point in section \ref{sec:sia}.

Regardless of how the classical obstacle problem is formulated, a \emph{free boundary} generally arises in the interior of the domain.  For example, as we move away from the midpoint in Figure \ref{fig:icelike} there are locations, near the ends of $[0,1]$, where the solution first becomes fully in contact with the obstacle.  At these locations both $u=\varphi$ and $u'=\varphi'$ hold, that is, the solution is tangent to the obstacle at the free boundary.  (The analogous facts for the glacier problem are that both the ice thickness and the horizontal ice flux go to zero at a grounded glacier margin.)  These simultaneous Dirichlet and Neumann ``conditions'' occur at a location which must be found as part of the solution.

The solution of the classical obstacle problem can be non-smooth at a free boundary even when the data is arbitrarily smooth.  For example, smoothing the source term $f$ in \eqref{eq:icelikedetails} would give a solution nearly the same as shown in Figure \ref{fig:icelike}.  One could make the source term $f$ into a smooth function by ``mollification'' \cite{Evans2010} so that the transition between positive and negative values would be $C^\infty$.  However, the free boundary would remain, and at that free boundary the second derivative $u''$ would jump from value $+16=-f$ to value $-2=\varphi''$.  In general it is known that for smooth $C^\infty$ data $f$ and $\phi$ the obstacle problem solution $u$ is limited to the Sobolev space $W^{2,\infty}$ \cite[section IV.6]{KinderlehrerStampacchia1980}, and not smoother.

\subsection*{Projected Gauss-Seidel}  Now consider the FE method for VI \eqref{eq:obstacleviresidual}.  On the $j$th level we will define an obstacle $\phi^j$ in $\mathcal{V}^j$, a constraint set $\mathcal{K}^j = \{v \ge \phi^j\} \subset \mathcal{V}^j$, and a residual function $F^j$.  Then the FE method seeks $y^j$ in $\mathcal{K}^j$ so that a finite-dimensional VI holds:
\begin{equation}
  F^j(y^j)[v-y^j] \ge 0 \quad \text{ for all } v \text{ in } \mathcal{K}^j. \label{eq:feobstacleviresidual}
\end{equation}

If we were to solve on the fine-level only, in a single-level method, then we would choose the obstacle $\phi^J=\varphi^J$ to be the piecewise-linear interpolant of the continuum obstacle $\varphi$, and $F^J$ as the original residual $F^J(w)[\cdot] = a(w,\cdot) - \ip{f}{\cdot}$.  However, in a multilevel method the constructions of $\phi^j$ and $F^j$ are nontrivial, including on the finest level.  For the classical obstacle problem we will always be able to write $F^j(w)[\cdot] = a(w,\cdot) - \ell^j[\cdot]$ for some linear functional $\ell^j$ in $(\mathcal{V}^j)'$, but we will have more to say regarding the obstacle $\phi^j$.

As with the linear Poisson problem, we can immediately propose an iterative method for solving \eqref{eq:feobstacleviresidual}, namely \emph{projected Gauss-Seidel} (PGS).  Supposing $w$ in $\mathcal{K}^j$ is any iterate, we will modify $w$ at the $p$th node so that \eqref{eq:feobstacleviresidual} holds ``at'' that node.  Note that if $w$ is admissible then $w+c\psi_p^j$ is admissible if and only if $w_p + c \ge \phi_p$ where $\phi_p = \phi^j(x_p^j)$.  (\emph{Warning.}  This equivalence holds for piecewise-linear elements, but not for higher-order polynomial elements.)

For clarity we describe PGS using the constrained minimization form \eqref{eq:obstaclemin}.  We define a convex scalar function based on the objective functional,
\begin{equation}
i(b) = I^j(w+b\psi_p^j),
\end{equation}
where $b$ is a real number and $I^j(w) = \frac{1}{2} a(w,w) - \ell^j[w]$, and we seek its minimizer over admissible perturbations,
\begin{equation}
  c = \argmin_{b \ge \phi_p - w_p} \, i(b).  \label{eq:pgsminimization}
\end{equation}
The minimum of $i(b)$ on the interval $\phi_p - w_p \le b < \infty$ occurs at either the critical point $i'(b)=0$ or at the left end of the interval.  Since $i'(b) = F^j(w)[\psi_p^j] + b a(\psi_p^j,\psi_p^j)$, we have
\begin{equation}
  c = \max\{-F^j(w)[\psi_p^j]/a(\psi_p^j,\psi_p^j), \phi_p - w_p\}  \label{eq:pgsformula}
\end{equation}
as the solution of \eqref{eq:pgsminimization}.  Then we update $w \gets w + c\psi_p^j$ and proceed to the next point.

The minimization view is not essential, however, and we can derive \eqref{eq:pgsformula} using only the VI form.  Given an iterate $w$ in $\mathcal{K}^j$ we seek $c$ such that $w+c\psi_p^j$ is also admissible and so that \eqref{eq:feobstacleviresidual} holds with all admissible $v=w+\tilde c\psi_p^j$.  That is, we find $c\ge \phi_p-w_p$ so that
\begin{equation}
  F^j(w+c\psi_p^j)[(w+\tilde c\psi_p^j) - (w+c\psi_p^j)] = (\tilde c - c) F^j(w+c\psi_p^j)[\psi_p^j] \ge 0,  \label{eq:pgspointwisevi}
\end{equation}
for all $\tilde c\ge \phi_p-w_p$.  Inequality \eqref{eq:pgspointwisevi} is a one-dimensional VI of form $(\tilde c - c)g(c) \ge 0$, on an interval, thus $g(c)=0$ or $c$ is at the end of the interval, which yields \eqref{eq:pgsformula}.

The following pseudocode, a small modification of \pr{gssweep}, implements PGS.
\begin{pseudo*}
\pr{pgssweep}(j,w,F,\phi)\text{:} \\+
    \ct{check admissibility: $w\ge \phi$} \\
    for $p=1,\dots,m_j$ \\+
        $c = \max\{-F(w)[\psi_p^j] \,\big/\, a(\psi_p^j,\psi_p^j),\, \phi(x_p^j)-w(x_p^j)\}$ \\
        $w \gets w + c \psi_p^j$
\end{pseudo*}
PGS converges to the solution $y^j$ of problem \eqref{eq:feobstacleviresidual} \cite[Proposition 4.5]{GraeserKornhuber2009}, but slowly on fine meshes.

However, note that $\|F(w)\|$ does not go to zero (in general) at PGS convergence.  Recalling the CP formulation of the problem \eqref{eq:obstaclecp}, at convergence we have $F(w)[\psi_p^j] = 0$ where $w(x_p^j) > \phi(x_p^j)$ (inactive points), but if $w(x_p^j) = \phi(x_p^j)$ (active points) then $F(w)[\psi_p^j]$ is only required to be nonnegative.  Thus the convergence criterion for iterated PGS sweeps is that the norm of the \emph{CP residual}, the vector
\begin{equation}
  (\hat \bF(w))_p = \begin{cases} F(w)[\psi_p^j], & w(x_p^j) > \phi(x_p^j), \\
                                  \min\{F(w)[\psi_p^j],0\}, & \text{otherwise}, \end{cases} \label{eq:cpresidual}
\end{equation}
in $\RR^{m_j}$, should be small: $\|\hat\bF(w)\| < \text{\texttt{rtol}}$.

PGS will serve as our smoother and coarse-level solver, applied one or two times on each level.  However, we are not yet prepared to build a multilevel method for problem \eqref{eq:feobstacleviresidual} because the construction of the obstacles $\phi^j$ and residuals $F^j$ on each mesh level is nontrivial.  In particular, we will \emph{not} choose $\phi^j$ to be the $j$th-level interpolant of the continuum obstacle $\varphi$, but instead decompose a fine-level constraint across the mesh-level hierarchy.

\subsection*{Multilevel constraint decomposition (MCD)}  Consider the coarse-level correction $e^{j-1}$ in a linear V-cycle (section \ref{sec:subspace}, equation \eqref{eq:coarsecorrection}), which is prolonged onto the $j$th level.  In the obstacle problem the resulting iterate ($w+Pe^{j-1}$) would need to be admissible, that is, above $\phi^j$.  However, as illustrated in Figure \ref{fig:prolongobstacle}, an iterate which is admissible on the $j-1$ level is \emph{not} necessarily admissible on the $j$ level if the obstacles are interpolants of a (common) continuum obstacle $\varphi$.  (In the glacier context, an admissible coarse-mesh ice surface may be breached by the fine-mesh bed topography.)  To resolve this issue we might try an ad hoc fix, namely overwriting $\tilde w = w+Pe^{j-1}$ with $\max\{\tilde w,\phi^j\}$, that is, enforce admissibility by \emph{truncation}.  However, this may reintroduce high frequencies, requiring additional smoother effort to remove.

\begin{figure}
\qquad \includegraphics[width=0.75\textwidth]{genfigs/prolongobstacle.pdf}
\caption{An admissible iterate on the coarse level, when prolonged, may not be admissible on the fine level if the obstacles interpolate a common continuum obstacle.}
\label{fig:prolongobstacle}
\end{figure}

So, how do we maintain admissibility throughout the V-cycle while not introducing high-frequencies?  One way forward is the \emph{multilevel constraint decomposition} (MCD) method of Tai \cite{Tai2003}, which can be formulated as a slash-cycle \cite[Algorithm 4.7]{GraeserKornhuber2009} or a V-cycle.

Start with $w^J$ in $\mathcal{V}^J$, on the finest level, which is admissible in the original sense, namely
\begin{equation}
  w^J \ge \varphi^J. \label{eq:fineadmissibleiterate}
\end{equation}
(Here $\varphi^J$ is the interpolant of the continuum obstacle $\varphi$, i.e.~$\varphi^J(x_p^J)=\varphi(x_p^J)$.)  We define the \emph{defect constraint} \cite{GraeserKornhuber2009} of $w^J$ to be
\begin{equation}
  \chi^J = \varphi^J - w^J,  \label{eq:defectconstraint}
\end{equation}
which is nonpositive ($\chi^J\le 0$).  The idea behind defining the defect constraint is that if we modify $w^J$ by adding $y$ then the result is admissible if and only if $y$ is above it:
\begin{equation}
  w^J + y \ge \varphi^J  \qquad \iff \qquad y \ge \chi^J.  \label{eq:defectmeaning}
\end{equation}

Now our intention is to \emph{put as much as possible of $\chi^J$ into the coarsest levels.}  Corrections on the coarse levels are inexpensive, so we seek to make the largest corrections there.  To decompose $\chi^J$ we use a \emph{monotone restriction} operator $\mR$ from $\mathcal{V}^j$ to $\mathcal{V}^{j-1}$ \cite[equation (4.22)]{GraeserKornhuber2009}, a nonlinear operator.  For $z$ in $\mathcal{V}^j$ one computes $\mR z$ by maximizing the node values of $z$ over the interior of the support of each $j-1$ level hat function.  That is, supposing $z = \sum_q z_q \psi_q^j$, we define
\begin{equation}
  \mR z = \sum_{p=1}^{m_{j-1}} \zeta_p \psi_p^{j-1} \qquad \text{where} \qquad \zeta_p = \max \{z_q \,:\, \psi_p^{j-1}(x_q^j) > 0\}.  \label{eq:monotonerestriction}
\end{equation}
Observe that $\mR z \ge z$ and that $\mR$ acts on functions $\mathcal{V}^j$ while canonical restriction $R$ acts on linear functionals $(\mathcal{V}^j)'$.

We then define the \emph{$j$th-level defect constraint} inductively by
\begin{equation}
  \chi^{j-1} = \mR \chi^j  \label{eq:chik}
\end{equation}
for $j=J$ down to $j=1$.  These defect constraints have two key properties: $\chi^j$ is in $\mathcal{V}^j$ and $\chi^j \le \chi^{j-1}$.  (Formally one might say ``$\chi^j \le P \chi^{j-1}$'', using the canonical prolongation operator \eqref{eq:canonicalprolongation}, so as to compare functions in the same space.)  Now the \emph{$j$th-level obstacle} is the difference:
\begin{equation}
  \phi^j = \chi^j - \chi^{j-1} \quad \text{ for } j=0,1,\dots,J,  \label{eq:levelobstacle}
\end{equation}
where we also define $\chi^{-1}=0$ so that $\phi^0 = \chi^0$.  Note $\phi^j\le 0$ is in $\mathcal{V}^j$.

We have now decomposed the defect constraint $\chi^J$ via a ``telescoping'' sum:
\begin{equation}
  \sum_{j=0}^J \phi^j = \chi^0 + (\chi^1 - \chi^0) + (\chi^2 - \chi^1) + \dots + (\chi^J - \chi^{J-1}) = \chi^J.  \label{eq:telescopingdecomposition}
\end{equation}
(Note that this sum does \emph{not} decompose the original obstacle $\varphi^J$.)  An example is shown in Figure \ref{fig:gooddecomposition}; compare Figure 1 in \cite{GraeserKornhuber2009}.  The obstacles $\phi^j$ are the gaps between the plotted defect constraints $\chi^j$.

%REGENERATE Figures \ref{fig:gooddecomposition} and \ref{fig:icelikedecomposition}:
%$ ./obstacle.py -jfine 5 -jcoarse 1 -irtol 1.0e-7 -random -randommodes 8 -diagnostics -up 0 -o defect.pdf
%fine level 5 (m=63) using 52 V(1,0) cycles (102.375 WU)
%...
%saving hierarchical decomposition to decomp_defect.pdf ...
%saving "ice-like" decomposition to icedec_defect.pdf ...
\begin{figure}
\includegraphics[width=0.75\textwidth]{fixfigs/decomp_defect.pdf}
\caption{The MCD method writes a fine-level defect constraint $\chi^J = \varphi^J - w^J$ as a sum \eqref{eq:telescopingdecomposition} of obstacles $\phi^j = \chi^j - \chi^{j-1}$ on each level.}
\label{fig:gooddecomposition}
\end{figure}

From the obstacles $\phi^j$ we finally define the closed, convex \emph{constraint sets}
\begin{equation}
\mathcal{K}^j = \left\{v \ge \phi^j\right\} \subseteq \mathcal{V}^j \label{eq:defineKj}
\end{equation}
for $j=0,1,\dots,J$.  Because $\phi^j \le 0$, the zero function is in each $\mathcal{K}^j$.  On the finest level, note that $\phi^J$ is not the same as $\varphi^J$, and so $\mathcal{K}^J$ does not approximate $\mathcal{K}$.  Instead, as suggested by Figure \ref{fig:gooddecomposition}, if the $J$th level is a high-resolution mesh and the defect constraint $\chi^J$ is smooth then $\phi^J\approx 0$, because $\chi^J$ is well-approximated on the $J-1$ level.

Corresponding to the defect constraint $\chi^J$ on the fine level, we also define the fine-level \emph{defect constraint set}
\begin{equation}
  \mathcal{D}^J = \left\{v \ge \chi^J\right\} \subset \mathcal{V}^J.
\end{equation}
The sets $\mathcal{K}^j$ decompose $\mathcal{D}^J$:
\begin{equation}
  \mathcal{D}^J = \mathcal{K}^0 + \mathcal{K}^1 + \dots + \mathcal{K}^J. \label{eq:constraintdecomposition}
\end{equation}
(The proof uses the telescoping sum \eqref{eq:telescopingdecomposition}.)  That is, every admissible modification of the current fine-mesh iterate, i.e.~every $y$ from $\mathcal{V}^J$ for which $y\ge \chi^J$, can be built by choosing a function from each of the mesh-level constraint sets $\mathcal{K}^j$.  As with \eqref{eq:subspacedecomposition}, wherein the mesh-level subspaces $\mathcal{V}^j$ decompose the whole FE space $\mathcal{V}^h$, this representation is usually not unique.  Note that constraint decomposition \eqref{eq:constraintdecomposition} is fundamental to any application of the MCD method in \cite{Tai2003}, but our specific choices follow \cite{GraeserKornhuber2009} in determining $\mathcal{K}^j$.

In the glacier context (section \ref{sec:sia}) the fine-mesh obstacle $\varphi^J$ will be the bed elevation and $w^J$ will be a candidate ice surface elevation, so $-\chi^J$ will be an ice thickness.  Figure \ref{fig:icelikedecomposition} illustrates the same constraint decomposition as in Figure \ref{fig:gooddecomposition}, but pictured as a decomposition of the ice into layers.  (Compared to Figure \ref{fig:icelike}, this bed elevation $\phi^J$ has bumpy bed topography.)  However, Figure \ref{fig:gooddecomposition} plots the decomposition in the \emph{correct} sense, such that each defect constraint $\chi^j$ and obstacle $\phi^j$ is piecewise-linear (i.e.~in $\mathcal{V}^j$); Figure \ref{fig:icelikedecomposition} is merely heuristic.

\begin{figure}
\includegraphics[width=0.7\textwidth]{fixfigs/icedec_defect.pdf}
\caption{A heuristic view (Figure \ref{fig:gooddecomposition} shows the correct plot): MCD visualized as a decomposition of the ``ice'' between a fine-mesh iterate $w^J$ and the fine-mesh obstacle $\varphi^J$.}
\label{fig:icelikedecomposition}
\end{figure}

We can now write the obstacle problem on the $j$th level in three forms, which are equivalent for the classical obstacle problem.  Recall that $F(w)[v] = a(w,v) - \ip{f}{v}$ denotes the original residual \eqref{eq:residual}, and $I(v) = \frac{1}{2} a(v,v) - \ip{f}{v}$ is the corresponding objective function.  As above, $w^J$ is an admissible iterate on the fine level: $w^J\ge \varphi^J$.  The $j$th-level forms below assume that we have already descended from the fine level $J$ to some level $j+1$, computing admissible corrections $y^k$ from the constraint set $\mathcal{K}^k$ for $k=j+1,\dots,J$.  By  \eqref{eq:defectmeaning} and the constraint decomposition \eqref{eq:constraintdecomposition}, observe that $z = w^J+y^J+\dots+y^{j+1}$ is admissible in the original sense, i.e.~$z\ge \varphi^J$.

We seek a $j$th-level correction $y^j$ in $\mathcal{K}^j$, i.e.~$y^j\ge \phi^j$, such that:
\begin{itemize}
\item variational inequality:
\begin{equation}
  F(w^J+y^J+\dots+y^{j+1}+y^j)[v - y^j] \ge 0 \qquad \text{for all } v \text{ in } \mathcal{K}^j.  \label{eq:mcdvi}
\end{equation}
\item constrained minimization:
\begin{equation}
  y^j = \argmin_{v \text{ in } \mathcal{K}^j} I(w^J+y^J+\dots+y^{j+1}+v).  \label{eq:mcdminimization}
\end{equation}
\item linear variational inequality:
\begin{equation}
  F^j(y^j)[v - y^j] \ge 0 \qquad \text{for all } v \text{ in } \mathcal{K}^j.   \label{eq:mcdvilinear}
\end{equation}
\end{itemize}

In form \eqref{eq:mcdvilinear} we have used linearity to define $F^j$:
\begin{align}
  F^j(z)[\cdot] &= F(w^J+y^J+\dots+y^{j+1}+z)[\cdot] \label{eq:residuallinearlevelderive} \\
                &= a(w^J+y^J+\dots+y^{j+1}+z,\cdot) - \ip{f}{\cdot} \notag \\
                &= a(z,\cdot) + F^{j+1}(y^{j+1})[\cdot]. \notag
\end{align}
That is, on each level we have a residual functional,
\begin{equation}
  F^j(z)[\cdot] = a(z,\cdot) - \ell^j[\cdot],  \label{eq:residuallinearlevel}
\end{equation}
with an inductively-defined source:
\begin{equation}
  \ell^j[\cdot] = \begin{cases} - F^{j+1}(y^{j+1})[\cdot], & j < J, \\
                                - F(w^J)[\cdot], & j = J. \end{cases} \label{eq:rhslinearlevel}
\end{equation}
Note that each $F^j$ here acts on $\mathcal{V}^h$, that is, on whole the FE space, and all its nested subspaces, and $\ell^j$ is in $(\mathcal{V}^h)'$.  However, in an implemented multigrid cycle (below) we will approximate $\ell^j$ by $\tilde \ell^j$ in $(\mathcal{V}^j)'$, using a representation in $j$th-level values $\{\ell^j[\psi_p^j]\}$.

For the classical obstacle problem, wherein the (continuum) residual $F(w)[v]$ is built from a symmetric bilinear form $a(w,v)$, the three problems \eqref{eq:mcdvi}--\eqref{eq:mcdvilinear} are equivalent.  However, the equivalence of \eqref{eq:mcdvi} and \eqref{eq:mcdminimization} depends on symmetry, i.e.~such that $F$ is a gradient, and the equivalence of \eqref{eq:mcdvi} and \eqref{eq:mcdvilinear} depends on bilinearity, and both fail for the glacier problem (section \ref{sec:sia}):
   $$\eqref{eq:mcdvilinear} \quad \stackrel{a \text{ bilinear}}{\iff\strut} \quad \eqref{eq:mcdvi} \quad \stackrel{F=\grad I}{\iff\strut} \quad \eqref{eq:mcdminimization}.$$
The nonlinear VI form \eqref{eq:mcdvi} is the most general, but note that if $F$ is not the gradient of a coercive objective function then additional structural properties of $F$ are required for well-posedness (e.g.~monotonicity \cite{Bueler2020,KinderlehrerStampacchia1980}).

\subsection*{MCD cycles and convergence results}  We are now ready to implement a multilevel solver for the classical obstacle problem.  The following pseudocode solves \eqref{eq:mcdvilinear}.  Here \pr{MCDL} stands for ``multilevel constraint decomposition (linear)'', and this procedure is analogous to \pr{msd-solver} in section \ref{sec:subspace}.  This outer loop sets up the fine-level defect constraint and residual.  Checks for convergence occur after calling the inner solver, addressed next.
\begin{pseudo*}
\pr{mcdl-solver}(F,\varphi,\id{rtol}=10^{-4})\text{:} \\+
    $w=\varphi$ \qquad\qquad\qquad\qquad\qquad\quad \ct{or other admissible fine-level initial iterate} \\
    $r_0=\|\hat\bF(w)\|$ \qquad\qquad\qquad\qquad\qquad \ct{initial CP residual norm; see \eqref{eq:cpresidual}} \\
    repeat \\+
        $\chi^J = \varphi - w$ \qquad\qquad\qquad\qquad\quad \ct{fine-level defect constraint} \\
        $\ell^J[\cdot] := - F(w)[\cdot]$ \qquad\qquad\qquad\quad \ct{see \eqref{eq:rhslinearlevel}} \\
        $F^J(y)[\cdot] := a(y,\cdot) - \ell^J[\cdot]$ \\
        $y=\pr{mcdl-slash}(J,F^J,\chi^J)$ \\
        $w\gets w+y$ \\-
    until $\|\hat\bF(w)\| \le r_0 \id{rtol}$  \qquad\qquad\qquad \ct{see \eqref{eq:cpresidual}} \\
    return $w$
\end{pseudo*}

The inner solver is a slash cycle, also known as V(1,0) \cite{GraeserKornhuber2009}.  (Figure \ref{fig:msdcycles} shows slash and true V-cycles.)  It is based on formulae \eqref{eq:chik}, \eqref{eq:levelobstacle}, \eqref{eq:residuallinearlevel}, and \eqref{eq:rhslinearlevel}.  Before computing the coarse-level correction, we approximate the residual by applying canonical restriction,
\begin{equation}
  \ell^{j-1}[\cdot] = - R(F^j(y^j))[\cdot] \label{eq:rhslinearlevelcycle}
\end{equation}
for $j=J,\dots,1$.  As noted in section \ref{sec:subspace}, this loses residual information for the high-frequency modes on the $j$th level, but the smoother has minimized the loss.  As a pseudocode:
\begin{pseudo*}
\pr{mcdl-slash}(J,F^J,\chi^J,\id{down}=1,\id{coarse}=1)\text{:} \\+
    for $j=J$ downto $j=1$ \\+
      $\chi^{j-1} = \mR \chi^j$ \\
      $\phi^j = \chi^j - P\chi^{j-1}$ \qquad\qquad\qquad\quad \ct{define obstacle} \\
      $y^j = 0$ \\
      $\text{\pr{pgssweep}}^{\text{\id{down}}}(j,y^j,F^j,\phi^j)$ \qquad\quad \ct{down smoother} \\
      $\ell^{j-1}[\cdot] := - R (F^j(y^j))[\cdot]$ \qquad\qquad \ct{update and restrict residual} \\
      $F^{j-1}(y)[\cdot] := a(y,\cdot) - \ell^{j-1}[\cdot]$ \\-
    $y^0 = 0$ \\
    $\text{\pr{pgssweep}}^{\text{\id{coarse}}}(0,y^0,F^0,\chi^0)$ \qquad\quad \ct{coarsest-level correction} \\
    for $j=1$ to $j=J$ \\+
      $y^j \gets y^j + P y^{j-1}$ \qquad\qquad\qquad\quad \ct{accumulate corrections} \\-
    return $y^J$
\end{pseudo*}

In the implementation, the functions $F^j(y)[\cdot]$ are not passed as functions in the pro\-gramming-language sense.  Rather, noting that the FE method includes a subroutine which evaluates $a(w,v)$ as needed, the linear functional $\ell^{j-1}$ is passed as a (row) vector, from which $F^{j-1}$ is constructed.  That is, the implementation signatures are actually \pr{pgssweep}$(j,y^j,\ell^j,\phi^j)$ and \pr{mcdl-slash}$(J,\ell^J,\chi^J)$, with the understanding that \eqref{eq:residuallinearlevel} defines $F^j$.

FIXME error norms for slash cycle on two problems: icelike and parabola; demonstrates convergence rate

% regenerate:
%   $ cd py/1D/
%   $ ./obstacle.py -plain -jfine 7 -o parabola.pdf -problem parabola
%   $ pdfcrop parabola.pdf parabola.pdf
\begin{figure}
\includegraphics[width=0.7\textwidth]{fixfigs/parabola.pdf}
\caption{A classic configuration with a parabolic obstacle.}
\label{fig:parabola}
\end{figure}

\begin{figure}
\includegraphics[width=0.6\textwidth]{genfigs/convergence.pdf}
\caption{Convergence of the numerical error norm $\|u-u_{\text{exact}}\|_2$ as a function of mesh spacing $h$ for the problems shown in Figures \ref{fig:icelike} and \ref{fig:parabola}.}
\label{fig:convergence}
\end{figure}


\subsection*{Nested iteration and performance results} FIXME describe nested iteration; give pseudocode \pr{mcdl-fcycle}; mention V(1,1) in Appendix B

FIXME explain WU

FIXME give figure with WU on $y$-axis and $N$ on $x$-axis for PGS only, slash cycle, V(2,0), F-cycle, and 2-iterations-on-way-up-F-cycle (5 solvers) on two problems: icelike and icelike-with-random-bed

FIXME run time per degree of freedom for same runs

\subsection*{Convergence theory and performance models}  Mathematical theory allows us to compare the convergence rates of our MCD solver versus the PGS method.  Let $u^h$, in $\mathcal{V}^h = \mathcal{V}^J$ and satisfying $u^h \ge \varphi^J$, be the exact solution of the finite-dimensional VI
\begin{equation}
  F(u^h)[v-u^h] \ge 0 \quad \text{ for all } v \text{ in $\mathcal{V}^h$ such that } v \ge \varphi^J. \label{eq:feobstaclevioriginal}
\end{equation}
This is the direct FE approximation of \eqref{eq:obstacleviresidual}; compare \eqref{eq:feobstacleviresidual} which uses the decomposition of the defect constraint.  Recall that $F=\grad I$ for an objective function $I(w)$ (see \eqref{eq:obstaclemin}).  Denote the spacing of the fine ($J$th) mesh by $h$.

First consider the PGS method.  Note that the following theorem describes the norm of the algebraic error of an iterate, i.e.~$\|w-u^h\|_{\mathcal{H}}$, relative to the objective value on the initial iterate.

\begin{theorem} \cite[Prop.~4.5]{GraeserKornhuber2009}\,  \label{thm:pgsconvergence}  Suppose $w^{(s)}$  results from $s$ applications of \pr{pgssweep} on the fine level, starting with any $w^{(0)}$.  There is $C>0$ independent of $h$ so that
\begin{equation}
  \|w^{(s)} - u^h\|_{\mathcal{H}}^2 \le 2 (1-C h^2)^s\,\left(I(w^{(0)}) - I(u^h)\right).  \label{eq:pgsconvergence}
\end{equation}
\end{theorem}

Thus the asymptotic convergence rate at which $\|w^{(s)} - u^h\|_{\mathcal{H}} \to 0$ is $O(\rho_{\text{PGS}}^s)$ where
    $$\rho_{\text{PGS}} = \sqrt{1-Ch^2} = 1 - O(h^2).$$
Thus as we refine and $h\to 0$ the value of $\rho_{\text{PGS}}$ goes to one.  In fact if we seek to reduce the error norm below $\eps$ times its original value then we expect to need $O(|\log\eps|/h^2)$ iterations.  The number of iterations increases rapidly as $h\to 0$.

\begin{theorem} \cite[Thm.~4.6]{GraeserKornhuber2009}\,  \label{thm:mcdlconvergence}  FIXME Suppose $w^{(s)}$  results from $s$ applications of \pr{mcdl-slash} on the fine level, starting with any $w^{(0)}$.  There is $\rho<1$ so that
\begin{equation}
  \|w^{(s+1)} - u^h\|_{\mathcal{H}} \le \rho \|w^{(s)} - u^h\|_{\mathcal{H}}.  \label{eq:mcdlconvergence}
\end{equation}
\end{theorem}

FIXME Compare Theorem \ref{thm:msdconvergence}; model performance when $h=O(2^{-J})$ and $N=2^{J+1}-1=O(2^J)$ so $J = O(\log N)$; Table \ref{tab:performancemodels}; all constants proportional to $|\log\eps|$

\begin{table}
\begin{tabular}{l|l|l}
\emph{method} & \emph{iterations} & \emph{work} \\ \hline
\pr{pgssweeps} & $O(N^2)$ & $O(N^3)$ \\
\pr{mcdl-solver} & $O(\log N)$ & $O(N \log N)$ \\ \hline
\pr{gmg-vcycle} & $O(1)$ & $O(N)$
\end{tabular}

\medskip
\label{tab:performancemodels}
\caption{Number of iterations to reduce the error norm $\|w-u^h\|_{\mathcal{H}}$ by a factor of $\eps$, and amount of work (flops) as degrees of freedom $N\to\infty$.}
\end{table}

FIXME in Table \ref{tab:performancemodels}, the first two solvers are for the classical obstacle problem, but \pr{gmg-vcycle} solves the linear Poisson equation

FIXME there are other fast solution methods, e.g.~PFAS in \cite{BrandtCryer1983} and inactive-set Newton-Multigrid \cite[Chapter 12]{Bueler2021}, and monotone and truncated multigrid \cite{GraeserKornhuber2009}, but theory is more complicated or missing

\section{Multigrid for the shallow-ice mass conservation problem} \label{sec:sia}

FIXME model problem in previous section has the wrong ``physics'' but correctly addresses the free boundary and obstacle nature of the glacier problem; cite for glaciers as obstacle problems \cite{Bueler2016,Bueler2020,Calvoetal2002,JouvetBueler2012,Jouvetetal2013}

FIXME for 2D domains the coarse mesh construction needs reconsideration


\section{Multigrid for Glen-Stokes glacier flow} \label{sec:stokes}

FIXME multigrid already used for Blatter-Pattyn model \cite{BrownSmithAhmadia2013} and for hybrid \cite{Jouvetetal2013}; one goal of this section is to make these approaches more understandable

FIXME we use Schur complement \cite{Bueler2021,Elmanetal2014} and compare it to Vanka monolithic smoother \cite{Farrelletal2019}

\small

\bigskip
\bibliography{review}
\bibliographystyle{siam}

\normalsize

%\clearpage
\appendix
\section{Notation}

\renewcommand{\arraystretch}{1.2}
\begin{longtable}{l|l}
\textbf{Symbol} {\Large$\strut$} & \textbf{Meaning} \\ \hline
$a(\cdot,\cdot)$ & bilinear form associated to the Poisson problem; left side of equation \eqref{eq:weakpoissonearly} \\
$\mathcal{D}^J$ & defect constraint set; $\mathcal{D}^J = \{v \ge \chi^J\}$ \\
$F(w)[\cdot]$ & residual of iterate $w$, e.g.~$F(w)[v] = a(w,v) - \ip{f}{v}$ for the Poisson equation \\
$\mathcal{H}$ & Hilbert space for the continuum problem; e.g.~$\mathcal{H}=H_0^1[0,1]$ \\
$I(w)$ & scalar-valued objective function for the Poisson problem; $I(w) = \frac{1}{2} a(v,v) - \ip{f}{v}$ \\
$J$ & level index of finest mesh \\
$j$ & index of mesh level; $j=0,1,\dots,J$ from coarse to fine \\
$\mathcal{K}$ & continuum constraint set; $\mathcal{K} = \{v \ge \varphi\} \subset \mathcal{H}$ \\
$\mathcal{K}^j$ & $j$th-level admissible functions; $\mathcal{K}^j = \{v \ge \phi^j\} \subset \mathcal{V}^j$ \\
$\ell[\cdot]$ & linear functional, e.g.~$\ell[v] = \ip{f}{v}$ \\
$m_j$ & number of nodes in $j$th-level; $\dim \mathcal{V}^j=m_j$ \\
$N$ & number of degrees of freedom in discretized problem \\
$P$ & canonical prolongation of functions, $\mathcal{V}^{j-1} \to \mathcal{V}^j$; see \eqref{eq:canonicalprolongation} and Table \ref{tab:restrictionsprolongations} \\
$\iP$ & injection prolongation of linear functionals, $(\mathcal{V}^{j-1})' \to (\mathcal{V}^j)'$; see \eqref{eq:injectprolong} and Table \ref{tab:restrictionsprolongations} \\
$R$ & canonical restriction of linear functionals, $(\mathcal{V}^j)' \to (\mathcal{V}^{j-1})'$; see \eqref{eq:canonicalrestriction} and Table \ref{tab:restrictionsprolongations} \\
$\mR$ & monotone restriction of functions, $\mathcal{V}^j \to \mathcal{V}^{j-1}$; see \eqref{eq:monotonerestriction} and Table \ref{tab:restrictionsprolongations} \\
$\mathcal{V}^h$ & finite element function space; $= \mathcal{V}^J$ \\
$\mathcal{V}^j$ & $j$th-level vector space \\
$(\mathcal{V}^j)'$ & dual space (linear functionals) of $\mathcal{V}^j$  \\
$x_p^j$ & $p$th node on $j$th-level mesh \\
$\varphi(x)$ & obstacle in continuum problem \\
$\varphi^J(x)$ & fine-level interpolant of continuum obstacle \\
$\phi^j(x)$ & $j$th-level obstacle in MCD method; $\phi^j=\chi^j - P\chi^{j-1}$ \\
$\chi^J(x)$ & fine-level defect obstacle; $\chi^J = \varphi^J - w^J$ for iterate $w^J$ \\
$\chi^j(x)$ & $j$th-level (monotone) restriction of defect obstacle; $\chi^{j-1} = \mR \chi^j$ \\
$\psi_p^j(x)$ & $j$th-level hat function at $x_p$ \\
$\ip{\cdot}{\cdot}$ & $L^2$ inner product \\
$\|\cdot\|$ & $L^2$ norm; $\|f\|=\ip{f}{f}^{1/2}$ \\
$\|\cdot\|_{\mathcal{H}}$ & norm on solution space; $\|f\|_{\mathcal{H}}^2 =\ip{f}{f} + \ip{f'}{f'}$ for $\mathcal{H}=H_0^1[0,1]$
\end{longtable}


\section{Mysterious V(1,1) cycles}  In section \ref{sec:obstacle} we implemented a ``slash cycle'', also known as a V(1,0) cycle because we only do smoothing downward in the mesh hierarchy.  The implementation of a true V-cycle for an obstacle problem, a so-called V(1,1)-cycle \cite{GraeserKornhuber2009} with smoothing after the coarse-level correction, requires reconsideration of the constraint decomposition and an additional prolongation operation.  As always, each correction from a different level needs to yield an admissible iterate, and in a V-cycle this applies on the way up, too.

Gr\"aser and Kornhuber \cite{GraeserKornhuber2009} implement the constraint decomposition in a particular manner which we have implemented, but only unsuccessfully regarding convergence.  They keep the $j$th-level defect constraints $\chi^j$ \eqref{eq:chik} the same, but split the $j$th-level obstacles $\phi^j$ in half and then apply them twice, yielding the following formulae for $j=1,\dots,J$:
\begin{equation}
\phi_V^j = \frac{1}{2}(\chi^j-\chi^{j-1}), \qquad \mathcal{K}_V^j = \{v \ge \phi_V^j\} \subset \mathcal{V}^j.
\end{equation}
The coarsest-level obstacle remains a special case, $\phi_V^0 = \chi^0,\mathcal{K}_V^0 = \{v \ge \chi^0\}$.  A telescoping sum like \eqref{eq:telescopingdecomposition}, but now with negative indices for going down and positive for going up, retains a consistent decomposition \cite{GraeserKornhuber2009,Tai2003}:
\begin{equation}
\chi^J = \sum_{j=-J}^J \phi_V^{|j|}, \qquad \mathcal{D}^J = \sum_{j=-J}^J \mathcal{K}_V^{|j|}.
\end{equation}
On the way up we denote the corrections by $\eta^j$.  Computation \eqref{eq:residuallinearlevelderive} is thus extended with additional terms:
\begin{equation}
  \tilde F^j(z)[\cdot] = F(w^J + y^J + \dots + y^0 + \eta^1 + \dots + \eta^{j-1} + z)[\cdot] = a(z,\cdot) - \tilde \ell^j[\cdot] \label{eq:residuallinearlevelv}
\end{equation}
where $\tilde\ell^j[\cdot] = - \tilde F^{j-1}(\eta^{j-1})[\cdot]$ for $j>1$ and $\tilde\ell^1[\cdot] = - F^0(y^0)[\cdot]$.  However, to use $\tilde\ell^j$ on the $j$th level, in a representation $\tilde{\bm{\ell}} = \{\tilde\ell^j[\psi_q^j]\}$ as a vector in $\RR^{m_j}$, it must be prolonged.  (This is the first time we have prolonged a linear functional; see Table \ref{tab:restrictionsprolongations}.)  Suppose $\ell$ is in $(\mathcal{V}^{j-1})'$ and that we want to compute its prolongation $\lambda$, a linear functional in $(\mathcal{V}^j)'$.  Recalling \eqref{eq:hatcombination} once again, we need to determine the values $\lambda[\psi_q^j]$ so that
\begin{equation}
  \sum_{q=1}^{m_j} c_{pq} \lambda[\psi_q^j] = \ell[\psi_p^{j-1}] \label{eq:dualprolongderive}
\end{equation}
for $p=1,\dots,m_{j-1}$, where $c_{pq} = \psi_p^{j-1}(x_q^j)$.  Equation \eqref{eq:dualprolongderive} is an underdetermined system of $m_{j-1}$ equations in $m_j$ unknowns, and it is exactly-solvable in many different ways.  However, noting that $c_{pq}=1$ if $x_q^j=x_p^{j-1}$, we may choose $\lambda[\psi_q^j] = \ell[\psi_p^{j-1}]$ when the nodes coincide, and choose the other values to be zero, and this yields \emph{injection prolongation},
\begin{equation}
  (\iP\ell)[\psi_q^j] = \begin{cases} \ell[\psi_p^{j-1}], & x_q^j=x_p^{j-1}, \\
                                      0, & \text{otherwise}. \end{cases}  \label{eq:injectprolong}
\end{equation}
In other words, we keep the values of $\ell$ on the coarser-level nodes and just fill-in with zero.  Using this prolongation gives a $j$th-level residual that is not smooth, but the solution of the equation on the $j$th level (i.e.~$\tilde F^j(y^j)[v]=0$ for all $v$ in $\mathcal{K}^j$) will be relatively smooth by ellipticity.  These ideas come together in the following pseudocode.
\begin{pseudo*}
\pr{mcdl-vcycle}(J,F^J,\chi^J,\id{down}=1,\id{up}=1,\id{coarse}=1)\text{:} \\+
    for $j=J$ downto $j=1$ \\+
      $\chi^{j-1} = \mR \chi^j$ \\
      $\phi_V^j = \frac{1}{2}(\chi^j - P\chi^{j-1})$ \qquad\qquad\quad  \ct{define obstacle} \\
      $y^j = 0$ \\
      $\text{\pr{pgssweep}}^{\text{\id{down}}}(j,y^j,F^j,\phi_V^j)$ \\
      $\ell^{j-1}[\cdot] := - R (F^j(y^j))[\cdot]$ \qquad\qquad \ct{update and restrict residual} \\
      $F^{j-1}(z)[\cdot] := a(z,\cdot) - \ell^{j-1}[\cdot]$ \\-
    $y^0 = 0$ \\
    $\text{\pr{pgssweep}}^{\text{\id{coarse}}}(0,y^0,F^0,\chi^0)$ \\
    $\omega^0 = y^0$ \\
    for $j=1$ to $j=J$ \\+
      $\ell^{j}[\cdot] := - \iP(\tilde F^{j-1}(\omega^{j-1}))[\cdot]$ \qquad\quad \ct{update and prolong residual} \\
      $\tilde F^{j}(z)[\cdot] := a(z,\cdot) - \ell^{j}[\cdot]$ \\
      $\eta^j = 0$ \\
      $\text{\pr{pgssweep}}^{\text{\id{up}}}(j,\eta^j,\tilde F^j,\phi_V^j)$ \\
      $\omega^j = y^j + \eta^j + P \omega^{j-1}$ \qquad\qquad\quad \ct{accumulate corrections} \\-
    return $w^J$
\end{pseudo*}
Note that the vectors $\omega^j$ in the second \kw{for} loop are used to add-up (accumulate) the corrections in \eqref{eq:residuallinearlevelv}, $y^j$ from the way down and $\eta^j$ from the way up.  The procedure returns an iterate in $\mathcal{V}^J$,
    $$\omega^J = y^J + \dots + y^1 + y^0 + \eta^1 + \dots + \eta^J,$$
and then the calling solver (\pr{mcdl-solver}) does the update $w^J \gets w^J + \omega^J$.

FIXME unable to get the fine-level residuals to become arbitrarily small, though initially they decrease a little

\end{document}
