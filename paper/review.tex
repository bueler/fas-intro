\documentclass[letterpaper,final,12pt,reqno]{amsart}

\usepackage[total={6.3in,9.2in},top=1.1in,left=1.1in]{geometry}

\usepackage{times,bm,bbm,empheq,fancyvrb,graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{longtable}

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}

\usepackage[kw]{pseudo}
\pseudoset{left-margin=15mm,topsep=5mm,idfont=\texttt}

% hyperref should be the last package we load
\usepackage[pdftex,
colorlinks=true,
plainpages=false, % only if colorlinks=true
linkcolor=blue,   % ...
citecolor=Red,    % ...
urlcolor=black    % ...
]{hyperref}

\renewcommand{\baselinestretch}{1.05}

\newtheoremstyle{claim}% name
  {5pt}% space above
  {5pt}% space below
  {\itshape}% body font
  {}% indent amount
  {}% theorem head font
  {.}% punctuation after theorem head
  {.5em}% space after theorem head
  {\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}% theorem head spec
\theoremstyle{claim}
\newtheorem*{theorem}{Theorem}
\newtheorem*{lemma}{Lemma}

\newcommand{\eps}{\epsilon}
\newcommand{\RR}{\mathbb{R}}

\newcommand{\grad}{\nabla}
\newcommand{\Div}{\nabla\cdot}
\newcommand{\trace}{\operatorname{tr}}

\newcommand{\hbn}{\hat{\mathbf{n}}}

\newcommand{\bb}{\mathbf{b}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bbf}{\mathbf{f}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bn}{\mathbf{n}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}

\newcommand{\bF}{\mathbf{F}}
\newcommand{\bV}{\mathbf{V}}
\newcommand{\bX}{\mathbf{X}}

\newcommand{\bxi}{\bm{\xi}}

\newcommand{\bzero}{\bm{0}}

\newcommand{\rhoi}{\rho_{\text{i}}}

\newcommand{\ip}[2]{\left<#1,#2\right>}

% numbering
\setcounter{tocdepth}{3}
\makeatletter
\def\l@subsection{\@tocline{2}{0pt}{4pc}{5pc}{}}
\makeatother

\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}


\begin{document}
\title[Geometric multigrid for glacier modeling]{Geometric multigrid for glacier modeling: \\ New concepts and techniques}

\author{Ed Bueler}

\begin{abstract} FIXME: two principles in introduction: mass conservation complementarity, solver optimality.  four examples in sections \ref{sec:subspace}--\ref{sec:stokes}: poisson equation from subspace decomp point of view, obstacle problem by subset decomposition, monotone multigrid for implicitly-evolving SIA geometry, Schur-complement and Vanka Newton-multigrid for fixed-geometry Glen-Stokes
\end{abstract}

\maketitle

\tableofcontents

\thispagestyle{empty}
\bigskip

\section{Introduction} \label{sec:intro}

The construction of effective numerical glacier and ice sheet models is challenging for two fundamental reasons.  First is the complexity of the equations and boundary conditions.  Indeed, the physics of glaciers is nonlinear, nontrivially-coupled, and subject to imperfectly-understood boundary processes, such as at contact with ocean water.  The coupling is critical in the sense that mass, momentum, and energy conservation interact in ways which are relevant to glaciological modeling goals, such as when basal sliding, and thus ice velocity, is only determined though a simultaneous momentum and energy solution.  Second, the geometry of glaciers and ice sheets is complex, and in particular the fastest-flowing parts of ice sheets are often located at the geometrically-nontrivial lateral boundary where fjord-like bed geometry is also common.  Numerical models therefore need to perform expensive fine-mesh calculations, so as to accomodate the complicated, changing boundary geometry, while solving relatively-complicated multiphysics equations.

On the other hand, since the 1980s researchers in numerial methods have developed multigrid methods to solve partial differential equations like those which describe the ice fluid in glaciers.   For simpler problems like scalar elliptic equations and the linear Stokes system, especially on domains which have a simpler geometry, these methods are now in routine use \cite{Briggsetal2000,Bueler2021,Trottenbergetal2001}.

FIXME perspectives \emph{not} found here: convergence of GMG (or much detail for application to linear problems); assumptions like ``SPD'' specific to constrained \emph{optimization} as opposed to VI/NCP viewpoint


\section{From subspace decomposition to multigrid} \label{sec:subspace}

\subsection*{Finite elements for a Poisson model problem}  In this section we will demonstrate how to solve a simple differential equation, namely a linear Poisson-like problem
\begin{equation}
- (\alpha(x)\,u'(x))' = f(x) \quad \text{on} \quad 0 \le x \le 1, \label{eq:poisson}
\end{equation}
with Dirichlet boundary conditions $u(0)=u(1)=0$, using a finite element (FE) discretization and a multigrid method.  The functions $\alpha(x)$ and $f(x)$ are given data here, sufficiently well-behaved for the computations which follow, and we assume $\alpha(x)$ is bounded and positive: $0 < c_1 \le \alpha(x) \le c_2$.  Over the course of the next three sections, this simple equation will evolve into a realistic model for glacier geometry.

Our numerical approximation of \eqref{eq:poisson} uses an unequally-spaced mesh of $m$ interior \emph{nodes} (points) $x_p$ on $(0,1)$.  The $m+1$ open intervals between the nodes are the \emph{elements}.  The numerical solution $u^h(x)$ is a linear combination of the piecewise-linear hat functions $\psi_p(x)$, shown in Figure \ref{fig:finehats}, one for each interior node:
\begin{equation}
u^h(x) = \sum_{p=1}^m u_p \psi_p(x). \label{eq:trialsolution}
\end{equation}
Each hat function $\psi_p(x)$ is continuous on $[0,1]$, linear on each element, and satisfies $\psi_p(x_q) = \delta_{pq}$.  The set $\{\psi_p(x)\}_{p=1}^m$ is a \emph{nodal basis} of the space $\mathcal{V}^h$ of continuous, piecewise-linear functions.  That is, the coefficients equal the function values: $u_p=u^h(x_p)$.  Note that the derivative of $u^h(x)$ is well-defined on the elements, thus almost everywhere, but not at the nodes.  In a computer program the coefficients $u_p$ will be formed into a column vector $\bu=\{u_p\}$ in $\RR^m$.

\begin{figure}
\includegraphics[width=0.65\textwidth]{genfigs/finehats.pdf}
\caption{Hat functions $\psi_p(x)$ at interior points $x_p$ form a basis for a vector space $\mathcal{V}^h$ of piecewise-linear functions.}
\label{fig:finehats}
\end{figure}

Our applications of multigrid ideas to glacier problems will be clearest if we adopt an FE approach based on re-phrasing \eqref{eq:poisson} into \emph{weak form} using integrals.  (Accessible introductions to FE methods are in \cite{Bueler2021,Elmanetal2014,Johnson2009}.  Note that the original equation is called the \emph{strong form}.)  To do this we suppose that the exact solution $u(x)$ comes from a vector space $\mathcal{H}$ of functions which are smooth enough to allow the computations which follow and which have value zero at $x=0$ and $x=1$.  While we will generally avoid the language of Banach and Sobolev spaces \cite[for example]{Evans2010}, it will be used when precision is needed, and in fact $\mathcal{H}=H_0^1[0,1]=W_0^{1,2}[0,1]$, and also we are assuming $f(x)$ is in $L^2[0,1]$ and $\alpha(x)$ is in $L^\infty[0,1]$.

The weak form arises by multiplying both sides of \eqref{eq:poisson} by a \emph{test function} from $\mathcal{H}$ and integrating by parts so that only first derivatives remain.  Choosing a test function $v(x)$, integrating on $[0,1]$, and using $v(0)=v(1)=0$, we find
\begin{equation}
\int_0^1 \alpha(x) u'(x) v'(x)\,dx = \int_0^1 f(x) v(x)\, dx.  \label{eq:weakpoissonearly}
\end{equation}
We write equation \eqref{eq:weakpoissonearly} more compactly as
\begin{equation}
  a(u,v) = \ip{f}{v}, \label{eq:weakpoisson}
\end{equation}
defining each side as in \eqref{eq:weakpoissonearly}.  The left side $a(u,v)$ is linear in each argument (\emph{bilinear}) while the right side defines a \emph{linear functional} $\ell[v] = \ip{f}{v}$ acting on $v$ in $\mathcal{H}$.  (The convenience of this abstract notation will become clear as we describe the multigrid algorithms.)

One may now substitute formula \eqref{eq:trialsolution} for $u^h$ into \eqref{eq:weakpoisson} to derive a linear system
\begin{equation}
A \bu = \bbf, \label{eq:linearsystem}
\end{equation}
where $A$ is an $m\times m$ matrix and $\bbf$ is in $\RR^m$.  Each equation (row) in system \eqref{eq:linearsystem} is constructed by using a hat function as a test function; substitution of $v=\psi_p$ into \eqref{eq:weakpoisson} gives the $p$th equation.  The symmetric matrix $A$, with entries $a_{pq} = a(\psi_p,\psi_q)$, is positive definite \cite{Elmanetal2014}.  For the right side one defines entries $f_p = \ip{f}{\psi_p}$ to form the vector $\bbf = \{f_p\}$.  The most straightforward way to solve assembled linear system \eqref{eq:linearsystem} would be via a direct method such as Gaussian elimination.  However, in higher-dimensional PDE problems such methods need much more that $O(m)$ operations to solve the system (where $m$ is the number of unknowns), while, as noted in the introduction, large-scale applications demand optimal $O(m)$ solution methods, or nearly so.  Furthermore, excluding the current section, all of our problems will be nonlinear, thus no finite-time direct method will be available.  We will generally not assemble matrix objects at all, and instead we will construct rapidly-convergent iterations for our FE methods, based on slowly-converging, but easy to implement, pointwise iterations.

\subsection*{Coarse levels in a multilevel decomposition}  From the above simple FE scheme we take the first step to build a \emph{multilevel} scheme.  Consider an enlarged set of hat functions:
    $$\underbrace{\psi_1(x),\dots,\psi_m(x)}_{\text{existing fine level}},\underbrace{\psi_{m+1}(x),\dots,\psi_M(x)}_{\text{\small coarser levels}}$$
For example, two coarser levels are shown in Figure \ref{fig:coarsehats}, derived from the fine level in Figure \ref{fig:finehats} by coarsening.  The first coarsening (top of Figure \ref{fig:coarsehats}) by-passes every other node on the fine mesh, and the next coarsening (bottom) does this again.  On 2D and 3D meshes this manner of constructing coarse-mesh hats is much less straightforward, and it is more common to start from a coarse mesh and refine level-by-level up to the fine level.  We will return to this issue in section \ref{sec:sia}, but we assert that the establishment of coarse levels makes sense on unstructured meshes.

\begin{figure}
\includegraphics[width=0.55\textwidth]{genfigs/coarsehats.pdf}
\smallskip

\includegraphics[width=0.55\textwidth]{genfigs/coarsesthats.pdf}
\caption{Coarser levels are additional sets of hat functions which spread over a greater distance.}
\label{fig:coarsehats}
\end{figure}

We need notation for the levels.  Suppose that the coarsest level is indexed as $j=0$ and the finest as $j=J$, with intermediate levels $j=1,\dots,J-1$, so the functions $\psi_p^j(x)$, for $p=1,\dots,m_j$, form the $j$th level.  The interior nodes $x_p^j$ use the same multilevel indexing scheme.  The fine level has now gained a superscript $J$; the original hats are $\psi_p^J(x)$ and the original nodes are $x_p^J$.  Figures \ref{fig:finehats} and \ref{fig:coarsehats} show a three-level scheme ($J=2$) with a total of $m_2=11$ hats (and interior nodes).  There are $m_1=5$ middle-level hats and $m_0=2$ coarsest-level hats.  The original mesh has $m_2+1=12$ elements, divisible by four, so the above three-level coarsening scheme works.

The $j$th-level hat functions are linearly-independent and form a basis for a vector space:
\begin{equation}
  \mathcal{V}^j = \operatorname{span}\{\psi_1^j(x),\dots,\psi_{m_j}^j(x)\} \subset \mathcal{H}.  \label{eq:definevk}
\end{equation}
These spaces are nested, i.e.~$\mathcal{V}^{j-1} \subset \mathcal{V}^j$, because each hat function can be written as a linear combination of coarser-level hats,
\begin{equation}
   \psi_p^{j-1}(x) = \sum_{q=1}^{m_j} c_{pq} \psi_q^j(x), \label{eq:hatcombination}
\end{equation}
and in fact $c_{pq} = \psi_p^{j-1}(x_q^j)$ because the hats $\{\psi_q^j\}$ form a nodal basis. Nonzero coefficients $c_{pq}$ occur when a fine-level node $x_q^j$ is in the non-zero set (\emph{support}) of a coarser hat $\psi_p^{j-1}(x)$.

A multilevel \emph{subspace decomposition} is now described by a vector-space sum:
\begin{equation}
  \mathcal{V}^h = \mathcal{V}^0 + \mathcal{V}^1 + \dots + \mathcal{V}^J. \label{eq:subspacedecomposition}
\end{equation}
This decomposition will be useful even though the final term $\mathcal{V}^J$ is actually equal to the original FE space $\mathcal{V}^h$.  Equation \eqref{eq:subspacedecomposition} asserts that a piecewise-linear function in $\mathcal{V}^h$ \emph{can} be written as a linear combination of hat functions from all the levels, but there is no unique representation.  A multilevel method will use this hierarchy to find the components of the solution in $\mathcal{V}^h$ via fast computations on all levels $\mathcal{V}^j$.

The levels $\mathcal{V}^0,\dots,\mathcal{V}^J$ provide a \emph{scale of frequencies}.  Informally, the value of the inner product of a function $g(x)$ with a fine-level hat, i.e.~$\ip{g}{\psi_p^J}$, is, relative to its norm $\|g\| = \ip{g}{g}^{1/2}$, mostly a measure of its high-frequency content at $x_p^J$.  The inner product with a coarser-mesh hat, by contrast, measures a lower frequency because it averages over many fine-mesh nodes.  If decomposition \eqref{eq:subspacedecomposition} were instead a Fourier decomposition, with each $\mathcal{V}^j$ spanned by waves of disjoint frequency ranges, then the sum would be orthogonal and the ``scale of frequencies'' meaning would be exact.  In any case, a multilevel method will reduce the energy of the error in a solution estimate (iterate) on each level $\mathcal{V}^j$, and doing so on a coarse level does rapidly reduce the low frequencies present in the error.

\subsection*{The residual and Gauss-Seidel}  Recall the weak form \eqref{eq:weakpoisson}.  Our FE method seeks a solution $u^h$ in $\mathcal{V}^h$ of the corresponding finite-dimensional weak-form,
\begin{equation}
  a(u^h,v) = \ip{f}{v},  \label{eq:feweakpoisson}
\end{equation}
for all test functions $v$ in $\mathcal{V}^h$.

Suppose $w$ in $\mathcal{V}^h$ is an approximation of $u^h$.  We define the \emph{residual} of $w$ as a linear functional acting on $v$ in $\mathcal{V}^h$:
\begin{equation}
  F(w)[v] = \ip{f}{v} - a(w,v).  \label{eq:residual}
\end{equation}
Finding $u^h$ which solves \eqref{eq:feweakpoisson} is equivalent to making all of the components of the residual zero: $F(u^h)[v]=0$ for all $v$ in $\mathcal{V}^h$.

We may represent an iterate $w$ using the basis of hat functions on any mesh level, i.e.~$w = \sum b_q \psi_q^j$.  By linearity of $a(\cdot,\cdot)$ in the first argument,
\begin{equation}
  F(w)[\psi_p^j] = \ip{f}{\psi_p^j} - \sum_{q=1}^{m_j} a(\psi_q^j,\psi_p^j) \,b_q.  \label{eq:residualpoisson}
\end{equation}
The number of nonzero terms in this sum is equal to the number of hat functions $\psi_q^j$ whose support overlaps the support of the test function $\psi_p^j$.  Assuming the values $\ip{f}{\psi_p^j}$ and $a(\psi_p^j,\psi_q^j)$ are already computed, it follows that the computation of the residual $F(w)[\psi_p^j]$, at a node $x_p^j$, requires $O(1)$ work.

Note that, for our 1D Poisson problem,
\begin{equation}
  a(\psi_p^j,\psi_q^j) = \int_0^1 \alpha(x) (\psi_p^j)'(x) (\psi_q^j)'(x)\,dx. \label{eq:poissonentries}
\end{equation}
We may write these values as the entries of a symmetric matrix $A^j$ with dimensions $m_j\times m_j$: $a_{p,q}^j = a(\psi_p^j,\psi_q^j)$.  For our problem, only three values $a_{p,p-1}^j, a_{p,p}^j, a_{p,p+1}^j$ are nonzero, so $A^j$ is tridiagonal.  (Writing these entries in detail may remind the reader of finite difference approximations for the Laplacian.)  Depending on the form of $\alpha(x)$, formula \eqref{eq:poissonentries} may be computed either exactly, or approximately by quadrature, and similarly for the values $\ip{f}{\psi_p^j} = \int_0^1 f(x) \psi_p^j(x)\,dx$.

Our basic solution method for \eqref{eq:feweakpoisson}, called \emph{Gauss-Seidel (GS) iteration}, is sequential and point-wise \emph{relaxation} of the residual.  (Though matrices are often used to present the GS algorithm \cite[for example]{Bueler2021,Greenbaum1997}, there is no need; we will present it using residuals and hat functions.)  The $j$th-level GS algorithm sweeps through the mesh nodes $x_p^j$, modifying the iterate $w(x)$ by a multiple of $\psi_p^j$ so as to make the residual at $x_p^j$ equal to zero.  That is, at each node it finds a real number $c$ so that
\begin{equation}
  F(w+c\,\psi_p^j)[\psi_p^j] = 0.  \label{eq:gaussseidelpoint}
\end{equation}
Noting that $F(w+c\,\psi_p^j)[\psi_p^j] = F(w)[\psi_p^j] - c\, a(\psi_p^j,\psi_p^j)$, we have the following algorithm:
\begin{pseudo*}
\pr{gssweep}(j,w,F)\text{:} \\+
    for $p=1,\dots,m_j$ \\+
        $\displaystyle c = F(w)[\psi_p^j]\, \big/ \,a(\psi_p^j,\psi_p^j)$  \\
        $w \gets w + c \psi_p^j$
\end{pseudo*}
Note that this procedure takes the residual function as an argument and it modifies $w$ in-place.  Assuming the values $a(\psi_p^j,\psi_q^j)$ and $\ip{f}{\psi_p^j}$ are available, computing $c$ involves $O(1)$ work, and thus one application of \textsc{gssweep} requires $O(m_j)$ work.

What does a sweep of GS do to an iterate $w$?  By sequentially making the residual zero on each hat function we can hope that the residual becomes smaller.  However, modifying $w$ to make the residual zero on one hat generally means the previously-zeroed values are no longer zero.  (That is, the equations are non-trivially coupled!)

Consider only fine-level GS sweeps for a moment.  One can prove for the Poisson problem that this iteration converges to the solution $u^h$ of \eqref{eq:feweakpoisson} \cite[for example]{Greenbaum1997}, but that is not our focus.  Instead, a key observation is that a fine-level GS sweep is a fast \emph{smoother} of the error $e=w-u^h$ even when it is slow to make the error (or residual) small.  Informally, the formula for $c$ in \textsc{gssweep} combines neighboring values of $w$ so as to flatten a peak or trough in the error.  This observation can be made quantitative by considering the frequencies supported on a mesh with equal spacing $h$.  The highest-frequency mode which is faithfully-representable on such a mesh is the sawtooth mode with (spatial) frequency $\omega=(2h)^{-1}$.  One GS sweep multiplies all modes with frequencies higher than $\frac{1}{2} \omega$ by factors at most $1/\sqrt{5}\approx 0.45$ \cite[Chapter 4]{Briggsetal2000}.  (This damping factor depends on the dimension and the differential operator.)  That is, a GS sweep \emph{damps the highest half} of the frequencies present in the error.

\begin{figure}[t]
\includegraphics[width=0.8\textwidth]{genfigs/residualpoints.pdf}
\caption{One Gauss-Seidel (GS) sweep on the fine level adjusts the iterate $w$ so that the residual $F(w)[\psi_p^J]$ at each successive node $x_p^J$ is zero (left).  The corresponding errors $e=w-u^h$ get significantly smoother (right).}
\label{fig:residualpoints}
\end{figure}

An example is shown in Figure \ref{fig:residualpoints}, where we start with a non-smooth initial iterate $w$ on an $m_K=6$ mesh; its residual $F(w)$ is at top-left.  The Figure shows the residual after each step in the \textbf{for} loop in \textsc{gssweep}, indicating the location which is zeroed.  (We plot the linear functionals $F(w)[\cdot]$ as piecewise-constant functions with values $F(w)[\psi_p^J]$.)  While both the residual and error become smaller in norm, the strongest effect is the damping of high frequencies.

\subsection*{Multilevel subspace corrections}  Once the GS smoother is applied on a given level $j$, whether once or a few times, the error and the residual no longer contain much energy in the $j$th-level modes.  Thus it makes sense to remove the energy in the next-coarser modes by applying GS on the $j-1$ level.  Starting on the finest level $J$, we have the following \emph{multilevel subspace correction} algorithm.
\begin{pseudo*}
\pr{msc-slash}(w,F)\text{:} \\+
    for $j=J$ downto $0$ \\+
        \pr{gssweep}(j,w,F)
\end{pseudo*}

The reason this downward-only scheme is called ``slash'' is shown in Figure \ref{fig:msccycles}.  Other correction sequences are possible, for example one may return to the fine level in a ``V'' cycle.
\begin{pseudo*}
\pr{msc-vcycle}(w,F)\text{:} \\+
    for $j=J$ downto $0$ \\+
        \pr{gssweep}(j,w,F) \\-
    for $j=1$ to $J$ \\+
        \pr{gssweep}(j,w,F)
\end{pseudo*}
Furthermore one may repeatedly apply the smoother on each level, which we denote using power notation as \pr{gssweep}$^j$, or go up-and-down the mesh hierarchy in more complicated ways (``W-cycles'', for example), or use coarser subspace corrections before finer; there are many possibilities \cite{Trottenbergetal2001}.

These \pr{msc} algorithms improve a fine-level iterate $w$ but they do not (generally) exactly solve the problem \eqref{eq:feweakpoisson} on the fine level.  Instead one iterates, testing for convergence using some tolerance on the (computable) fine-level residual.  For example, the following algorithm requires the residual to decrease by a certain factor (with representative default value).
\begin{pseudo*}
\pr{msc-solver}(F,\id{rtol}=10^{-4})\text{:} \\+
    $w=0$ \qquad\qquad\qquad\qquad\qquad \ct{or other fine-level initial iterate} \\
    $r_0 = \|F(w)\|$ \\
    repeat \\+
        \pr{msc-slash}(w,F) \qquad\qquad \ct{or \pr{msc-vcycle}, etc.} \\-
    until $\|F(w)\| \le r_0\, \id{rtol}$
\end{pseudo*}

\begin{figure}
\input{tikz/msccycles.tex}
\caption{Subspace corrections can be downward-only as in \pr{msc-slash} (left) or a V-cycle as in \pr{msc-vcycle} (right).  Each dot in this three-level hierarchy ($J=2$) is an application of \pr{gssweep} on that level.}
\label{fig:msccycles}
\end{figure}

These simply-stated schemes, which appeared relatively-late in the history of multigrid \cite{Xu1992}, are the essential multigrid schemes and possess their excellent convergence properties.  For example, the following theorem applies to linear elliptic PDEs in any dimension.  The constant $\rho$ depends only on the element aspect ratios (\emph{shape regularity} \cite{Elmanetal2014}) in the mesh---not relevant in 1D---and on the coefficient-minimum value $\alpha_0=\inf_x \alpha(x)$.  Recall that $u^h$ is the exact solution of the FE weak form \eqref{eq:feweakpoisson}.  Note that we use the norm $\|\cdot\|$ on $\mathcal{H}$.

\begin{theorem} \cite[Thm.~3.10]{GraeserKornhuber2009}  There is $\rho<1$ so that if $w^{(s)}$  results from $s$ applications of \pr{msc-slash} or \pr{msc-vcycle} on the fine level, starting with any $w^{(0)}$, then
\begin{equation}
  \|w^{(s+1)} - u^h\| \le \rho \|w^{(s)} - u^h\|.  \label{eq:mscconvergence}
\end{equation}
\end{theorem}

It follows that if we want the error norm $e^{(s)} = \|w^{(s)}-u^h\|$ to be reduced to below some level $\eps>0$ then we should do $s>(\log\eps - \log e^{(0)})/\log \rho$ iterations, where $e^{(0)}$ is the initial error norm.  That is, we need $O(\log\eps)$ iterations, and the constant is better when $\rho<1$ is smaller.

Unfortunately, these \pr{msc} methods are not yet \emph{fast} solvers.  Their performance entirely depends on how iterates $w$ and residuals $F(w)[\cdot]$ are represented on each mesh level.  For example, if we represent $w$ and $F(w)$ using the fine-level basis $\{\psi_p^J\}$ then one application of \pr{gsweep}$(J,w,F)$ is indeed fast, i.e.~$O(m_J)$ operations, with a small constant, because evaluating $F(w)[\psi_p^J]$ requires only a few operations.  However, evaluating $F(w)[\psi_p^j]$ for $j<J$, from the fine-mesh representation of $F(w)$, means computing integrals over the wide support of $\psi_p^j$, which is nonzero at many fine-mesh nodes $x_p^J$.  It is not clear that \pr{gssweep}$(j,w,F)$ is an $O(m_j)$ operation with a small constant.  The \pr{msc} methods are not fully-specified, as solvers, until we have hierarchical representations which are compatible with multilevel corrections.

\subsection*{Geometric multigrid}  Once the GS smoother has been applied on the finest level, the error and the residual no longer contain much energy in the high-frequency modes for which we need the $J$th-level representation.  The GS smoother should then be applied on the $J-1$ level, but efficiently using $J-1$ level data structures to represent the problem.  By using coarse-level representations we go beyond multilevel subpace corrections, and we unlock the power of multigrid.

Efficiency on coarser levels requires addressing two concerns:
\renewcommand{\labelenumi}{\emph{\roman{enumi})}}
\begin{enumerate}
\item How do we represent an iterate $w$ and an residual $F(w)[v]=0$ on the $j$th level?
\item How do we hand-down a partially-solve $j$ level problem to the $j-1$ level?
\end{enumerate}

Our answer to \emph{i)} is straightforward.  As already noted, a function $w(x)$ in $\mathcal{V}^j$ is represented by its nodal values $w_p=w(x_p^j)$, thus $w = \sum_p w_p \psi_p^j$, and $\bw = \{w_p\}$ is a vector in $\RR^{m_j}$.  Representing a residual, a linear functional in $(\mathcal{V}^j)'$, is just as simple because the values $F_p = F(w)[\psi_p^j]$ form a vector $\bF=\{F_p\}$, also in $\RR^{m_j}$.  It is common to think of $\bw$ as a column vector and $\bF$ as a row vector, but this is not essential.

For \emph{ii)} we need to derive a new equation for the coarser level which loses little information if the key quantities are smooth.  First, for a iterate $w$ in the original FE space $\mathcal{V}^h$, the residual definition \eqref{eq:residual} can be rewritten as the equation
\begin{equation}
  a(w,v) = \ip{f}{v} - F(w)[v].  \label{eq:residualrewrite}
\end{equation}
Subtracting the weak form \eqref{eq:feweakpoisson} from \eqref{eq:residualrewrite} we cancel the source term:
\begin{equation}
  a(w,v) - a(u^h,v) = - F(w)[v].  \label{eq:errorequationearly}
\end{equation}
Because of the linearity of $a(\cdot,\cdot)$ in the first position, we have the (weak-form) \emph{error equation},
\begin{equation}
  a(e,v) = - F(w)[v],  \label{eq:errorequation}
\end{equation}
for all $v$ in $\mathcal{V}^h$, where $e=w-u^h$.

Equation \eqref{eq:errorequation} is actually equivalent to the original FE weak form \eqref{eq:feweakpoisson}, but if GS has already been applied on the finest level then both $e$ and the residual $F(w)$ are smoothed quantities which should have faithful representations in terms of the basis $\{\psi_p^{J-1}\}$, that is, not using the fine-level hats.  However, as stated above, on the fine level $\mathcal{V}^h=\mathcal{V}^J$ we have represented $e$ by a vector of its nodal values $\be = \{e_p\}$ and $F(w)$ by its values $\bF=\{F_p\}$, both in $\RR^{m_J}$.  Therefore we approximate equation \eqref{eq:errorequation} by a new equation wherein all quantities are represented using only the $J-1$ mesh.  That is, geometric multigrid proposes the following \emph{coarse-level equation} for linear problems:
\begin{equation}
  a(e^{J-1},v) = - (RF(w))[v]  \label{eq:coarsecorrection}
\end{equation}
for all $v$ in $\mathcal{V}^{J-1}$.  In \eqref{eq:coarsecorrection} we propose to represent $e^{J-1}$ in the basis $\{\psi_p^{J-1}\}$ and to use $v=\psi_p^{J-1}$ as test functions.  Furthermore we use the \emph{canonical restriction operator} $R$ to put the $J$th-level residual onto the $J-1$ level.  It maps a linear functional $\ell$ on the $J$ level to the $J-1$ level:
\begin{equation}
  (R \ell)[v] = \ell[v] \label{eq:canonicalrestriction}
\end{equation}
for all $v$ in $\mathcal{V}^{J-1}$.  That is, $R \ell$ acts on $\mathcal{V}^{J-1}$ the same as $\ell$, but we represent such linear functionals by their values on the $J-1$ level hats, thus
\begin{equation}
  (R \ell)[\psi_p^{J-1}] = FIXME
\end{equation}
by \eqref{eq:hatcombination}.

FIXME  Given the solution $e^{j-1}$, the update
\begin{equation}
  w^j \gets w^j + P e^{j-1}  \label{eq:update}
\end{equation}
``corrects'' the fine-level iterate.  Note we have introduced yet another operation, the \emph{prolongation} $P$, and it is defined in \eqref{eq:canonicalprolongation} below.  One could write \eqref{eq:update} using an index indicating the iteration, e.g.~$w^{j,s+1} = w^{j,s} + Pe^{j-1,s}$, but for simplicity we keep the level index $j$.

Combining the above ideas gives a \emph{multigrid V-cycle}.  In addition to the GS smoother, a V-cycle adds a recursive solution of the coarse-level correction equation \eqref{eq:coarsecorrection}, followed by update \eqref{eq:update} and more smoother sweeps.  The following pseudocode, which modifies the iterate $w$ in-place, can be called repeatedly so as to improve the iterate $w$.
\begin{pseudo*}
\pr{gmg-vcycle}(j,w,F)\text{:} \\+
    if $j=0$ \\+
        $w =$ \pr{coarsesolve}(F) \\-
    else \\+
        $\text{\pr{gssweep}}^{\text{\id{down}}}(j,w,F)$ \\
        FIXME $r^j[\cdot] = \ell[\cdot] - a^j(w,\cdot)$ \\
        $e^{j-1} =$ \pr{vcycle}(j-1,0,-R r^j) \\
        $w \gets w + P e^{j-1}$ \\
        $\text{\pr{gssweep}}^{\text{\id{up}}}(j,w,F)$ \\-
\end{pseudo*}

Observe that after \texttt{down} smoother sweeps, denoted here by $\text{\textsc{gssweep}}^{\text{\texttt{down}}}$, the coarse-level correction equation \eqref{eq:coarsecorrection} is applied on the next-coarser level with a ``new'' right-hand linear functional $\ell^{j-1}[v]=-(R r^j(w^j))[v]$ and a zero initial iterate.  That is, the coarse-level problem has a new source term, which is the amount the fine-level iterate $w^j$ did not already solve the problem.  After this correction an additional \texttt{up} sweeps are done to remove high-frequency components from the update.  Figure \ref{fig:vcycle} illustrates a three-level V-cycle.

FIXME Next, the coarse-level error $e^{j-1}$ corrects the fine-level iterate $w^j$ using the \emph{canonical prolongation operator} $P$ acting on a function $z^{j-1}$ in $\mathcal{V}^{j-1}$:
\begin{equation}
  (P z^{j-1})(x) = z^{j-1}(x). \label{eq:canonicalprolongation}
\end{equation}
Again the result $P z^{j-1}$ is really just the same as the input function $z^{j-1}$.

Though the operators $R$ and $P$ thus seem to do nothing, and make no choices, which is the meaning of ``canonical'', in fact they represent computational work because of how we represent functions and functionals on each level.  That work is contained in the perhaps-forgotten equation \eqref{eq:hatcombination} which describes how each coarse-level hat function is a linear combination of fine-level hats.  For example, if a fine-level linear functional $\ell^j$ is represented on the computer as a vector of its values $\ell[p] = \ell^j[\psi_p^j]$, then the vector representing the restricted linear functional $R \ell^j$ has values computed via \eqref{eq:hatcombination}.  Likewise, if a coarse-level function $z^{j-1}(x)$ is represented by its coefficients in the basis $\{\psi_p^{j-1}\}$ then \eqref{eq:hatcombination} determines the coefficients of $P z^{j-1}$ in the fine-level basis $\{\psi_p^j\}$.  Linear operators $R$ and $P$ may be represented as rectangular matrices, but this is not necessary; they can be applied using $O(m_j)$ work via straightforward local-averaging (\emph{full weighting} \cite{Briggsetal2000}) formulae which come from \eqref{eq:hatcombination}.

\begin{figure}
\input{tikz/vcycle.tex}
\caption{A V-cycle with distinguished down-smoother (solid dots), up-smoother (circles), and coarse-level (square) solvers.}
\label{fig:vcycle}
\end{figure}

The above \textsc{gmg-vcycle} algorithm also calls a coarse solver on the $j=0$ level.  For a linear problem like our current Poisson problem it is traditional to apply a direct solver for this linear system (i.e.~equation \eqref{eq:linearsystem}), but this is not an option for the nonlinear glacier geometry problem.  Instead we suppose \textsc{coarsesolve} is implemented as a fixed number of GS sweeps using an initial iterate zero:
\begin{pseudo*}
\pr{coarsesolve}(F)\text{:} \\+
    $w=0$ \\
    $\text{\pr{gssweep}}^{\text{\id{coarse}}}(0,w,F)$ \\
    return $w$
\end{pseudo*}
If the coarsest mesh has a single node then a single sweep gives an exact solution.

We have now presented a basic geometric multigrid algorithm via a particular FE viewpoint, the subspace decomposition approach pioneered by Xu \cite{Xu1992} and others, an idea which applies both to multilevel and domain-decomposition algorithms.  We could now show computational results featuring the efficiency of the V-cycle algorithm, namely evidence of optimal $O(m_J)$ time to solve the problem.  (Such results appear in multigrid references \cite{Briggsetal2000,Bueler2021,Elmanetal2014,Trottenbergetal2001}.)  However, we first introduce a less-trivial ``obstacle'' problem which has the essential free-boundary character of the glacier geometry problem.  Computational results will be given in the next three sections.


\section{Constraint decomposition for the obstacle problem} \label{sec:obstacle}

\subsection*{An ice-like model problem}  We now have a basic view of the geometric multigrid (GMG) method from the multilevel subspace decomposition point of view.  However, as addressed in the Introduction, the main problem in glacier modeling, of how the ice geometry and velocity co-evolve in response to climatic inputs, needs an inequality constraint for well-posedness.  The fact that the ice surface elevation is above the bed generates the land-terminating boundary condition for the mass conservation problem, and this inequality constraint by itself makes the problem nonlinear.  It also reduces solution regularity in a manner which is challenging to numerical methods, with further challenges from the non-Newtonian viscosity of the fluid.

To address such inequality constraints, and before actually modeling glaciers in section \ref{sec:sia}, we introduce a more-convincing model problem, namely the \emph{classical obstacle problem}.  This simply adds an inequality constraint to the same linear differential equation, the Poisson equation.  After stating the weak form, which is now solved over a \emph{subset} of the function space, we will modify the multilevel subspace decomposition approach to be a \emph{constraint decomposition} method.  Each mesh level will host an inequality-constrained problem, and together all the levels will capture the original constraint set.  Switching the differential equation to the nonlinear SIA model in section \ref{sec:sia} will then be a comparatively easy change.

We use the same 1D domain and solution space $\mathcal{H}=H_0^1[0,1]$ as the Poisson problem \eqref{eq:poisson}.  Recall that functions in $\mathcal{H}$ have well-defined first derivatives and satisfy zero boundary conditions.  Let $\varphi(x)$ be a fixed function, also in $\mathcal{H}$, which is the obstacle.  The strong form of the obstacle problem is the following \emph{complementarity problem} (CP) \cite{Bueler2021,KinderlehrerStampacchia1980} which says that the Poisson equation applies whereever the solution $u(x)$ is strictly above the obstacle:
\begin{align}
  u - \varphi &\ge 0 \label{eq:obstaclecp} \\
  -u''-f &\ge 0 \notag \\
  (u-\varphi)(-u''-f) &= 0 \notag
\end{align}
The last condition, complementarity, implies that for each $x$ in $[0,1]$ the solution either coincides with the obstacle ($u(x)=\phi(x)$) or the Poisson equation holds at that point ($-u''(x)=f(x)$).  (Or both, but in the generic \emph{nondegenerate} \cite{KinderlehrerStampacchia1980} case the obstacle does not itself solve the Poisson equation.)  Conditions \eqref{eq:obstaclecp} also say that where the solution coincides with the obstacle the source term is bounded above: $u=\varphi \implies f \le -\varphi''$.  In this region, where $u=\varphi$, the constraint is said to be \emph{active}, while the (Poisson) differential equation holds in the \emph{inactive} portion where $u>\varphi$.

The glacier problem will also have a CP formulation (section \ref{sec:sia}; see also \cite{Calvoetal2002}).  The obstacle will be the bed elevation, the solution the glacier surface elevation, and the source term the surface mass balance.  In the inactive region, i.e.~on the glacier, the mass conservation equation will apply, but off the glacier, in the active region, the surface mass balance will be negative.

Simply by choosing a source term $f(x)$ which is positive in the middle of the domain and negative near the boundaries, we get an ``ice-like'' solution to \eqref{eq:obstaclecp} as shown in Figure \ref{fig:icelike}.  In detail, the Figure shows the exact, piecewise-quadratic solution $u(x)$ for the following data:
\begin{equation}
\varphi(x) = x(1-x) \quad \text{ and } \quad f(x) = \begin{cases} 8, & 0.2 < x < 0.8, \\
                                                               -16, & x<0.2 \text{ or } x>0.8. \end{cases}  \label{eq:icelikedetails}
\end{equation}
(Note that $f$ is in $L^2[0,1]$, and defined almost everywhere.)  Finding the formula for $u(x)$, which smoothly-connects five quadratic pieces, is an exercise for the reader.\footnote{Find the solution in: \, \href{https://github.com/bueler/mg-glaciers/blob/master/py/obstacle.py}{\texttt{github.com/bueler/mg-glaciers/blob/master/py/obstacle.py}}.}

% regenerate:
%   $ cd py/
%   $ ./obstacle.py -plain -kfine 5 -o icelike.pdf
%   $ pdfcrop icelike.pdf icelike.pdf
\begin{figure}
\includegraphics[width=0.7\textwidth]{fixfigs/icelike.pdf}
\caption{An ice-like configuration of the classical obstacle problem.}
\label{fig:icelike}
\end{figure}

Observe that the solution $u$ to \eqref{eq:obstaclecp} does not depend linearly on the source function $f$.  In fact, if $f(x)\le 0$ for all $x$ then the solution for the same obstacle $\varphi(x)=x(1-x)$ shown in Figure \ref{fig:icelike} is $u=\varphi$.  If $\tilde u$ solves the problem for $\tilde f= -1$, for example, then $2\tilde u$ \emph{does not} solve the problem for source term $2\tilde f = -2$.  In this sense the classical obstacle problem is nonlinear even though the corresponding PDE, the Poisson equation, which is the \emph{interior condition} of the CP \cite{KinderlehrerStampacchia1980}, is linear.

\subsection*{Weak formulation with constraints}  In considering the weak form of \eqref{eq:obstaclecp} we observe that the solution and test functions must be in a closed subset which incorporates the constraint:
\begin{equation}
\mathcal{K} = \left\{v \ge \varphi\right\} \subseteq \mathcal{H}.  \label{eq:Kdefine}
\end{equation}
This subset is not a vector space, but it is \emph{convex}.  That is, if $v,w$ are in $\mathcal{K}$ then any point on the line segment connecting them, namely $\theta v + (1-\theta) w$ for $0 \le \theta \le 1$, is also in $\mathcal{K}$.

As expected, derivation of the weak form of \eqref{eq:obstaclecp} involves multiplying by a test function and integrating by parts.  However, the inequalities enter nontrivially into the derivation; see \cite[Chapter 12]{Bueler2021}, \cite{JouvetBueler2012}, or \cite{KinderlehrerStampacchia1980}.  The result is a single \emph{variational inequality} (VI):
\begin{equation}
  a(u,v-u) \ge \ip{f}{v-u} \quad \text{ for all } v \text{ in } \mathcal{K}. \label{eq:obstaclevi}
\end{equation}
The bilinear form on the left is the same as in equation \eqref{eq:weakpoisson}: $a(u,v) = \int_0^1 u'(x) v'(x)\,dx$.

Inequality problems \eqref{eq:obstaclecp} and \eqref{eq:obstaclevi} are equivalent up to the same regularity concerns which relate the strong and weak forms of a PDE.  (See reference \cite{Evans2010} regarding the solution regularity of PDEs, and \cite{KinderlehrerStampacchia1980} for the corresponding VI concepts.)  However, the intuition behind a VI is needed for understanding.  To help, we provide another weak form.  Inequality \eqref{eq:obstaclevi} is precisely equivalent to \emph{constrained minimization}:
\newcommand{\argmin}{\mathop{\mathrm{arg\text{-}min}}}
\begin{equation}
  u = \argmin_{w \text{ in } \mathcal{K}} J(w) \quad \text{where} \quad J(w) = \frac{1}{2} a(w,w) - \ip{f}{w}. \label{eq:obstaclemin}
\end{equation}
That is, $u$ is the minimizer, over the constraint set $\mathcal{K}$, of the scalar, quadratic \emph{objective} functional $J$.  The proof of equivalence is standard in continuous optimization.  One computes the (Gateaux) derivative, a linear functional in $\mathcal{H}'$:
\begin{equation}
  \grad J(w)[v] = \lim_{\eps\to 0} \frac{J(w+\eps v) - J(w)}{\eps} = a(u,v) - \ip{f}{v}.  \label{eq:gradobjective}
\end{equation}
This gradient $\nabla J(u)$ already appears in \eqref{eq:obstaclevi}, and the VI can be re-stated as follows:
\begin{equation}
  \nabla J(u)[v-u] \ge 0 \quad \text{ for all } v \text{ in } \mathcal{K}. \label{eq:obstaclevigradient}
\end{equation}

The solution $u$ of \eqref{eq:obstaclevi}, \eqref{eq:obstaclemin}, or \eqref{eq:obstaclevigradient} sits at a location in $\mathcal{K}$, possibly on the boundary of $\mathcal{K}$, where a vector pointing further into the constraint set, namely $v-u$ for $v$ in $\mathcal{K}$, points ``uphill'' on the graph of $J$.  If we think of $\nabla J(u)$ as a vector, instead of a linear functional, we could write \eqref{eq:obstaclevigradient} as $\ip{\nabla J(u)}{v-u} \ge 0$, thus the angle between $\nabla J(u)$ and $v-u$ is at most 90 degrees.  One possibility for our 1D obstacle problem is that the constraint is strict for every $x$, i.e.~$u(x) > \varphi(x)$, in which case \eqref{eq:obstaclevigradient} implies the unconstrained minimum condition $\nabla J(u)[v] = 0$ for all $v$ in $\mathcal{H}$; in this case the gradient of $J$ is zero at $u$.

Of course, $\mathcal{K}$ in \eqref{eq:Kdefine} is an infinite-dimensional set.  To aid the intuition we propose the following low-dimensional, but global, mental image.  Imagine the constraint set $\mathcal{K}$ as analogous to the closed first quadrant in the plane, $\mathcal{Q} = \left\{x = (x_1,x_2)\,:\,x_i\ge 0\right\}$, and $J$ as analogous to a smooth, concave-up (i.e.~\emph{coercive} \cite{Evans2010}), objective function $\gamma(x)$ defined on the plane.  The solution of the corresponding VI, say $\hat x$ in $\mathcal{Q}$, may not be a location where $\nabla \gamma$ is zero.  However, it will have as low a value of $\gamma$ as possible, even if it is on the boundary of $\mathcal{Q}$.  At $\hat x$ all directions \emph{into} $\mathcal{Q}$ will increase the value of $\gamma$, as sketched by contours in Figure \ref{fig:cartoonplane}.  The obstacle problem solution shown in Figure \ref{fig:icelike} is an infinite-dimensional version of the solution point shown in the Figure \ref{fig:cartoonplane} cartoon view.

\begin{figure}
\includegraphics[width=0.35\textwidth]{genfigs/cartoonplane.pdf}
\caption{For analogy only:  A 2-dimensional constrained-minimization or VI solution, in a planar quadrant $\mathcal{Q}$, with contours of a scalar function $\gamma(x)$.}
\label{fig:cartoonplane}
\end{figure}

The glacier problem in the next section will also be formulated as a CP like \eqref{eq:obstaclecp}, and as a VI like \eqref{eq:obstaclevi}.  However, for general bed elevation functions (obstacles) the glacier problem has no constrained minimization formulation like \eqref{eq:obstaclemin}; the essential reason is the lack of symmetry of the weak form in the general case \cite{JouvetBueler2012}.  In other words, for the glacier problem we possess a map like $\grad J$, from $\mathcal{H}$ to $\mathcal{H}'$, but not a scalar objective like $J$ itself.  We will return to this point in section \ref{sec:sia}.

Regardless of how the classical obstacle problem is formulated, a \emph{free boundary} generally arises in the interior of the domain.  For example, as we move away from the midpoint in Figure \ref{fig:icelike} there are locations, near the ends of $[0,1]$, where the solution first becomes fully in contact with the obstacle.  At these locations both $u=\varphi$ and $u'=\varphi'$ hold, that is, the solution is tangent to the obstacle at the free boundary.  These simultaneous Dirichlet and Neumann ``conditions'' occur at a location which must be found as part of the solution.  (In addition, there are fixed boundary conditions because the solution $u$ is in $\mathcal{H}$.)

The solution of the classical obstacle problem can be non-smooth at the free boundary even when the data is arbitrarily smooth.  For example, smoothing the source term $f$ in \eqref{eq:icelikedetails} would give a solution nearly the same as shown in Figure \ref{fig:icelike}.  That is, one could make the source term $f$ into a smooth function by ``mollification'' \cite{Evans2010} so that the transition between positive and negative values of $f$ would be $C^\infty$.  However, the free boundary would remain, and at that free boundary the second derivative $u''$ would jump from value $+16=-f$ to value $-2=\varphi''$.  Thus the 1D obstacle problem solution, for smooth data, is limited to the Sobolev space $W^{2,\infty}$ \cite[section IV.6]{KinderlehrerStampacchia1980}, and (generally) not smoother.

\subsection*{Multilevel constraint decomposition}  Recall from section \ref{sec:subspace} that each mesh level corresponds to $m_j$ hat functions $\psi_p^j(x)$, at interior nodes $x_p^j$, and to the corresponding vector space $\mathcal{V}_j$ of piecewise-linear functions with value zero at $x=0,1$.  (See Figures \ref{fig:finehats}, \ref{fig:coarsehats}.)  In this section we add an obstacle function $\phi^j(x)$ on each level, to define a closed, convex $k$\emph{th-level constraint set}
\begin{equation}
\mathcal{K}^j = \left\{v \text{ in } \mathcal{V}^j \text{ such that } v \ge \phi^j\right\}.  \label{eq:levelKdefine}
\end{equation}
However, the construction of the $j$th-level obstacle $\phi^j$ is nontrivial; it is \emph{not} the $j$th-level interpolant of the continuum obstacle $\varphi$.  Indeed, the manner in which we decompose the continuum constraint $\mathcal{K}$ into multilevel sets $\mathcal{K}^j$ is the key concern of this section.  On the $j$th level we have the finite-dimensional VI for $u^j$ in $\mathcal{K}^j$:
\begin{equation}
  a(u^j,v-u^j) \ge \ell^j[v-u] \quad \text{ for all } v \text{ in } \mathcal{K}^j. \label{eq:levelvi}
\end{equation}
As in the previous section, $\ell^J[v]=\ip{f}{v}$ on the finest level, but on coarser levels we will redefine the linear functional $\ell^j$ for the right-hand side.

As we did for GMG on the Poisson equation, on each mesh level we can immediately propose an iterative solution method for the weak form \eqref{eq:levelvi}.  The \emph{projected Gauss-Seidel (PGS) iteration} is a small modification of the original GS method.  This method might ultimately solve the problem---a proof is in \cite{BrandtCryer1983}---but here it serves as a smoother; we only apply one or two iterations on each level.  The PGS iteration finds the solution to VI \eqref{eq:levelvi}, though slowly, by approximately descending the gradient $\grad J$ while remaining in the constraint region $\mathcal{K}^j = \{w\ge \phi^j\}$.  Specifically, PGS descends by adjusting the iterate one point at a time.  Recall that conventional GS successively zeros the residual at points, i.e.~it finds $c$ so that $r^j(w+c\psi_p^j)[\psi_p^j]=0$, but the residual is the same as the \emph{negative} gradient of the objective:
\begin{equation}
  r^j(w)[v] = - \grad J^j(w)[v].  \label{eq:levelresidualgradient}
\end{equation}
Also, because $\mathcal{K}^j$ consists of piecewise-linear functions, if $w$ is in $\mathcal{K}^j$ then one can test whether the modified iterate is still in $\mathcal{K}^j$ via a pointwise comparison, i.e.~testing $w[p] + c \ge \phi^j[p]$.  Thus we have the following smoother algorithm:
\begin{pseudo*}
\pr{pgssweep}(j,w,\ell,\phi)\text{:} \\+
    for $p=1,\dots,m_j$ \\+
        $\displaystyle c = r^j(w)[\psi_p^j] \, \big/ \,a(\psi_p^j,\psi_p^j)$ \qquad \ct{where $r^j(w)[\cdot] := \ell[\cdot] - a(w,\cdot)$} \\
        $w[p] \gets \max\{w[p] + c,\phi[p]\}$
\end{pseudo*}

However, we still need to define the $j$th-level obstacle $\phi^j$.  Suppose that on the fine level we have a solution method which computes iterates $w^J$ in $\mathcal{K}^J$ which converge to the solution $u^J$ of the fine level problem \eqref{eq:levelvi}.  (Applying PGS on the fine level would generate such iterates, but they would converge very slowly.)  Define $\varphi^J$ to be the piecewise-linear interpolant (i.e.~in $\mathcal{V}^J$) of the continuum obstacle $\varphi$.  We define the \emph{defect constraint} \cite{GraeserKornhuber2009} on the fine level to be
\begin{equation}
  \chi^J = \varphi^J - w^J.  \label{eq:defectconstraint}
\end{equation}
Note that admissibility for the iterate $w^J$ is equivalent to a nonpositive defect constraint: $w^J \ge \varphi^J \iff 0 \ge \chi^J$.  More generally, if we modify an iterate $w^J$ by adding $v$ then the result is admissible if and only if $v$ is above the defect constraint:
\begin{equation}
  w^J + v \ge \varphi^J  \qquad \iff \qquad v \ge \chi^J.  \label{eq:defectmeaning}
\end{equation}
This is the essential meaning of ``defect constraint.''

A main principle for our constraint decomposition method is that
\begin{quote}
\emph{as much as possible of the fine-level defect constraint $\chi^J$ is in the coarsest levels.}
\end{quote}
We will define coarser-level versions of the defect constraint, namely $\chi^j$ in equation \eqref{eq:chik} below, for all levels $j=0,1,\dots,J-1$.  From these we define the \emph{$j$th-level obstacles}:
\begin{equation}
  \phi^j = \chi^j - P\chi^{j-1} \quad \text{ for } j=1,2,\dots,J, \text{ and } \quad \phi^0 = \chi^0.  \label{eq:levelobstacle}
\end{equation}
Recall from section \ref{sec:subspace} that the canonical prolongation operator $P$ actually does nothing; in definition \eqref{eq:levelobstacle} it takes $\chi^{j-1}$ in $\mathcal{V}^{j-1}$ to the same function $P\chi^{j-1}$ in $\mathcal{V}^j$.  Therefore, from now on we simply write $\phi^j = \chi^j - \chi^{j-1}$.

The multilevel constraint decomposition is now a ``telescoping'' sum:
\begin{equation}
  \sum_{j=0}^J \phi^j = \chi^0 + (\chi^1 - \chi^0) + (\chi^2 - \chi^1) + \dots + (\chi^J - \chi^{J-1}) = \chi^J.  \label{eq:telescopingdecomposition}
\end{equation}
That is, we have decomposed the fine-level defect constraint $\chi^J$ into the obstacles $\phi^j$ which we will actually use on each level.  (Note that the sum does \emph{not} give a decomposition of the original obstacle $\varphi$, and it depends on a fine-mesh iterate.)  An example decomposition is shown in Figure \ref{fig:gooddecomposition}; compare Figure 1 in \cite{GraeserKornhuber2009}.

%REGENERATE Figures \ref{fig:icelikedecomposition} and \ref{fig:gooddecomposition}:
%$ ./obstacle.py -jfine 5 -jcoarse 1 -irtol 1.0e-7 -random -randommodes 8 -diagnostics -up 0 -o defect.pdf
%fine level 5 (m=63) using 52 V(1,0) cycles (102.375 WU)
%saving final iterate and obstacle to defect.pdf ...
%saving residual and inactive residual to resid_defect.pdf ...
%saving hierarchical decomposition to decomp_defect.pdf ...
%saving "ice-like" decomposition to icedec_defect.pdf ...
\begin{figure}
\includegraphics[width=0.75\textwidth]{fixfigs/decomp_defect.pdf}
\caption{The constraint decomposition writes the fine-mesh defect obstacle $\chi^J = \varphi^J - w^J$ as a sum of the obstacles (gaps) $\phi^j = \chi^j - \chi^{j-1}$ on each level.  The $j$th-level constraint set in our multilevel method is $\mathcal{K}^j = \{v \ge \phi^j\} \subseteq \mathcal{V}^j$.}
\label{fig:gooddecomposition}
\end{figure}

However, we need to define the coarser-level defect constraints $\chi^j$.  On the fine level we have already set $\chi^J = \varphi^J - w^J$, and we now define the others inductively, down to the coarsest mesh ($k=0$), so that each difference $\phi^j = \chi^j - \chi^{j-1}$ is nonpositive (Figure \ref{fig:gooddecomposition}).  That is, our goals are that $\chi^{j-1}$ is in $\mathcal{V}^{j-1}$ and that $\chi^{j-1}(x) \ge \chi^j(x)$ for all $x$ in $[0,1]$.  Thus we define a \emph{monotone restriction} operator $\hat R$ from $\mathcal{V}^j$ to $\mathcal{V}^{j-1}$.  For $z$ in $\mathcal{V}^j$ one computes $\hat R z$ by maximizing the node values of $z$ over the interior of the support of each level $j-1$ hat function.  That is, if $z = \sum_{q=1}^{m_j} b_q \psi_q^j$ then
\begin{equation}
  \hat R z = \sum_{p=1}^{m_{j-1}} c_p \psi_p^{j-1} \qquad \text{where} \qquad c_p = \max_{\psi_p^{j-1}(x_q^j) > 0} b_q.  \label{eq:monotonerestriction}
\end{equation}
Recall that our hat functions form nodal bases, and thus $c_p = (\hat R z)(x_p^{j-1})$, for example.  Also note that while canonical restriction \eqref{eq:canonicalrestriction} acts on linear functionals ($(\mathcal{V}^j)'$), monotone restriction acts on functions ($\mathcal{V}^j$).  Finally define
\begin{equation}
  \chi^{j-1} = \hat R \chi^j.  \label{eq:chik}
\end{equation}

Each $j$th-level obstacle $\phi^j = \chi^j - \chi^{j-1}$ also defines a constraint set
\begin{equation}
  \mathcal{K}^j = \{v \ge \phi^j\} \subseteq \mathcal{V}^j.  \label{eq:levelconstraint}
\end{equation}
Given a fine-mesh iterate $w^J$ and the corresponding defect constraint $\chi^J = \varphi^J - w^J$, we have a sum of subsets which is analogous to \eqref{eq:subspacedecomposition}:
\begin{equation}
  \left\{v\,:\,w^J + v \ge \varphi^J\right\} = \left\{v \ge \chi^J\right\} = \mathcal{K}^0 + \mathcal{K}^1 + \dots + \mathcal{K}^J. \label{eq:constraintdecomposition}
\end{equation}
The meaning here is that every admissible modification $v$ of the current fine-mesh iterate---by \eqref{eq:defectmeaning} this is every $v$ above the fine-mesh defect constraint $\chi^J$---can be built, though not uniquely, by choosing an element from each of the mesh-level constraint sets $\mathcal{K}^j = \{v \ge \phi^j\}$.  Note that the proof of \eqref{eq:constraintdecomposition} is an inequality version of the telescoping sum \eqref{eq:telescopingdecomposition}.

In the ice context (section \ref{sec:sia}) the fine-mesh obstacle $\varphi^J$ will be the bed elevation and $w^J$ a current estimate of the ice surface elevation.  Thus $-\chi^J$ is the ice thickness on the fine mesh.  Figure \ref{fig:icelikedecomposition} shows the same constraint decomposition as in Figure \ref{fig:gooddecomposition} as though it is a decomposition of the ice into layers.  (Compared to Figure \ref{fig:icelike}, note that this bed elevation $\phi^J$ has a more interesting, but still smooth, bed topography.)  However, Figure \ref{fig:gooddecomposition} plots the decomposition in the correct sense: each defect constraint $\chi^j$ and obstacle $\phi^j$ is piecewise-linear on level $j$.

\begin{figure}
\includegraphics[width=0.7\textwidth]{fixfigs/icedec_defect.pdf}
\caption{Multilevel constraint decomposition visualized as a decomposition of the ``ice'' between the current fine-mesh iterate $w^J$ and the obstacle $\varphi^J$.  (Figure \ref{fig:gooddecomposition} shows the correct plot of this constraint decomposition.)}
\label{fig:icelikedecomposition}
\end{figure}

FIXME state V-cycle for V(1,0); run as \pr{cd-slash}$(J,r^J,\chi^J)$ where $w^J$ is an existing admissible fine-mesh iterate, $\chi^J=\varphi^J-w^J$, and $r^J[\cdot] = \ell^J[\cdot] - a(w^J,\cdot)$; use the result $v$ to update the iterate: $w^J \gets w^J + v$
\begin{pseudo*}
\pr{cd-slash}(j,r,\chi)\text{:} \\+
    $v=0$ \\
    if $j=0$ \\+
        $\text{\pr{pgssweep}}^{\text{\id{coarse}}}(0,v,r,\chi)$ \\
        return $v$ \\-
    $\chi^{j-1} = \hat R \chi$ \\
    $\phi^j = \chi - P\chi^{j-1}$ \\
    $\text{\pr{pgssweep}}^{\text{\id{down}}}(j,v,r,\phi^j)$ \\
    $\tilde r[\cdot] = r[\cdot] - a^j(v,\cdot)$ \\
    $v^{j-1} =$ \pr{cd-slash}(j-1,R \tilde r,\chi^{j-1}) \\
    return $v + P v^{j-1}$
\end{pseudo*}


FIXME state V(1,1)-cycle; run the same way
\begin{pseudo*}
\pr{cd-vcycle}(j,r,\chi)\text{:} \\+
    $v=0$ \\
    if $j=0$ \\+
        $\text{\pr{pgssweep}}^{\text{\id{coarse}}}(0,v,r,\chi)$ \\
        return $v$ \\-
    $\chi^{j-1} = \hat R \chi$ \\
    $\phi^j = \frac{1}{2} (\chi - P\chi^{j-1})$ \\
    $\text{\pr{pgssweep}}^{\text{\id{down}}}(j,v,r,\phi^j)$ \\
    $\tilde r[\cdot] = r[\cdot] - a^j(v,\cdot)$ \\
    $v^{j-1} =$ \pr{cd-vcycle}(j-1,R \tilde r,\chi^{j-1}) \\
    $v \gets v + P v^{j-1}$ \\
    $\text{\pr{pgssweep}}^{\text{\id{up}}}(j,\hat v,r,\phi^j)$ \\
    return $v$
\end{pseudo*}

FIXME cite for multigrid obstacle \cite{BrandtCryer1983,Bueler2021,GraeserKornhuber2009,Jouvetetal2013}; cite for subset decomp \cite{Tai2003}

\subsection*{Convergence and performance results} FIXME


\section{Multigrid for a shallow-ice mass conservation problem} \label{sec:sia}

FIXME model problem in previous section has the wrong ``physics'' but correctly addresses the free boundary and obstacle nature of the glacier problem

FIXME cite for glaciers as obstacle problems \cite{Bueler2016,Bueler2020,Calvoetal2002,JouvetBueler2012}

FIXME for 2D domains the coarse mesh construction needs reconsideration


\section{Multigrid for a Glen-Stokes glacier flow} \label{sec:stokes}

FIXME multigrid already used for Blatter-Pattyn model \cite{BrownSmithAhmadia2013} and for hybrid \cite{Jouvetetal2013}; one goal of this section is to make these approaches more understandable

FIXME we use Schur complement \cite{Bueler2021,Elmanetal2014} and compare it to Vanka monolithic smoother \cite{Farrelletal2019}

\small

\bigskip
\bibliography{review}
\bibliographystyle{siam}

\normalsize
%\clearpage

\appendix
\section{Notation}

\renewcommand{\arraystretch}{1.2}
\begin{longtable}{l|l}
\textbf{Symbol} {\Large$\strut$} & \textbf{Meaning} \\ \hline
$a(\cdot,\cdot)$ & bilinear form associated to the Poisson problem; left side of equation \eqref{eq:weakpoissonearly} \\
$F(w)[\cdot]$ & residual of iterate $w$, e.g.~$F(w)[v] = \ip{f}{v} - a(w,v)$ for the Poisson equation \\
$\mathcal{H}$ & Hilbert space for the continuum problem \\
$J$ & level index of finest mesh \\
$j$ & index of mesh level; $j=0,1,\dots,J$ from coarse to fine \\
$\mathcal{K}$ & constraint (admissible) subset; $\mathcal{K} \subset \mathcal{H}$; e.g.~$\mathcal{K} = \{v \ge \varphi\}$ \\
$\mathcal{K}^j$ & $j$th-level admissible functions; $\mathcal{K}^j = \{v \ge \phi^j\} \subset \mathcal{V}^j$ \\
$\ell[\cdot]$ & linear functional, e.g.~$\ell[v] = \ip{f}{v}$ \\
$m_j$ & number of nodes in $j$th-level; $\dim \mathcal{V}^j=m_j$ \\
$P$ & canonical prolongation of functions, $\mathcal{V}^{j-1} \to \mathcal{V}^j$; equation \eqref{eq:canonicalprolongation} \\
$R$ & canonical restriction of linear functionals, $(\mathcal{V}^j)' \to (\mathcal{V}^{j-1})'$; equation \eqref{eq:canonicalrestriction} \\
$\hat R$ & monotone restriction of functions, $\mathcal{V}^j \to \mathcal{V}^{j-1}$; equation \eqref{eq:monotonerestriction} \\
$\mathcal{V}^h$ & finite element function space; $= \mathcal{V}^J$ \\
$\mathcal{V}^j$ & $j$th-level vector space \\
$(\mathcal{V}^j)'$ & dual space (linear functionals) of $\mathcal{V}^j$  \\
$x_p^j$ & $p$th node on $j$th-level mesh \\
$\Phi$ & scalar-valued objective function, e.g.~$\Phi(v) = \frac{1}{2} a(v,v) - \ip{f}{v}$ for the Poisson equation \\
$\grad \Phi(w)[\cdot]$ & gradient of objective function; note $F(w)[v] = -\grad \Phi(w)[v]$ \\
$\varphi$ & obstacle in continuum problem \\
$\phi^j(x)$ & $j$th-level obstacle in constraint decomposition method; $\phi^j=\chi^j - P\chi^{j-1}$ \\
$\psi_p^j(x)$ & $j$th-level hat function at $x_p$ \\
$\chi^j(x)$ & $j$th-level (monotone) restriction of defect obstacle; $\chi^J = \varphi^J - w^J$ and $\chi^{j-1} = \hat R \chi^j$ \\
$\ip{\cdot}{\cdot}$ & $L^2$ inner product \\
$\|\cdot\|$ & $L^2$ norm; $\|f\|=\ip{f}{f}^{1/2}$
\end{longtable}

\end{document}
