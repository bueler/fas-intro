\documentclass[letterpaper,final,12pt,reqno]{amsart}

\usepackage[total={6.3in,9.2in},top=1.1in,left=1.1in]{geometry}

\usepackage{times,bm,bbm,empheq,verbatim,fancyvrb,graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}

% hyperref should be the last package we load
\usepackage[pdftex,
colorlinks=true,
plainpages=false, % only if colorlinks=true
linkcolor=blue,   % ...
citecolor=Red,    % ...
urlcolor=black    % ...
]{hyperref}

\renewcommand{\baselinestretch}{1.05}

\newtheorem{lemma}{Lemma}

\newcommand{\Matlab}{\textsc{Matlab}\xspace}
\newcommand{\eps}{\epsilon}
\newcommand{\RR}{\mathbb{R}}

\newcommand{\grad}{\nabla}
\newcommand{\Div}{\nabla\cdot}
\newcommand{\trace}{\operatorname{tr}}

\newcommand{\hbn}{\hat{\mathbf{n}}}

\newcommand{\bb}{\mathbf{b}}
\newcommand{\bbf}{\mathbf{f}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bn}{\mathbf{n}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}

\newcommand{\bV}{\mathbf{V}}
\newcommand{\bX}{\mathbf{X}}

\newcommand{\bxi}{\bm{\xi}}

\newcommand{\bzero}{\bm{0}}

\newcommand{\rhoi}{\rho_{\text{i}}}
\newcommand{\ip}[2]{\left<#1,#2\right>}

\begin{document}
\title[FAS, the full approximation storage scheme]{FAS, the full approximation storage scheme: \\ A 1D example}

\author{Ed Bueler}

\begin{abstract}  This short note describes the FAS scheme for an easy 1D finite element solution of a nonlinear boundary value problem.  An accompanying Python script implements the scheme.
\end{abstract}

\maketitle

\thispagestyle{empty}
\bigskip

We consider Brandt's FAS (full approximation storage) scheme \cite{Brandt1977} for an easy nonlinear elliptic equation.  A helpful write-up of the scheme can already be found in a well-known textbook \cite[Chapter 6]{Briggsetal2000}, but we want to describe the scheme from a finite element point of view which is compatible with our other codes, related to the approach in \cite{GraeserKornhuber2009}, and furthermore we provide a Python implementation.

The problem is an ordinary differential equation (ODE) boundary value problem, namely the nonlinear (semilinear) Liouville-Bratu problem \cite[for example]{Bueler2021}
\begin{equation}
  -u'' - \mu e^u = 0,  \qquad u(0) = u(1) = 0  \label{liouvillebratu}
\end{equation}
where $\mu$ is constant.  Our goal is to solve this problem in optimal time by a Python implementation (\texttt{py/fas1.py}) of the FAS.  This note serves as documentation for the program.


\section{The main components}

We solve problem \eqref{liouvillebratu} using the finite element (FE) method \cite{Bueler2021,Elmanetal2014}, so first we rewrite it in weak form.  Let $F$ be the nonlinear operator
\begin{equation}
  F(u)[v] = \int_0^1 u'(x) v'(x) - \mu e^{u(x)} v(x)\, dx,  \label{operator}
\end{equation}
acting on $u$ and $v$ in the space of functions $\mathcal{H}=H_0^1[0,1]$.  These are functions which have zero boundary values and a square-integrable derivative.  We want to find $u$ in $\mathcal{H}$ so that $F(u)[v] = 0$ for all such $v$ in $\mathcal{H}$.  One derives this equation, and thus the operator \eqref{operator}, by multiplying \eqref{liouvillebratu} by $v$ and integrating by parts.

Actually, we solve the more general equation
  $$-u'' - \mu e^u = g$$
for a given function $g(x)$, with the same boundary conditions.  This is in the optional case for runtime option \texttt{-mms}, the ``method of manufactured solutions''.  (In detail, the exact solution is $u(x)=\sin(3\pi x)$ and thus $g(x)=9\pi^2 u(x)-\mu e^{u(x)}$.)  The nonlinear operator is the same but the weak form is
\begin{equation}
  F(u)[v] = \ip{g}{v} \label{weakform}
\end{equation}
for all $v$, where $\ip{g}{v} = \int_0^1 g(x) v(x) dx$.  Equation \eqref{liouvillebratu} is the case where $g=0$.  Observe that $F(u)[\cdot]$ and $\ip{g}{\cdot}$ are (continuous) linear functionals, acting on functions in $\mathcal{H}$.  From now on, \eqref{weakform} is the official weak form of our problem.

As this example uses the simplest possible FE setup, we define equally-spaced meshes on $[0,1]$ with $m$ subintervals, of length $h=1/m$, and $p=1,\dots,m-1$ interior nodes (points) $x_p=ph$.  On this mesh there is a finite-dimensional space $\mathcal{S}^h$ of piecewise-linear and continuous functions with zero boundary values; note $\mathcal{S}^h$ is a subset of $\mathcal{H}$.  The vector space $\mathcal{S}^h$ has a basis of hat functions $\{\lambda_p(x)\}$, for the interior nodes, defined by the two properties that $\lambda_p$ is in $\mathcal{S}^h$ and that $\lambda_p(x_q)=\delta_{pq}$.

In terms of the hat function basis, the numerical solution is
\begin{equation}
  u^h(x) = \sum_{p=1}^{m-1} u[p] \lambda_p(x).  \label{fesolution}
\end{equation}
Note $u^h(x)$ is a function but, because the hat functions form a ``nodal basis'', the solution is also represented as a (coefficient) vector with point values: $\bu^h =\{u[p]\}$.

Function $u^h$ will solve a finite-dimensional nonlinear system, and in fact we will not compute it exactly.  (At best, to within rounding errors.)  We will, at any stage, possess an iterate $w^h(x)$ instead, for which the error is
\begin{equation}
  e^h = w^h - u^h,  \label{error}
\end{equation}
and we want the norm $\|e^h\|$ to be small.  In the \texttt{-mms} case where the exact solution of the continuum problem is known, $e^h$ will be computable, but otherwise only our knowledge of the residual (below) will allow us to know the rate at which $\|e^h\|$ goes to zero, i.e.~the rate at which iterates $w^h$ are approaching $u^h$.

The FE approximation of nonlinear operator $F$, also defined by formula \eqref{operator}, but now acting on piecewise-linear functions, is denoted $F^h$.  Function \texttt{FF()} in \texttt{fas1.py} computes $F^h(w^h)$ on the given mesh for a given iterate $w^h$ with point values $w[p]$, that is, it computes a value $F^h(w^h)[\lambda_p]$ for each interior point:
\begin{align}
  F^h(w^h)[\lambda_p] &= \int_0^1 (w^h)'(x) \lambda_p'(x) - \mu e^{w^h(x)} \lambda_p(x)\, dx  \label{feoperator} \\
    &= \int_{x_{p-1}}^{x_{p+1}} (w^h)'(x) \Big(\pm \frac{1}{h}\Big)\,dx - \mu \int_{x_{p-1}}^{x_{p+1}} e^{w^h(x)} \lambda_p(x)\, dx \notag \\
    &\approx h \left(\frac{w[p]-w[p-1]}{h} - \frac{w[p+1]-w[p]}{h}\right) - h \mu e^{w[p]}  \notag \\
    &= \frac{1}{h}\left(2w[p]-w[p-1]-w[p+1]\right) - h \mu e^{w[p]} \notag
\end{align}
Notice that the support of $\lambda_p(x)$ is $[x_{p-1},x_{p+1}]$ and that the derivative of $\lambda_p$ is $\pm 1/h$, depending on the side of $x_p$.  The first integral is computed exactly while the second uses the trapezoid rule.  In fact we define $F^h$ as the final expression, which is a differently-scaled form of the obvious $O(h^2)$ finite difference scheme.

Recall that equation \eqref{weakform}, with a nonzero right-hand side, is our weak form, and thus in general we are not seeking a zero of $F^h$ itself.  Denote the approximate right-hand-side function by $g^h$; in practice this is the interpolant of $g(x)$.  To solve \eqref{weakform} we seek an iterate $w^h$ so that the residual
\begin{equation}
  r^h(w^h)[v] = \ip{g^h}{v} - F^h(w^h)[v]  \label{feresidual}
\end{equation}
is small.  The integral $\ip{g^h}{v}$ is done by the trapezoid rule; again we actually define $r^h(w^h)$ by the resulting expression.

Solving \eqref{weakform} is equivalent, of course, to finding $w^h$ so that $r^h(w^h)[v]=0$ for all $v$ in $\mathcal{H}$.  The object $r^h(w^h)$ might be regarded as a vector with point values, but we prefer to regard it, and likewise $F^h(w^h)$, as a linear functional acting on functions in $\mathcal{S}_h$.

In the next section we actually describe the multigrid FAS algorithm, but here we finish our description of elementary components by describing an iteration which will, if carried far enough, also solve the problem.  The method is a nonlinear form of the Gauss-Seidel iteration \cite{Briggsetal2000}, called ``NGS''.  It updates the iterate $w^h$ by changing its point value at $x_p$ to make the residual at that point zero.  That is, it solves the problem
\begin{equation}
r^h(w^h + c \lambda_p)[\lambda_p] = 0  \label{ngspointproblem}
\end{equation}
for a scalar $c$.  That is, at each point NGS solves a scalar problem $f(c)=0$ where
\begin{equation}
  f(c) = r^h(w^h + c \lambda_p)[\lambda_p], \label{ngspointresidual}
\end{equation}
and then it updates
\begin{equation}
  w^h \leftarrow w^h + c \lambda_p  \label{ngspointupdate}
\end{equation}
or equivalently $w[p] \leftarrow w[p] + c$.  Following the idea of the linear Gauss-Seidel iteration \cite{Greenbaum1997}, each point value is updated using the already-updated values, i.e.~including the latest values of the neighbors $w[p-1]$ and $w[p+1]$ when evaluating the residual in \eqref{ngspointproblem}.  Gauss-Seidel-type methods are sometimes called ``multiplicative'' or ``successive'' corrections, in contrast to ``additive'' or ``parallel'' corrections, of which the Jacobi iteration is the best known \cite{Bueler2021}.

Even solving the scalar problem $f(c)=0$ cannot be done exactly when considering a transcendental problem like \eqref{liouvillebratu}.  Thus we will use a fixed number (\texttt{-niters}; defaults to 2) of Newton iterations to generate a (scalar) sequence $\{c_k\}$.  We start from initial iterate $c_0=0$, thus assuming that the current iterate $w^h$ for the whole problem is reasonably close
\begin{equation}
f'(c_k)\, s_k = -f(c_k),  \qquad  c_{k+1} = c_k + s_k. \label{ngsnewton}
\end{equation}
Because the vast majority of the FAS algorithm work is in evaluating these NGS expressions, we record the following details which follow from \eqref{feoperator} and \eqref{feresidual}:
\begin{align}
   f(c) &= -\frac{1}{h} \left(2(w[p]+c) - w[p-1] - w[p+1]) + h \left(\mu e^{w[p]+c} + g[p]\right)\right), \label{ngsnewtondetails} \\
   f'(c) &= -\frac{2}{h} + h \mu e^{w[p]+c}. \notag
\end{align}

The NGS method is applied by ``sweeping'' through the mesh, zeroing the residual at successive points in some order.  Function \texttt{ngssweep()} in \texttt{fas1.py} computes one sweep of NGS by using a fixed number (\texttt{-niters}) of scalar Newton iterations \eqref{ngsnewton} to solve \eqref{ngspointproblem} at each point.  Note that as soon as the residual is made zero at one point it is no longer zero at the previous points (which were just zeroed).  In a linear case the iteration is known to converge based on structural properties of the matrices \cite[for example]{Greenbaum1997} which correspond to ellipticity of the original problem, and we expect that at least for weak nonlinearities (e.g.~small $\mu$ in \eqref{liouvillebratu}) our method will converge as solution method for \eqref{weakform}.  However, one observes immediately that, after distinct progress in the first few sweeps, soon NGS stagnates.  Following Brandt \cite{Brandt1977}, who asserts that any such ``stalling'' scheme must be ``wrong'', we adopt a multilevel approach next.


\section{The FAS algorithm}

FIXME FAS is a multigrid scheme, and thus it includes a hierarchy of meshes and also choices of a ``smoother'' on each level and a coarse-mesh solution method \cite{Briggsetal2000}.  We describe only two levels at first, with the coarser mesh having a larger spacing $H>h$ corresponding to fewer subintervals.  The program \texttt{fas1.py} assumes refinement only by a factor of two, so the coarse mesh has $m/2$ subintervals of length $H=2H$.  An iterate on the coarse mesh will be denoted $w^H$.

FIXME NGS is smoother

A few sweeps of NGS accomplishes two actions on the current iterate $w^h$ on the fine mesh:
\renewcommand{\labelenumi}{\emph{\roman{enumi})}}
\begin{enumerate}
\item making the residual $r(w^h)$ smooth, but not necessarily small, and
\item making the error (difference) $e^h = w^h - u^h$ smooth, but not necessarily small.
\end{enumerate}
Using more sweeps of NGS would eventually make the residual $r(w^h)$ small and thus solve problem \eqref{weakform}.  However, as a multigrid method, FAS instead transfers the problem to a coarser mesh where the same residual function is, essentially, less smooth; more of the magnitude of the coarse-mesh interpolant of the fine mesh iterate can be eliminated by smoothing.  Thus NGS sweeps on the coarser mesh are effective at reducing the magnitude of the residual.  Descending further, to yet coarser meshes, eventually leads to a residual which can be eliminated entirely at the few interior points of the coarsest mesh.

Noting that the operator $F$ in \eqref{operator} is nonlinear in $u$, the FAS method  proposes a \emph{new} equation on the coarse mesh.  Because the fine-mesh smoother has already been applied, the new equation relates smooth quantities which should be well-approximated on the coarse mesh.  The FAS equation is
\begin{equation}
  F^H(u^H) - F^H(R w^h) = R r^h(w^h). \label{fasequation}
\end{equation}
Here $u^H$ is the solution of this equation on the coarse mesh, a function in the space of piecewise-linear and continuous functions $S^H$ on this mesh.  The linear restriction operation $R:S^h \to S^H$ maps a vector on the fine mesh to the coarse mesh by ``full-weighting'', i.e.~by averaging onto the coarse mesh.

FIXME where does the FAS equation come from

Note that if $w^h$ were the exact solution to the fine mesh problem then the right side of \eqref{fasequation} would be zero and the solution would be $u^H = R w^h$ by well-posedness.

Thus on the coarse mesh we need to solve
\begin{equation}
  F^H(u^H)[v] = \ip{g^H}{v},  \label{weakformcoarse}
\end{equation}
for $v$ any test function on the coarse mesh, where
  $$g^H = R (g^h - F^h(w^h)[v]) + F^H(R w^h)[v]$$

If the coarse grid is in fact the coarsest grid then we propose to solve the problem by sufficiently many sweeps of nonlinear Gauss-Seidel so that the residual $r(u^H)[v] = \ip{g^H}{v} - F^H(u^H)[v]$ is essentially zero.  However, if the coarse grid consists of only one interior point, which it will have with default settings, then this could involve a single NGS update as long as the Newton iteration is very accurate.

After solving the coarse problem \eqref{weakformcoarse} we have $u^H$, assumed exact for the rest of the presentation.  The final step of FAS at a given level is the update on the finer mesh,
\begin{equation}
  w^h \longleftarrow w^h + P(u^H - R w^h) \label{fasupdate}
\end{equation}
Here $P=R^\top$ is prolongation, acting by linear interpolation.

FIXME V-cycles

\small

\bigskip
\bibliography{fas}
\bibliographystyle{siam}

\end{document}
