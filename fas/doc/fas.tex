\documentclass[letterpaper,final,12pt,reqno]{amsart}

\usepackage[total={6.3in,9.2in},top=1.1in,left=1.1in]{geometry}

\usepackage{times,bm,bbm,empheq,verbatim,fancyvrb,graphicx}
\usepackage[dvipsnames]{xcolor}

\usepackage[kw]{pseudo}

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}

% hyperref should be the last package we load
\usepackage[pdftex,
colorlinks=true,
plainpages=false, % only if colorlinks=true
linkcolor=blue,   % ...
citecolor=Red,    % ...
urlcolor=black    % ...
]{hyperref}

\renewcommand{\baselinestretch}{1.05}

\newtheorem{lemma}{Lemma}

\newcommand{\Matlab}{\textsc{Matlab}\xspace}
\newcommand{\eps}{\epsilon}
\newcommand{\RR}{\mathbb{R}}

\newcommand{\grad}{\nabla}
\newcommand{\Div}{\nabla\cdot}
\newcommand{\trace}{\operatorname{tr}}

\newcommand{\hbn}{\hat{\mathbf{n}}}

\newcommand{\bb}{\mathbf{b}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bbf}{\mathbf{f}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bn}{\mathbf{n}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}

\newcommand{\bV}{\mathbf{V}}
\newcommand{\bX}{\mathbf{X}}

\newcommand{\bxi}{\bm{\xi}}

\newcommand{\bzero}{\bm{0}}

\newcommand{\rhoi}{\rho_{\text{i}}}
\newcommand{\ip}[2]{\left<#1,#2\right>}

\begin{document}
\title[Full approximation storage]{The full approximation storage scheme: \\ 1D finite element example}

\author{Ed Bueler}

\begin{abstract}  This note describes the FAS scheme for an easy 1D finite element solution of a nonlinear boundary value problem.  An accompanying Python script implements the scheme.
\end{abstract}

\maketitle

\thispagestyle{empty}
\bigskip

\section{Introduction}

We consider the FAS (full approximation storage) scheme, originally described by Brandt in \cite{Brandt1977}, for an easy nonlinear elliptic equation.  Like other multigrid schemes it can exhibit optimal solver complexity \cite{Bueler2021}.  While helpful write-ups of FAS can be found in well-known textbooks like \cite[Chapter 6]{Briggsetal2000} and \cite{Trottenbergetal2001}, we describe the scheme from a finite element point of view, compatible with other codes in the current repository\footnote{\href{https://github.com/bueler/mg-glaciers}{\texttt{github.com/bueler/mg-glaciers}}}, and following the approaches in \cite{GraeserKornhuber2009} for obstacle problems.

Our problem is an ordinary differential equation (ODE) boundary value problem, namely the nonlinear (semilinear) Liouville-Bratu equation \cite{Bratu1914,Liouville1853}:
\begin{equation}
  -u'' - \lambda\, e^u = g,  \qquad u(0) = u(1) = 0.  \label{liouvillebratu}
\end{equation}
In this problem $\lambda$ is a real constant, $g(x)$ is given, and we seek $u(x)$.

Our goal is to solve \eqref{liouvillebratu} in optimal $O(m)$ time on a mesh of $m$ elements.  A Python implementation of FAS, namely \texttt{fas1.py} in directory \texttt{py/}, accomplishes this, and this note serves as its documentation.  The program defaults to solving the classic Liouville-Bratu equation with $g=0$, but the runtime option \texttt{-mms}, for the ``method of manufactured solutions'' \cite{Bueler2021}, specifies a problem with known exact solution, facilitating testing.  In this case $u_{\text{ex}}(x)=\sin(3\pi x)$, and thus by differentiation we have $g(x)=9\pi^2 \sin(3\pi x) - \lambda e^{\sin(3\pi x)}$.


\section{The finite element method}

We solve the problem using the finite element (FE) method \cite{Bueler2021,Elmanetal2014}, so first we rewrite \eqref{liouvillebratu} in weak form.  Let $F$ be the nonlinear operator
\begin{equation}
  F(u)[v] = \int_0^1 u'(x) v'(x) - \lambda e^{u(x)} v(x)\, dx,  \label{operator}
\end{equation}
acting on $u$ and $v$ from the space of functions $\mathcal{H}=H_0^1[0,1]$, namely functions which have zero boundary values and one square-integrable derivative.  One derives \eqref{operator} by multiplying \eqref{liouvillebratu} by a test function $v$ and integrating by parts.  Then the weak form of \eqref{liouvillebratu} is
\begin{equation}
  F(u)[v] = \ell[v] \label{weakform}
\end{equation}
for all $v$, where
\begin{equation}
  \ell[v] = \ip{g}{v} = \int_0^1 g(x) v(x) dx.  \label{rhsfunctional}
\end{equation}
Observe that $F(u)[\cdot]$ and $\ell[\cdot]$ are (continuous) linear functionals, acting on functions in $\mathcal{H}$.

From now on we address seemingly-abstract problem \eqref{weakform}.  This level of abstraction is useful because in an FE context a clear separation can be made between functions like the solution $u(x)$, on the one hand, and the equations we are solving on the other.  Both functions and equations will be indexed, similarly to how in linear algebra one has both column indices for unknowns and row indices for equations.  In our continuum problem the ``$v$th equation'', i.e.~indexed by the test function $v$, is exactly \eqref{weakform}.  For the FE discrete problem we will reduce to a finite number of unknowns by writing $u(x)$ in a basis of a finite-dimensional subspace of $\mathcal{H}$.  We will reduce to finitely-many equations by only choosing test functions $v$ from the same subspace.

We apply the simplest possible FE setup, namely an equally-spaced mesh on $[0,1]$ of $m$ subintervals (elements) of lengths $h=1/m$.  The interior nodes (points) are $x_p=ph$ for $p=1,\dots,m-1$.  As promised, this mesh supports a finite-dimensional vector subspace of $\mathcal{H}$:
\begin{equation}
\mathcal{S}^h = \left\{v(x)\,\big|\,v \text{ is continuous, linear on each subinterval, and } v(0)=v(1)=0\right\}.  \label{fespace}
\end{equation}

The FE space $\mathcal{S}^h$ has a basis of ``hat'' functions $\{\psi_p(x)\}$, one for each interior node (Figure \ref{fig:onehat}).  A hat function $\psi_p$ is defined by two properties: $\psi_p$ is in $\mathcal{S}^h$ and $\psi_p(x_q)=\delta_{pq}$ for all $q$.  Note that the $L^2$ norm of $\psi_p$ depends on the mesh resolution $h$, and that $\ip{\psi_p}{\psi_q}\ne 0$ for three indices $q=p-1,p,p+1$.  It follows that this basis, while well-conditioned, is not orthonormal.

\begin{figure}
\includegraphics[width=0.6\textwidth]{figs/onehat.pdf}
\caption{A piecewise-linear hat function $\psi_p(x)$ lives at each interior node $x_p$.}
\label{fig:onehat}
\end{figure}

In terms of this basis the numerical solution is
\begin{equation}
  u^h(x) = \sum_{p=1}^{m-1} u[p] \psi_p(x),  \label{fesolution}
\end{equation}
We seek the coefficients $u[p]$.  In fact, because the hat functions form a ``nodal basis'' \cite{Elmanetal2014}, the function $u^h$ may be represented as a vector in $\RR^{m-1}$ either by its coefficients or its point values:
\begin{equation}
\bu =\{u[p]\} = \{u^h(x_p)\}.  \label{fevector}
\end{equation}

The function $u^h(x)$, equivalently the vector $\bu$, will solve a finite-dimensional nonlinear system.  We will not compute it exactly, but we seek to find it within a smaller error than the discretization error of the FE method \cite{Elmanetal2014}.  However, at each stage of the method we will possess exact knowledge of an iterate $w^h(x)$ in $\mathcal{S}^h$, for which the numerical error (function) is
\begin{equation}
  e^h = w^h - u^h.  \label{feerror}
\end{equation}
While we want the norm $\|e^h\|$ to be small, generally only the residual norm (below) is computable.  The rate at which $\|e^h\|$ goes to zero as $h\to 0$ will be controlled by measurable residual norms $\|r^h\|$ because, at least in the linear case, $\|e^h\|$ is bounded to within a condition number by $\|r^h\|$ \cite[Chapter 2]{Bueler2021}.  In the \texttt{-mms} case of our program \texttt{fas1.py}, where the exact solution of the continuum problem is known, $\|e^h\|$ will also be directly computable.

The FE approximation $F^h$ of the nonlinear operator $F$ in \eqref{operator} acts on piecewise-linear functions in $\mathcal{S}^h$.  Its values $F^h(w^h)[\psi_p]$ are easily computable if the transcendental part is approximated using the trapezoid rule:
\begin{align}
  F^h(w^h)[\psi_p] &= \int_0^1 (w^h)'(x) \psi_p'(x) - \lambda e^{w^h(x)} \psi_p(x)\, dx  \label{feoperator} \\
    &= \int_{x_{p-1}}^{x_{p+1}} (w^h)'(x) (\pm 1/h)\,dx - \lambda \int_{x_{p-1}}^{x_{p+1}} e^{w^h(x)} \psi_p(x)\, dx \notag \\
    &\approx h \left(\frac{w[p]-w[p-1]}{h} - \frac{w[p+1]-w[p]}{h}\right) - h \lambda e^{w[p]}  \notag \\
    &= \frac{1}{h}\left(2w[p]-w[p-1]-w[p+1]\right) - h \lambda e^{w[p]} \notag
\end{align}
Notice that the support of $\psi_p(x)$ is $[x_{p-1},x_{p+1}]$, and note that the derivative of $\psi_p$ is $\pm 1/h$ depending on the side of $x_p$.  The final formula in \eqref{feoperator} is a rescaled version of a well-known $O(h^2)$ finite difference scheme \cite[Chapter 3]{Bueler2021}.  Function \texttt{FF()} in \texttt{fas1.py} computes this formula on the given mesh for an iterate $w^h$.

Now consider the right-hand-side functional $\ell[v]$ in \eqref{weakform}, which we approximate by $\ell^h[v]$ acting on $\mathcal{S}^h$.  We again apply the trapezoid rule to compute the integral $\ip{g}{\psi_p}$ and thus we get the very simple formula
\begin{equation}
  \ell^h[\psi_p] = h\, g(x_p). \label{ferhs}
\end{equation}
Note we will continue to distinguish between the linear functional $\ell^h$ and the function $g$ even though their values only differ by a factor of the mesh size $h$.

To solve \eqref{weakform} we seek an iterate $w^h$ so that the \emph{residual}
\begin{equation}
  r^h(w^h)[v] = \ell^h[v] - F^h(w^h)[v]  \label{feresidual}
\end{equation}
is small for all $v$ in $\mathcal{S}^h$.  Like $F^h(w^h)$, the object $r^h(w^h)$ is a linear functional acting on functions in $\mathcal{S}_h$, so it suffices to choose only from a basis of test functions, i.e.~$v=\psi_p$, so computationally we will have this formula for the residual:
\begin{equation}
  r^h(w^h)[\psi_p] = \ell^h[\psi_p] - \frac{1}{h}\left(2w[p]-w[p-1]-w[p+1]\right) + h \lambda e^{w[p]}.  \label{feresidualdetail}
\end{equation}
Function \texttt{residual()} in \texttt{fas1.py} computes \eqref{feresidualdetail}.  On the original mesh, which will soon be called the ``fine mesh'', we will insert formula \eqref{ferhs}, but the FAS algorithm is fundamentally a systematic way to introduce new right-hand sides $\ell^h$ on coarser meshes (see below).  In any case, exactly solving the finite-dimensional nonlinear system, i.e.~the FE approximation of \eqref{weakform}, is equivalent to finding $w^h$ in $\mathcal{S}^h$ so that $r^h(w^h)[\psi_p]=0$ for $p=1,\dots,m-1$.

Next we describe an iteration which will, if carried far enough, solve the problem.  The method is a nonlinear form of the Gauss-Seidel iteration \cite{Briggsetal2000}, called NGS.  It updates the iterate $w^h$ by changing its point value at $x_p$ to make the residual at that point zero.  That is, NGS solves the problem
\begin{equation}
r^h(w^h + c \psi_p)[\psi_p] = 0  \label{ngspointproblem}
\end{equation}
for a scalar $c$ at each point, so it solves $\phi(c)=0$ where
\begin{equation}
  \phi(c) = r^h(w^h + c \psi_p)[\psi_p], \label{ngspointresidual}
\end{equation}
and then it updates
\begin{equation}
  w^h \leftarrow w^h + c \psi_p  \label{ngspointupdate}
\end{equation}
or equivalently $w[p] \leftarrow w[p] + c$.  Following the idea behind the linear Gauss-Seidel iteration \cite{Greenbaum1997}, $w[p]$ is updated in some nodal ordering using all known values when evaluating the residual in \eqref{ngspointproblem}.  (By default in \texttt{fas1.py}, in increasing order on $p$.)  Gauss-Seidel-type methods are called ``multiplicative'' \cite{Bueler2021} or ``successive'' \cite{GraeserKornhuber2009} corrections, in contrast to ``additive'' or ``parallel'' corrections, of which the Jacobi iteration \cite{Bueler2021} is the best known example.

Solving the scalar problem $\phi(c)=0$ cannot be done exactly when considering a transcendental problem like \eqref{liouvillebratu}.  Thus we will use a fixed number of Newton iterations \cite[Chapter 4]{Bueler2021} to generate a (scalar) sequence $\{c_k\}$ converging to $c$.  Starting from $c_0=0$ we compute
\begin{equation}
\phi'(c_k)\, s_k = -\phi(c_k),  \qquad  c_{k+1} = c_k + s_k, \label{ngsnewton}
\end{equation}
for $k=0,1,\dots$  In detail, following \eqref{feresidualdetail} we have
\begin{align}
   \phi(c) &= \ell^h[\psi_p] - \frac{1}{h} \left(2(w[p]+c) - w[p-1] - w[p+1]\right) + h \lambda e^{w[p]+c}, \label{ngsnewtondetails} \\
   \phi'(c) &= -\frac{2}{h} + h \lambda e^{w[p]+c}. \notag
\end{align}
The vast majority of the computational work of our FAS algorithm will be in evaluating these NGS expressions.

The NGS method is applied by ``sweeping'' through the mesh, zeroing the residual at successive nodes $x_p$ in some order.  Function \texttt{ngssweep()} in \texttt{fas1.py} computes one sweep of NGS by using a fixed number (\texttt{-niters}; defaults to 2) of scalar Newton iterations \eqref{ngsnewton} to solve \eqref{ngspointproblem} at each point, as in the following pseudocode.

\pseudoset{left-margin=15mm,topsep=5mm,idfont=\texttt}

\begin{pseudo*}
\pr{ngssweep}(w,F,\ell)\text{:} \\+
    $r(w)[v] := \ell[v] - F(w)[v]$ \\
    for $p=1,\dots,m-1$ \\+
        $\phi(c) := r(w + c \psi_p)[\psi_p]$ \\
        $c=0$ \\
        for $k=1,\dots,$\id{niters} \\+
            $c \gets c - \phi(c) / \phi'(c)$ \\-
        $w[p] \gets w[p] + c$ \\-
    return $w$
\end{pseudo*}

During an NGS sweep, as soon as the residual is made zero at one point it is no longer zero at the previous points.  In a linear case the iteration is known to converge subject to matrix assumptions which correspond to ellipticity of the original problem \cite[for example]{Greenbaum1997}.  We expect that, at least for weak nonlinearities, e.g.~small $\lambda$ in \eqref{liouvillebratu}, our method will converge as solution method for \eqref{weakform}.  However, one observes in practice that, after substantial progress in the first few sweeps, soon NGS stagnates.  Following Brandt \cite{Brandt1977}, who asserts that such a ``stalling'' scheme must be ``wrong'', we adopt the multigrid approach next.


\section{The FAS equation for two levels}

Full approximation storage (FAS) \cite{Brandt1977,Briggsetal2000} is a multigrid scheme, and thus it includes a hierarchy of meshes, a choice of a ``smoother'' on each level, and a coarse-mesh solution method.  The fundamental goal of any multigrid scheme is to do a minimal amount of work (smoothing) on a given mesh and then switch to an inexpensive coarser mesh to do the rest of the work.  That is, by transferring (restricting) a version of the problem to the coarser mesh one nearly solves for the remaining error.  This coarse-mesh approximation of the error is added-back (prolonged) to correct the solution on the finer mesh.

We describe only two levels at first, with a coarser mesh having $2h$ spacing and $M=m/2$ subintervals.  Notationally, quantities on the coarse mesh will have superscript ``$2h$'', for example an iterate is $w^{2h}$.  The program \texttt{fas1.py} only refines by such factors of two, but the ideas generalize to other refinement factors.

A small fixed number of NGS sweeps is our smoother on the fine mesh.  (For \texttt{fas1.py}, option \texttt{-downsweeps} defaults to $1$.)  Each sweep, solving \eqref{ngspointproblem} and doing update \eqref{ngspointupdate} at every point $x_p$, is an $O(m)$ operation with a small constant.  The constant relates to the number of Newton iterations and the expense of evaluating nonlinearities at each point, e.g.~$\lambda e^u$ in \eqref{liouvillebratu}.

A few sweeps of NGS on the fine mesh produces two results on the current iterate $w^h$:
\begin{itemize}
\item the residual $r^h(w^h)$ becomes smooth, but not necessarily small, and
\item the error $e^h = w^h - u^h$ becomes smooth, but not necessarily small.
\end{itemize}

Using more sweeps of NGS would eventually make the residual and error small and thus solve problem \eqref{weakform}, but this is inefficient in the sense that many sweeps may be needed, generally giving an $O(m^k)$ method for $k\gg 1$.  However, NGS sweeps on a coarser mesh see the ``same'' residual, i.e.~the coarse-mesh interpolant of the fine mesh residual, as less smooth, thus a large fraciton of the error is quickly eliminated by smoothing.  Descending to yet coarser meshes after a few sweeps, in a V-cycle as described in the next section, leads to a coarsest mesh on which the error can be eliminated entirely by applying NGS at a few interior points.  (In the default settings for \texttt{fas1.py}, the coarsest mesh has two subintervals and only one interior point.)

The question is, what is the coarse-mesh version of the problem?  To derive this equation, namely to explain Brandt's proposed FAS equation, we start from the finite element approximation of the weak form \eqref{weakform} on a given (fine) mesh, i.e.
\begin{equation}
  F^h(u^h)[v] = \ip{g^h}{v} \qquad \text{for all } v \text{ in } \mathcal{S}^h.  \label{feweakform}
\end{equation}
Suppose we have a current iterate $w^h$.  (The solution $u^h$ is generally unknown.)  Subtracting $F^h(w^h)[v]$ from both sides of \eqref{feweakform} gives the residual \eqref{feresidual} on the right:
\begin{equation}
  F^h(u^h)[v] - F^h(w^h)[v] = r^h(w^h)[v] \label{fasproto}
\end{equation}
for $v$ in $\mathcal{S}^h$.  Three key observations apply for equation \eqref{fasproto}:
\begin{itemize}
\item If $F^h$ were linear in $w^h$ then we could rewrite the equation in terms of the error:
    $$F^h(e^h)[v] = -r^h(w^h)[v], \qquad (\text{\emph{if $F^h$ linear}})$$
the linear error equation $A\be=-\br$ \cite[Chapter 2]{Bueler2021}.
\item If NGS sweeps have generated iterate $w^h$ then $e^h$ and $r^h(w^h)$ are smooth.
\item Both $w^h$ and $r^h(w^h)$ are known and/or computable.
\end{itemize}

Noting that our operator $F^h$ is in fact nonlinear (in $w^h$), the FAS method  proposes a \emph{new} equation on the coarse mesh based on the above considerations.  In the linear case the new equation could be phrased directly in terms of the error.  However, in general the coarse-mesh version of \eqref{fasproto} uses linear restriction operators on the computable quantities, and it also re-discretizes the nonlinear operator to $F^{2h}$ acting on $\mathcal{S}^{2h}$.  We denote the restriction operators by $R'$ and $R$:
\begin{equation}
  F^{2h}(u^{2h})[v] = R' (r^h(w^h))[v] + F^{2h}(R w^h)[v] \label{fasequation}
\end{equation}
for all $v$ in $\mathcal{S}^{2h}$.  Here $u^{2h}$ in $\mathcal{S}^{2h}$ is the solution on the coarse mesh.

Note that if $w^h=u^h$, that is, if $w^h$ were the exact solution to the fine mesh problem \eqref{feweakform}, then $r^h(w^h)=0$ and the right side of \eqref{fasequation} would simplify to $F^{2h}(R w^h)[v]$.  In this case the solution of \eqref{fasequation} would be $u^{2h} = R w^h$ by well-posedness, and we would simply produce the restriction of the solution onto the coarser mesh.

If the coarse mesh is in fact the coarsest one in the hierarchy then we propose to solve \eqref{fasequation} by sufficient NGS sweeps so that $u^{2h}$ is computed almost exactly.  Specifically, if the coarse mesh consists of only one interior point, which it will under the default settings in \texttt{fas1.py}, then this requires a single NGS update if the Newton iteration is sufficiently-accurate.

After solving the FAS coarse-mesh equation \eqref{fasequation} we have $u^{2h}$, assumed exact for the next formula.  Two-level FAS updates the iterate on the finer mesh,
\begin{equation}
  w^h \gets w^h + P(u^{2h} - R w^h) \label{fasupdate}
\end{equation}
Here $P$ is the prolongation operation which extends a function in $\mathcal{S}^{2h}$ to a function in $\mathcal{S}^h$.

Formulas \eqref{fasequation} and \eqref{fasupdate} complete the two-level FAS algorithm for a given smoother.  Using the NGS smoother, including as a coarse-mesh solver, it is the following pseudocode.

\begin{pseudo*}[left-margin=15mm]
\pr{fas2level}(w)\text{:} \\+
    for $j=1,\dots,$\id{downsweeps} \\+
        \pr{ngssweep}(w,F^h,g^h) \\-
    $g^{2h}[v] := R' (r^h(w))[v] + F^{2h}(R w^h)[v]$  FIXME \\
    for $j=1,\dots,$\id{coarsesweeps} \\+
        \pr{ngssweep}(w) \\
    $w^h \gets w^h + P(u^{2h} - R w^h)$ \\
    return $a$
\end{pseudo*}

However, in order to understand and implement it, we must clarify the action of operators $R'$, $R$, and $P$ in the context of FE methods.  This is the purpose of the next section.


\section{Restriction and prolongation operators}

To explain the two different restriction operators in \eqref{fasequation}, plus the prolongation in \eqref{fasupdate}, first note that functions $w^h$ in $\mathcal{S}^h$ are distinct objects from linear functionals like the residual $r^h(w^h)$, which act on $v$ in $\mathcal{S}^h$.  We denote the space of such linear functionals by $(\mathcal{S}^h)'$.  The operators are thereby distinguished by their domain and range spaces:
\begin{align}
  R' &: (\mathcal{S}^h)' \to (\mathcal{S}^{2h})', \label{rpoperators} \\
  R  &: \mathcal{S}^h \to \mathcal{S}^{2h}, \notag \\
  P  &: \mathcal{S}^{2h} \to \mathcal{S}^h. \notag
\end{align}

Next note that both functions in $\mathcal{S}^h$ and linear functionals in $(\mathcal{S}^h)'$ are representable by vectors in $\RR^{m-1}$.  For a function $w^h$ one stores the coefficients $w[p]$ with respect to an expansion in the hat function basis $\{\psi_p\}$, as in \eqref{fesolution} for example, while one stores a functional $\ell=r^h(w^h)$ by its values $\ell[\psi_p]$.  Both representations are thus vectors in $\RR^{m-1}$, and though it might make sense to represent $w^h$ as a column vector and $\ell$ as a row vector \cite{TrefethenBau1997}, in Python it is easiest to use ``flat'' one-dimensional NumPy arrays for both.  Observe that within the FAS algorithm an iterate $w^h$ has zero boundary values, and likewise $\ell$ acts on $v$ with zero boundary values, thus only interior-point hat functions are needed in these representations.

But how do $R'$, $R$, and $P$ actually operate in the finite element (FE) case?  A key calculation relates the coarse-mesh hat functions $\psi_q^{2h}(x)$ to the fine mesh hats $\psi_p^h(x)$:
\begin{equation}
  \psi_q^{2h}(x) = \frac{1}{2} \psi_{2q-1}^h(x) + \psi_{2q}^h(x) + \frac{1}{2} \psi_{2q+1}^h(x), \qquad q=1,2,\dots,M-1. \label{hatrelation}
\end{equation}
(Recall that $M=m/2$, and we assume $m$ is even.)  See Figure \ref{fig:hatcombination}.

\begin{figure}
\includegraphics[width=0.6\textwidth]{figs/hatcombination.pdf}
\caption{Formula \eqref{hatrelation} writes a coarse-mesh hat $\psi_q^{2h}(x)$ (solid) as a linear combination of three fine-mesh hats $\psi_p^h(x)$ (dotted) for $p=2q-1,2q,2q+1$.}
\label{fig:hatcombination}
\end{figure}

First consider the prolongation $P$.  It is defined as the injection of $\mathcal{S}^{2h}$ into $\mathcal{S}^h$, without changing the function.  Indeed, a piecewise-linear function on the coarse mesh is also a piecewise-linear function on the fine mesh.  Suppose $w^{2h}(x)$ is in $\mathcal{S}^{2h}$, so
    $$w^{2h}(x) = \sum_{q=1}^{M-1} w[q] \psi_q^{2h}(x).$$
Thus we use \eqref{hatrelation} to compute $P w^{2h}$ in terms of fine-mesh hat functions:
\begin{align}
(P w^{2h})(x) &= \sum_{q=1}^{M-1} w[q] \left(\frac{1}{2} \psi_{2q-1}^h(x) + \psi_{2q}^h(x) + \frac{1}{2} \psi_{2q+1}^h(x)\right) \label{pformula} \\
              &= \frac{1}{2} w[1] \psi_1^h(x) + w[1] \psi_2^h(x) + \left(\frac{1}{2} w[1] + \frac{1}{2} w[2]\right) \psi_3^h(x) \notag \\
              &\qquad + w[2] \psi_4^h(x) + \left(\frac{1}{2} w[2] + \frac{1}{2} w[3]\right) \psi_5^h(x) \notag \\
              &\qquad + \dots + w[M-1] \psi_{m-2}^h(x) + \frac{1}{2} w[M-1] \psi_{m-1}^h(x) \notag
\end{align}
As a matrix, $P$ acts on the representation vectors, i.e.~$P:\RR^{M-1} \to \RR^{m-1}$ has $M-1$ columns and $m-1$ rows:
\begin{equation}
P = \begin{bmatrix}
1/2 & & & \\
1 & & & \\
1/2 & 1/2 & & \\
 & 1 & & \\
 & 1/2 & 1/2 & \\
 & & & \ddots
\end{bmatrix} \label{pmatrix}
\end{equation}
The column sums of $P$ are all two, because from \eqref{hatrelation} each column has a nonzero block $[1/2,1,1/2]^\top$, and the columns are linearly-independent.  Except for the first and last rows, the row sums are all one.

The restriction $R'$ acts on fine-mesh linear functionals $\ell:\mathcal{S}^h \to \RR$.  It is sometimes called ``canonical restriction'' \cite{GraeserKornhuber2009} because the output functional $R'\ell:\mathcal{S}^{2h}\to \RR$ acts on coarse-mesh functions the same way $\ell$ itself acts on those functions.  Thus we may define it using $P$:
\begin{equation}
  (R'\ell)[v] = \ell[Pv]  \label{rprimedefinition}
\end{equation}
for $v$ in $\mathcal{S}^{2h}$.  As noted earlier, $\ell$ is represented by a vector $\bm{\ell}$ in $\RR^{m-1}$, of values $\ell[p] = \ell[\psi_p^h]$, so one computes the values of $R'\ell$ using \eqref{hatrelation}:
\begin{align}
  (R'\ell)[q] &= (R'\ell)[\psi_q^{2h}] = \ell[P\psi_q^{2h}] = \ell\left[\frac{1}{2} \psi_{2q-1}^h + \psi_{2q}^h + \frac{1}{2} \psi_{2q+1}^h\right]  \label{rprimeformula} \\
      &= \frac{1}{2} \ell[\psi_{2q-1}^h] + \ell[\psi_{2q}^h] + \frac{1}{2} \ell[\psi_{2q+1}^h] = \frac{1}{2} \ell[2q-1] + \ell[2q] + \frac{1}{2} \ell[2q+1]  \notag
\end{align}
for $q=1,2,\dots,M-1$.  In other words, as a matrix $R'$ is the transpose of $P$, with $M-1$ rows and $m-1$ columns:
\begin{equation}
R' = \begin{bmatrix}
1/2 & 1 & 1/2 &   &     & \\
    &   & 1/2 & 1 & 1/2 & \\
    &   &     &   & 1/2 & \\
    &   &     &   &     & \ddots
\end{bmatrix} \label{rprimematrix}
\end{equation}

Finally we consider the restriction $R:\mathcal{S}^h\to\mathcal{S}^{2h}$ acting on functions.  This is a more interesting map as it loses information present in the piecewise-linear fine-mesh function.  The result is linear across some fine-mesh nodes (those which are not in the coarse mesh).  By contrast, prolongation $P$ and canonical restriction $R'$ essentially preserve the input object, without loss, but with a reinterpretation on the other mesh.

\newcommand{\Rpr}{R_{\text{pr}}}
\newcommand{\Rin}{R_{\text{in}}}
\newcommand{\Rfw}{R_{\text{fw}}}

Let
\begin{equation}
  w^h = \sum_{p=1}^{m-1} w[p] \psi_p^{h} \label{wfine}
\end{equation}
be a $\mathcal{S}^h$ function on the fine mesh.  There are three well-known versions of restriction which send $w^h$ to a coarse-mesh function in $\mathcal{S}^{2h}$:
\begin{itemize}
\item $\Rpr$ is defined as projection, by the property
\begin{equation}
  \ip{\Rpr w^h}{v} = \ip{w^h}{v} \label{rprdefinition}
\end{equation}
for all $v\in \mathcal{S}^{2h}$.  However, computing the entries of $\Rpr$ requires solving a linear system.  In particular, one must first form the invertible, sparse, symmetric mass matrices \cite{Elmanetal2014}, namely $Q_{jk}^{h} = \ip{\psi_j^{h}}{\psi_k^{h}}$ for the fine mesh and $Q_{jk}^{2h} = \ip{\psi_j^{2h}}{\psi_k^{2h}}$ for the coarse.  Then one solves a matrix equation for $\Rpr$:
\begin{equation}
  Q^{2h} \Rpr = P^\top Q^{h}.  \label{rprequation}
\end{equation}
Here $P$ is the prolongation in \eqref{pformula} and \eqref{pmatrix}; clearly $\Rpr = (Q^{2h})^{-1} P^\top Q^{h}$.

Equation \eqref{rprequation} is justified by using $v=\psi_s^{2h}$ in definition \eqref{rprdefinition}, and then applying \eqref{hatrelation}, as follows.  Write $z = \Rpr w^h = \sum_{q=1}^{M-1} z[q] \psi_q^{2h}$ and expand both sides:
\begin{align*}
\ip{z}{\psi_s^{2h}} &= \ip{w^h}{\psi_s^{2h}} \\
\sum_{q=1}^{M-1} z[q] \ip{\psi_q^{2h}}{\psi_s^{2h}} &= \sum_{p=1}^{m-1} w[p] \ip{\psi_p^{h}}{\frac{1}{2} \psi_{2s-1}^{h} + \psi_{2s}^{h} + \frac{1}{2} \psi_{2s+1}^{h}} \\
\sum_{q=1}^{M-1} Q_{sq}^{2h} z[q] &= \sum_{p=1}^{m-1} \left(\frac{1}{2} Q_{2s-1,p} + Q_{2s,p} + \frac{1}{2} Q_{2s+1,p}\right) w[p] \\
(Q^{2h} \Rpr w^h)[s] &= (P^\top Q^h w^h)[s]
\end{align*}
Hence $Q^{2h} \Rpr = P^\top Q^h$ because $w^h$ in $\mathcal{S}^h$ and index $s$ were arbitrary.

In 1D the mass matrices $Q^{2h},Q^h$ are tridiagonal, thus each column of $\Rpr$ can be found by solving equation \eqref{rprequation} using an $O(M)$ algorithm \cite{TrefethenBau1997}, and forming $\Rpr$ by computing its $M-1$ columns implies $O(M^2)$ work.  While this is possible, and the result can even be found by hand in this case, the alternatives below are simpler.
\item $\Rin$ is defined as another kind of injection,
\begin{equation}
  \Rfw w^h = \sum_{q=1}^{M-1} w[2q] \psi_q^{2h}. \label{rindefinition}
\end{equation}
Observe that $(\Rin w^h)(x_q) = w^h(x_q) = w[2q]$ for each $q$.  In other words, to compute $\Rin w^h$ in $\mathcal{S}^{2h}$ we drop the nodal values at those fine-mesh nodes which are not in the coarse mesh.  As a matrix---compare \eqref{rprimematrix}---this is
\begin{equation}
\Rin = \begin{bmatrix}
0 & 1 &   &   &   &   &\\
  &   & 0 & 1 &   &   & \\
  &   &   &   & 0 & 1 & \\
  &   &   &   &   &   & \ddots
\end{bmatrix}. \label{rinmatrix}
\end{equation}
This restriction is very simple but it may lose track of the magnitude of $w^h$.  For example, sampling a sawtooth function at the coarse-mesh nodes would capture only the peaks or only the troughs.
\item $\Rfw$, the ``full-weighting'' restriction \cite{Briggsetal2000}, averages nodal values onto the coarse mesh:
\begin{equation}
  \Rfw w^h = \sum_{q=1}^{M-1} \left(\frac{1}{4} w[2q-1] + \frac{1}{2} w[2q] + \frac{1}{4} w[2q+1]\right) \psi_q^{2h}. \label{rfwdefinition}
\end{equation}
This computes each coarse-mesh nodal value of $z=\Rfw w^h$ as a weighted average of the value of $w^h$ at that same node plus multiples of the coefficients at the two neighboring fine-mesh nodes.  The latter two nodes are not in the coarse mesh.  The matrix is a multiple of the canonical restriction matrix in \eqref{rprimematrix}:
\begin{equation}
\Rfw = \begin{bmatrix}
1/4 & 1/2 & 1/4 &     &     &  \\
    &     & 1/4 & 1/2 & 1/4 &  \\
    &     &     &     & 1/4 &  \\
    &     &     &     &     & \ddots
\end{bmatrix} = \frac{1}{2} R'. \label{rfwmatrix}
\end{equation}
\end{itemize}

Because of its relative simplicity, and its ability to filter-out high-frequency information while approximately preserving norms, we will choose $R=\Rfw$ as the restriction operator in the implementation.


\section{Multigrid V-cycles}

FIXME V-cycles


\section{Results: convergence and optimality}

FIXME


\small

\bigskip
\bibliography{fas}
\bibliographystyle{siam}

\end{document}
