\documentclass[letterpaper,final,12pt,reqno]{amsart}

\usepackage[total={6.3in,9.2in},top=1.1in,left=1.1in]{geometry}

\usepackage{times,bm,bbm,empheq,verbatim,fancyvrb,graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing}

% hyperref should be the last package we load
\usepackage[pdftex,
colorlinks=true,
plainpages=false, % only if colorlinks=true
linkcolor=blue,   % ...
citecolor=Red,    % ...
urlcolor=black    % ...
]{hyperref}

\renewcommand{\baselinestretch}{1.05}

\newtheorem{lemma}{Lemma}

\newcommand{\Matlab}{\textsc{Matlab}\xspace}
\newcommand{\eps}{\epsilon}
\newcommand{\RR}{\mathbb{R}}

\newcommand{\grad}{\nabla}
\newcommand{\Div}{\nabla\cdot}
\newcommand{\trace}{\operatorname{tr}}

\newcommand{\hbn}{\hat{\mathbf{n}}}

\newcommand{\bb}{\mathbf{b}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bbf}{\mathbf{f}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bn}{\mathbf{n}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}

\newcommand{\bV}{\mathbf{V}}
\newcommand{\bX}{\mathbf{X}}

\newcommand{\bxi}{\bm{\xi}}

\newcommand{\bzero}{\bm{0}}

\newcommand{\rhoi}{\rho_{\text{i}}}
\newcommand{\ip}[2]{\left<#1,#2\right>}

\begin{document}
\title[Full approximation storage]{The full approximation storage scheme: \\ 1D finite element example}

\author{Ed Bueler}

\begin{abstract}  This short note describes the FAS scheme for an easy 1D finite element solution of a nonlinear boundary value problem.  An accompanying Python script implements the scheme.
\end{abstract}

\maketitle

\thispagestyle{empty}
\bigskip

We consider the FAS (full approximation storage) scheme for an easy nonlinear elliptic equation.  A helpful write-up of the scheme, originally described by Brandt in  \cite{Brandt1977}, can be found in well-known textbooks like \cite[Chapter 6]{Briggsetal2000} and \cite{Trottenbergetal2001}.  However, we want to describe the scheme from a finite element point of view, thus compatible with our other codes which relate to the approach in \cite{GraeserKornhuber2009}, and furthermore we provide a Python implementation.

The problem is an ordinary differential equation (ODE) boundary value problem, namely the nonlinear (semilinear) Liouville-Bratu problem \cite{Bueler2021}
\begin{equation}
  -u'' - \mu e^u = 0,  \qquad u(0) = u(1) = 0  \label{liouvillebratu}
\end{equation}
where $\mu$ is constant.  Our goal is to solve this problem in optimal time by a Python implementation of FAS, namely \texttt{fas1.py} in directory \texttt{py/}.  This note serves as its documentation.


\section{The main components}

We solve the problem using the finite element (FE) method \cite{Bueler2021,Elmanetal2014}, so first we plan to rewrite \eqref{liouvillebratu} in weak form.  However, we will actually solve the more general equation
\begin{equation}
  -u'' - \mu e^u = g  \label{stronggeneral}
\end{equation}
for a given function $g(x)$, with the same boundary conditions $u(0)=u(1)=0$.  Equation \eqref{liouvillebratu} is the case where $g=0$.  In \texttt{fas1.py} there is a runtime option \texttt{-mms}, the ``method of manufactured solutions'', for such a generalized problem.  For this case the exact solution is $u(x)=\sin(3\pi x)$, and thus $g(x)=9\pi^2 u(x)-\mu e^{u(x)}$.

Let $F$ be the nonlinear operator
\begin{equation}
  F(u)[v] = \int_0^1 u'(x) v'(x) - \mu e^{u(x)} v(x)\, dx,  \label{operator}
\end{equation}
acting on $u$ and $v$ in the space of functions $\mathcal{H}=H_0^1[0,1]$, namely functions which have zero boundary values and a square-integrable derivative.  One derives operator \eqref{operator} by multiplying \eqref{stronggeneral} by $v$ and integrating by parts.  Then the weak form of \eqref{stronggeneral} is
\begin{equation}
  F(u)[v] = \ip{g}{v} \label{weakform}
\end{equation}
for all $v$, where $\ip{g}{v} = \int_0^1 g(x) v(x) dx$.  Observe that $F(u)[\cdot]$ and $\ip{g}{\cdot}$ are (continuous) linear functionals, acting on functions in $\mathcal{H}$.  From now on we address problem \eqref{weakform}.

As this example uses the simplest possible FE setup, we define equally-spaced meshes on $[0,1]$ with $m$ subintervals (elements), of length $h=1/m$, and $p=1,\dots,m-1$ interior nodes (points) $x_p=ph$.  On this mesh lives a finite-dimensional vector space
\begin{equation}
\mathcal{S}^h = \left\{v(x)\,\big|\,v \text{ is continuous, linear on each subinterval, and } v(0)=v(1)=0\right\}.  \label{fespace}
\end{equation}
Note $\mathcal{S}^h$ is a subset of $\mathcal{H}$, and it has a basis of ``hat'' functions $\{\lambda_p(x)\}$ for the interior nodes.  These hat functions are defined by the two properties that $\lambda_p$ is in $\mathcal{S}^h$ and that $\lambda_p(x_q)=\delta_{pq}$.  Note that the $L^2$ norm of a hat function depends on the mesh resolution, and $\ip{\lambda_p}{\lambda_q}\ne 0$ for $q=p\pm1$, thus the basis $\{\lambda_p\}$, while well-conditioned, is not close to orthonormal.

In terms of the hat function basis, the numerical solution is
\begin{equation}
  u^h(x) = \sum_{p=1}^{m-1} u[p] \lambda_p(x).  \label{fesolution}
\end{equation}
Because the hat functions form a ``nodal basis'', the function $u^h$ may be represented as a vector in $\RR^{m-1}$ by its coefficients or equivalently by its point values:
\begin{equation}
\bu =\{u[p]\} = \{u^h(x_p)\}.  \label{fevector}
\end{equation}

Function $u^h$ will solve a finite-dimensional nonlinear system.  However, we will not compute it exactly, and at best it will be known within rounding error.  Instead we will, at any stage, possess an iterate $w^h(x)$, for which the error is
\begin{equation}
  e^h = w^h - u^h.  \label{feerror}
\end{equation}
We want the norm $\|e^h\|$ to be small, but generally only the residual norm (below) will be computable.  The rate at which iterates $w^h$ approach $u^h$ will be controlled by measurable residual norms because, at least in the linear case, these quantities are bounded to within a condition number \cite[Chapter 2]{Bueler2021}.  In the \texttt{-mms} case, where the exact solution of the continuum problem is actually known, $\|e^h\|$ will be directly computable.

The FE approximation $F^h$ of the nonlinear operator $F$ acts on piecewise-linear functions in $\mathcal{S}^h$.  Its interior-point values $F^h(w^h)[\lambda_p]$ are easily computable if the transcendental part is approximated using the trapezoid rule:
\begin{align}
  F^h(w^h)[\lambda_p] &= \int_0^1 (w^h)'(x) \lambda_p'(x) - \mu e^{w^h(x)} \lambda_p(x)\, dx  \label{feoperator} \\
    &= \int_{x_{p-1}}^{x_{p+1}} (w^h)'(x) (\pm 1/h)\,dx - \mu \int_{x_{p-1}}^{x_{p+1}} e^{w^h(x)} \lambda_p(x)\, dx \notag \\
    &\approx h \left(\frac{w[p]-w[p-1]}{h} - \frac{w[p+1]-w[p]}{h}\right) - h \mu e^{w[p]}  \notag \\
    &= \frac{1}{h}\left(2w[p]-w[p-1]-w[p+1]\right) - h \mu e^{w[p]} \notag
\end{align}
Notice that the support of $\lambda_p(x)$ is $[x_{p-1},x_{p+1}]$ and that the derivative of $\lambda_p$ is $\pm 1/h$, depending on the side of $x_p$.  The final formula is a rescaled version of an $O(h^2)$ finite difference scheme \cite[Chapter 3]{Bueler2021}.  Function \texttt{FF()} in \texttt{fas1.py} computes this formula on the given mesh for a given iterate $w^h$ with point values $w[p]$.

Denote the approximate right-hand-side function in \eqref{weakform} by $g^h$.  In practice this is the interpolant of $g(x)$, or equivalently we compute $\ip{g}{\lambda_p}\approx h g[p]$ by the trapezoid rule.   To solve \eqref{weakform} we seek an iterate $w^h$ so that the \emph{residual}
\begin{equation}
  r^h(w^h)[v] = \ip{g^h}{v} - F^h(w^h)[v]  \label{feresidual}
\end{equation}
is small for all $v\in \mathcal{S}^h$.  The object $r^h(w^h)$, likewise $F^h(w^h)$, is a linear functional acting on functions in $\mathcal{S}_h$.  If $v=\lambda_p$ is a hat function then
\begin{equation}
  r^h(w^h)[\lambda_p] = h g[p] - \frac{1}{h}\left(2w[p]-w[p-1]-w[p+1]\right) + h \mu e^{w[p]}.  \label{feresidualdetail}
\end{equation}
Function \texttt{residual()} in \texttt{fas1.py} computes this formula.  Solving \eqref{weakform} is equivalent to finding $w^h$ so that $r^h(w^h)[\lambda_p]=0$ for $p=1,\dots,m-1$.

In the next section we actually describe the multigrid FAS algorithm, but here we conclude the elementary components by describing an iteration which will, if carried far enough, also solve the problem.  The method is a nonlinear form of the Gauss-Seidel iteration \cite{Briggsetal2000}, called NGS.  It updates the iterate $w^h$ by changing its point value at $x_p$ to make the residual at that point zero.  That is, NGS solves the problem
\begin{equation}
r^h(w^h + c \lambda_p)[\lambda_p] = 0  \label{ngspointproblem}
\end{equation}
for a scalar $c$ at each point.  That is, it solves $\phi(c)=0$ where
\begin{equation}
  \phi(c) = r^h(w^h + c \lambda_p)[\lambda_p], \label{ngspointresidual}
\end{equation}
and then it updates
\begin{equation}
  w^h \leftarrow w^h + c \lambda_p  \label{ngspointupdate}
\end{equation}
or equivalently $w[p] \leftarrow w[p] + c$.  Following the idea of the linear Gauss-Seidel iteration \cite{Greenbaum1997}, $w[p]$ is updated in some nodal ordering using all known values when evaluating the residual in \eqref{ngspointproblem}.  (By default in \texttt{fas1.py} this is in increasing $p$.)  Gauss-Seidel-type methods are called ``multiplicative'' or ``successive'' corrections, in contrast to ``additive'' or ``parallel'' corrections, of which the Jacobi iteration is the best known example.

Even solving the scalar problem $\phi(c)=0$ cannot be done exactly when considering a transcendental problem like \eqref{liouvillebratu}.  Thus we will use a fixed number of Newton iterations to generate a (scalar) sequence $\{c_k\}$.  Starting from initial iterate $c_0=0$ we compute
\begin{equation}
\phi'(c_k)\, s_k = -\phi(c_k),  \qquad  c_{k+1} = c_k + s_k, \label{ngsnewton}
\end{equation}
for $k=0,1,\dots$  In detail, following \eqref{feresidualdetail} we have
\begin{align}
   \phi(c) &= -\frac{1}{h} \left(2(w[p]+c) - w[p-1] - w[p+1]) + h \left(\mu e^{w[p]+c} + g[p]\right)\right), \label{ngsnewtondetails} \\
   \phi'(c) &= -\frac{2}{h} + h \mu e^{w[p]+c}. \notag
\end{align}
The vast majority of the FAS algorithm work is in evaluating these NGS expressions.

The NGS method is applied by ``sweeping'' through the mesh, zeroing the residual at successive points in order.  Function \texttt{ngssweep()} in \texttt{fas1.py} computes one sweep of NGS by using a fixed number (\texttt{-niters}; defaults to 2) of scalar Newton iterations \eqref{ngsnewton} to solve \eqref{ngspointproblem} at each point.  Note that as soon as the residual is made zero at one point it is no longer zero at the previous points (which were just zeroed).  In a linear case the iteration is known to converge based on structural properties of the matrices \cite[for example]{Greenbaum1997} which correspond to ellipticity of the original problem.  We expect that at least for weak nonlinearities (e.g.~small $\mu$ in \eqref{liouvillebratu}) our method will converge as solution method for \eqref{weakform}.  However, one observes in practice that, after substantial progress in the first few sweeps, soon NGS stagnates.  Following Brandt \cite{Brandt1977}, who asserts that such a ``stalling'' scheme must be ``wrong'', we adopt the multigrid approach next.


\section{The FAS equation for two levels}

Full approximation storage (FAS) \cite{Brandt1977,Briggsetal2000} is a multigrid scheme, and thus it includes a hierarchy of meshes, a choice of a ``smoother'' on each level, and a coarse-mesh solution method.  The fundamental goal of any multigrid scheme is to do a minimal amount of work (smoothing) on a given mesh and then switch to an inexpensive coarser mesh to do the rest of the work.  That is, by transferring (restricting) a version of the problem to the coarser mesh one nearly solves for the remaining error.  This coarse-mesh approximation of the error is added-back (prolonged) to correct the solution on the finer mesh.

We describe only two levels at first, with a coarser mesh having $2h$ spacing and $M=m/2$ subintervals.  Notationally, quantities on the coarse mesh will have superscript ``$2h$'', for example an iterate is $w^{2h}$.  The program \texttt{fas1.py} only refines by such factors of two, but the ideas generalize to other refinement factors.

A small fixed number of NGS sweeps is our smoother on the fine mesh.  (For \texttt{fas1.py}, option \texttt{-downsweeps} defaults to $1$.)  Each sweep, solving \eqref{ngspointproblem} and doing update \eqref{ngspointupdate} at every point $x_p$, is an $O(m)$ operation with a small constant.  The constant relates to the number of Newton iterations and the expense of evaluating nonlinearities at each point, e.g.~$\mu e^u$ in \eqref{liouvillebratu}.

A few sweeps of NGS on the fine mesh produces two results on the current iterate $w^h$:
\begin{itemize}
\item the residual $r^h(w^h)$ becomes smooth, but not necessarily small, and
\item the error $e^h = w^h - u^h$ becomes smooth, but not necessarily small.
\end{itemize}

Using more sweeps of NGS would eventually make the residual and error small and thus solve problem \eqref{weakform}, but this is inefficient in the sense that many sweeps may be needed, generally giving an $O(m^k)$ method for $k\gg 1$.  However, NGS sweeps on a coarser mesh see the ``same'' residual, i.e.~the coarse-mesh interpolant of the fine mesh residual, as less smooth, thus a large fraciton of the error is quickly eliminated by smoothing.  Descending to yet coarser meshes after a few sweeps, in a V-cycle as described in the next section, leads to a coarsest mesh on which the error can be eliminated entirely by applying NGS at a few interior points.  (In the default settings for \texttt{fas1.py}, the coarsest mesh has two subintervals and only one interior point.)

The question is, what is the coarse-mesh version of the problem?  To derive this equation, namely to explain Brandt's proposed FAS equation, we start from the finite element approximation of the weak form \eqref{weakform} on a given (fine) mesh, i.e.
\begin{equation}
  F^h(u^h)[v] = \ip{g^h}{v} \qquad \text{for all } v \text{ in } \mathcal{S}^h.  \label{feweakform}
\end{equation}
Suppose we have a current iterate $w^h$.  (The solution $u^h$ is generally unknown.)  Subtracting $F^h(w^h)[v]$ from both sides of \eqref{feweakform} gives the residual \eqref{feresidual} on the right:
\begin{equation}
  F^h(u^h)[v] - F^h(w^h)[v] = r^h(w^h)[v] \label{fasproto}
\end{equation}
for $v$ in $\mathcal{S}^h$.  Three key observations apply for equation \eqref{fasproto}:
\begin{itemize}
\item If $F^h$ were linear in $w^h$ then we could rewrite the equation in terms of the error:
    $$F^h(e^h)[v] = -r^h(w^h)[v], \qquad (\text{\emph{if $F^h$ linear}})$$
the linear error equation $A\be=-\br$ \cite[Chapter 2]{Bueler2021}.
\item If NGS sweeps have generated iterate $w^h$ then $e^h$ and $r^h(w^h)$ are smooth.
\item Both $w^h$ and $r^h(w^h)$ are known and/or computable.
\end{itemize}

Noting that our operator $F^h$ is in fact nonlinear (in $w^h$), the FAS method  proposes a \emph{new} equation on the coarse mesh based on the above considerations.  In the linear case the new equation could be phrased directly in terms of the error.  However, in general the coarse-mesh version of \eqref{fasproto} uses linear restriction operators on the computable quantities, and it also re-discretizes the nonlinear operator to $F^{2h}$ acting on $\mathcal{S}^{2h}$.  We denote the restriction operators by $R'$ and $R$:
\begin{equation}
  F^{2h}(u^{2h})[v] = R' (r^h(w^h))[v] + F^{2h}(R w^h)[v] \label{fasequation}
\end{equation}
for all $v$ in $\mathcal{S}^{2h}$.  Here $u^{2h}$ in $\mathcal{S}^{2h}$ is the solution on the coarse mesh.

Note that if $w^h=u^h$, that is, if $w^h$ were the exact solution to the fine mesh problem \eqref{feweakform}, then $r^h(w^h)=0$ and the right side of \eqref{fasequation} would simplify to $F^{2h}(R w^h)[v]$.  In this case the solution of \eqref{fasequation} would be $u^{2h} = R w^h$ by well-posedness, and we would simply produce the restriction of the solution onto the coarser mesh.

If the coarse mesh is in fact the coarsest one in the hierarchy then we propose to solve \eqref{fasequation} by sufficient NGS sweeps so that $u^{2h}$ is computed almost exactly.  Specifically, if the coarse mesh consists of only one interior point, which it will under the default settings in \texttt{fas1.py}, then this requires a single NGS update if the Newton iteration is sufficiently-accurate.

After solving the FAS coarse-mesh equation \eqref{fasequation} we have $u^{2h}$, assumed exact for the next formula.  Two-level FAS updates the iterate on the finer mesh,
\begin{equation}
  w^h \longleftarrow w^h + P(u^{2h} - R w^h) \label{fasupdate}
\end{equation}
Here $P$ is the prolongation operation which extends a function in $\mathcal{S}^{2h}$ to a function in $\mathcal{S}^h$.

Formulas \eqref{fasequation} and \eqref{fasupdate} specify the FAS algorithm.  However, in order to understand and implement it, we must clarify the action of operators $R'$, $R$, and $P$.  This is the purpose of the next section.


\section{Restriction and prolongation operators for FE methods}

To explain the two different restriction operators in \eqref{fasequation}, plus the prolongation in \eqref{fasupdate}, first note that functions $w^h$ in $\mathcal{S}^h$ are distinct objects from linear functionals like the residual $r^h(w^h)$, which act on $v$ in $\mathcal{S}^h$.  We denote the space of such linear functionals by $(\mathcal{S}^h)'$.  The operators are thereby distinguished by their domain and range spaces:
\begin{align}
  R' &: (\mathcal{S}^h)' \to (\mathcal{S}^{2h})', \label{rpoperators} \\
  R  &: \mathcal{S}^h \to \mathcal{S}^{2h}, \notag \\
  P  &: \mathcal{S}^{2h} \to \mathcal{S}^h. \notag
\end{align}

Next note that both functions in $\mathcal{S}^h$ and linear functionals in $(\mathcal{S}^h)'$ are representable by vectors in $\RR^{m-1}$.  For a function $w^h$ one stores the coefficients $w[p]$ with respect to an expansion in the hat function basis $\{\lambda_p\}$, as in \eqref{fesolution} for example, while one stores a functional $\ell=r^h(w^h)$ by its values $\ell[\lambda_p]$.  Both representations are thus vectors in $\RR^{m-1}$, and though it might make sense to represent $w^h$ as a column vector and $\ell$ as a row vector \cite{TrefethenBau1997}, in Python it is easiest to use ``flat'' one-dimensional NumPy arrays for both.  Observe that within the FAS algorithm an iterate $w^h$ has zero boundary values, and likewise $\ell$ acts on $v$ with zero boundary values, thus only interior-point hat functions are needed in these representations.

But how do $R'$, $R$, and $P$ actually operate in the finite element (FE) case?  A key calculation relates the coarse-mesh hat functions $\lambda_q^{2h}(x)$ to the fine mesh hats $\lambda_p^h(x)$:
\begin{equation}
  \lambda_q^{2h}(x) = \frac{1}{2} \lambda_{2q-1}^h(x) + \lambda_{2q}^h(x) + \frac{1}{2} \lambda_{2q+1}^h(x), \qquad q=1,2,\dots,M-1. \label{hatrelation}
\end{equation}
(Recall that $M=m/2$, and we assume $m$ is even.)  See Figure \ref{fig:hatcombination}.

\begin{figure}
\includegraphics[width=0.6\textwidth]{figs/hatcombination.pdf}
\caption{Formula \eqref{hatrelation} writes a coarse-mesh hat function $\lambda_q^{2h}(x)$ (solid) as a linear combination of three fine-mesh hat functions $\lambda_p^h(x)$ (dotted) for $p=2q-1,2q,2q+1$.}
\label{fig:hatcombination}
\end{figure}

First consider the prolongation $P$.  It is defined as the injection of $\mathcal{S}^{2h}$ into $\mathcal{S}^h$, without changing the function.  Indeed, a piecewise-linear function on the coarse mesh is also a piecewise-linear function on the fine mesh.  Suppose $w^{2h}(x)$ is in $\mathcal{S}^{2h}$, so
    $$w^{2h}(x) = \sum_{q=1}^{M-1} w[q] \lambda_q^{2h}(x).$$
Thus we use \eqref{hatrelation} to compute $P w^{2h}$ in terms of fine-mesh hat functions:
\begin{align}
(P w^{2h})(x) &= \sum_{q=1}^{M-1} w[q] \left(\frac{1}{2} \lambda_{2q-1}^h(x) + \lambda_{2q}^h(x) + \frac{1}{2} \lambda_{2q+1}^h(x)\right) \label{pformula} \\
              &= \frac{1}{2} w[1] \lambda_1^h(x) + w[1] \lambda_2^h(x) + \left(\frac{1}{2} w[1] + \frac{1}{2} w[2]\right) \lambda_3^h(x) \notag \\
              &\qquad + w[2] \lambda_4^h(x) + \left(\frac{1}{2} w[2] + \frac{1}{2} w[3]\right) \lambda_5^h(x) \notag \\
              &\qquad + \dots + w[M-1] \lambda_{m-2}^h(x) + \frac{1}{2} w[M-1] \lambda_{m-1}^h(x) \notag
\end{align}
As a matrix, $P$ acts on the representation vectors, i.e.~$P:\RR^{M-1} \to \RR^{m-1}$ has $M-1$ columns and $m-1$ rows:
\begin{equation}
P = \begin{bmatrix}
1/2 & & & \\
1 & & & \\
1/2 & 1/2 & & \\
 & 1 & & \\
 & 1/2 & 1/2 & \\
 & & & \ddots
\end{bmatrix} \label{pmatrix}
\end{equation}
The column sums of $P$ are all two, because from \eqref{hatrelation} each column has a nonzero block $[1/2,1,1/2]^\top$, and the columns are linearly-independent.  Except for the first and last rows, the row sums are all one.

The restriction $R'$ acts on fine-mesh linear functionals $\ell:\mathcal{S}^h \to \RR$.  It is sometimes called ``canonical restriction'' \cite{GraeserKornhuber2009} because the output functional $R'\ell:\mathcal{S}^{2h}\to \RR$ acts on coarse-mesh functions the same way $\ell$ itself acts on those functions.  Thus we may define it using $P$:
\begin{equation}
  (R'\ell)[v] = \ell[Pv]  \label{rprimedefinition}
\end{equation}
for $v$ in $\mathcal{S}^{2h}$.  As noted earlier, $\ell$ is represented by a vector $\bm{\ell}$ in $\RR^{m-1}$, of values $\ell[p] = \ell[\lambda_p^h]$, so one computes the values of $R'\ell$ using \eqref{hatrelation}:
\begin{align}
  (R'\ell)[q] &= (R'\ell)[\lambda_q^{2h}] = \ell[P\lambda_q^{2h}] = \ell\left[\frac{1}{2} \lambda_{2q-1}^h + \lambda_{2q}^h + \frac{1}{2} \lambda_{2q+1}^h\right]  \label{rprimeformula} \\
      &= \frac{1}{2} \ell[\lambda_{2q-1}^h] + \ell[\lambda_{2q}^h] + \frac{1}{2} \ell[\lambda_{2q+1}^h] = \frac{1}{2} \ell[2q-1] + \ell[2q] + \frac{1}{2} \ell[2q+1]  \notag
\end{align}
for $q=1,2,\dots,M-1$.  In other words, as a matrix $R'$ is the transpose of $P$, with $M-1$ rows and $m-1$ columns:
\begin{equation}
R' = \begin{bmatrix}
1/2 & 1 & 1/2 &   &     & \\
    &   & 1/2 & 1 & 1/2 & \\
    &   &     &   & 1/2 & \\
    &   &     &   &     & \ddots
\end{bmatrix} \label{rprimematrix}
\end{equation}

The restriction $R:\mathcal{S}^h\to\mathcal{S}^{2h}$ which acts on functions requires an additional choice.  There are two possibilities which we give the temporary names $R_1$ and $R_2$:
\begin{itemize}
\item $R_1$ is defined by \emph{projection}, namely the property
    $$\ip{R_1 w^h}{v} = \ip{w^h}{v}$$
for all $v\in \mathcal{S}^{2h}$.  Unfortunately this requires solving a nontrivial linear system.  In particular, one must form the \emph{mass matrix} for the coarse mesh, i.e.~$Q_{st}^{2h} = \ip{\lambda_s^{2h}}{\lambda_t^{2h}}$, and then solve an equation
    $$FIXME R_1 Q^{2h} = Q^{2h}$$
\item $R_2$ FIXME
\end{itemize}

FIXME formula for $R$



\section{FAS V-cycles}

FIXME V-cycles

\small

\bigskip
\bibliography{fas}
\bibliographystyle{siam}

\end{document}
